{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voorwoord en opdracht\n",
    "Hoe ontsluit je data en verwerk je het tot bruikbare inzichten? Python biedt heel veel mogelijkheden \n",
    "om data te ontsluiten, te prepareren en te analyseren. Aan al deze mogelijkheden heb je alleen wat als \n",
    "je ze ook echt toe kunt passen. Aan jou de opdracht om de technieken die je tijdens de colleges hebt \n",
    "geleerd in de praktijk te brengen. Dit doe je door een eigen dataproject uit te voeren: zelf relevante \n",
    "data zoeken en inladen, deze data opschonen en combineren tot relevante inzichten.  \n",
    "\n",
    "## Beoordeling \n",
    "We zijn daarom op zoek naar een originele toepassing waarmee je de inhoud van de college kan \n",
    "vertalen in goed werkende en netjes gedocumenteerde python code. De specifieke scores en weging \n",
    "van deze criteria kun je op de volgende pagina vinden. \n",
    "\n",
    "\n",
    "| **Originaliteit** | **Inhoud** | **Code**| **Documentatie**|\n",
    "|---|---|---|---|\n",
    "| Het project omvat een unieke toepassing die in college niet aan bod is gekomen en de student weet hiermee perfect gebruik te maken van de kracht van python. | Alle data-analysestappen worden doorlopen Hierbij wordt aandacht geschonken aan alle geleerde onderwerpen. Geleerde packages worden verrijkt met code uit meerdere andere bronnen | De gehele code werkt en geeft de juiste resultaten weer. Fouten worden automatisch en correct afgehandeld. De code is volledig modulair en werkt met minimale aanpassingen ook op andere datasets. | De code, documentatie en bijpassende titels en teksten maken van het project een duidelijk geheel dat door anderen zonder verdere uitleg zou kunnen worden begrepen |\n",
    "## Deadline \n",
    "We ontvangen je project graag uiterlijk 1 februari, voor (14:00) via Canvas. Mocht je project tot een \n",
    "onvoldoende beoordeling leiden, dan heb je nog een keer de kans om een herkansing in te leveren. \n",
    "Het is je eigen verantwoordelijkheid om op tijd met je project te starten, we zullen tussendoor dus niet \n",
    "controleren hoe ver je bent of gedeeltelijke projecten beoordelen. \n",
    "Tijdens het laatste college vragen we je om collega studenten een korte presentatie te geven over je \n",
    "project. Die presentatie wordt zelf niet beoordeeld maar geeft je wel de kans om te laten zien hoe je \n",
    "met criteria zoals originaliteit of onvoorziene tegenslagen bent omgegaan.   \n",
    "We raden je aan om de code in Jupyter notebooks te schrijven, maar het staat je vrij om daar een \n",
    "ander programma voor te kiezen zolang wij aan het eind van het project de code en resultaten kunnen \n",
    "beoordelen.  \n",
    "\n",
    "## Onderwerp \n",
    "Omdat we het leuk vinden als je de geleerde python vaardigheden meteen in de praktijk kan \n",
    "toepassen, willen we je vragen om zelf een onderwerp te kiezen. Tijdens de colleges zullen we een \n",
    "aantal voorbeelden geven van eerdere projecten. \n",
    "Mocht je nog vragen hebben over het project dan kun je je vragen het best stellen op de Canvas-\n",
    "cursuspagina. Natuurlijk kun je ons ook altijd mailen of tijdens het college aanspreken.\n",
    "\n",
    "## Wat moet er aan de pas komen\n",
    "\n",
    "### Masterclass 0\n",
    "- ~~Variables~~\n",
    "- ~~Calculations / Arithmetics~~\n",
    "- Statements & Expressions\n",
    "- Datatypes\n",
    "- Operators\n",
    "\n",
    "### Masterclass 1\n",
    "- if, else and elif-statements\n",
    "- One-line if-statements\n",
    "- Conditional expression\n",
    "- ~~While-loop~~\n",
    "- ~~For-loop~~\n",
    "\n",
    "### Masterclass 2\n",
    "- functions, parameters and arguments\n",
    "- ~~docstrings~~\n",
    "- classes (niet verplicht)\n",
    "- imports\n",
    "\n",
    "\n",
    "### Masterclass 3\n",
    "\n",
    "### Masterclass 4\n",
    "\n",
    "### Masterclass 5\n",
    "\n",
    "### Masterclass 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL)\n",
    "In deze notebook ga ik een eerste simpele kennismaking met Reinforcement Learning doen. Reinforcement Learning, RL in het kort, is een tak van machinelearning/deeplearning waarbij er een **agent** wordt aangemaakt die in een **environment** diverse **actions** onderneemt en hiervoor een reward **krijgt**. Het eerste deel van dit notebook wordt gewijt aan het implementeren van een basale RL agent in een voorafgemaakte environment. \n",
    "\n",
    "Zodra de basis van RL is uitgewerkt (Sectie 1 t/m 5) gaan we verder met het toepassen van de kennis op een andere omgeving (Sectie 6). Als dit allemaal geslaagd is gaan we proberen een custom environment te maken voor Pac-Man.\n",
    "\n",
    "## 1.0 Wat is RL in het kort?\n",
    "Met reinforcement Learning wordt geprobeerd een agent te trainen de juiste beslissingen te maken. Beslissingen worden gemaakt in een bepaalde omgeving (bijvoorbeeld een lab of een videospel). Om de analogie van het videospel aan te houden: de agent (mario) voert een actie uit (springen) en maakt een observatie (een muntje komt uit een blokje). Deze observaties en acties worden aan elkaar gekoppeld doormiddel van een beloningssysteem. Gedurende veel itteraties zal de agent trainen en het beste resultaat proberen te behalen.\n",
    "\n",
    "Binnen RL doen we een aantal aannamens of stellen we een paar beperkingen:\n",
    "* Het probleem is (zeer) complex, anders is RL overkill\n",
    "* Het betreft een Markov achtige omgeving --> elke actie heeft een reactie ofwel observaties en acties volgen elkaar op\n",
    "* trainen kan lang duren en is niet altijd stabiel\n",
    "\n",
    "\n",
    "## 1.1 RL raamwerk\n",
    "In onderstaande afbeelding is schematisch weergegeven hoe het raamwerk van Reinforcement Learning inwerking treed. De agent (Pacman) voert een actie uit (linksaf, rechtsaf, omlaag, omhoog), hij eet daardoor meer bolletjes en zijn scoren stijgt (imediate/shaped reward). De spookjes bewegen ook, de algehele staat van het speelveld komt als observatie binnen bij de agent die vervolgens weer een actie neemt. Deze cyclus herhaalt zich tot de agent het level heeft gehaald of af is gegaan. Zodra de agent het level haalt zal deze nog een reward krijgen (sparse rewards). Dit geheel vormt het raamwerk voor RL.\n",
    "\n",
    "\n",
    "![Reinforcement Learning Framework](https://miro.medium.com/max/1400/1*CjLFVeYssOIJaeijrxPHPg.png)\n",
    "\n",
    "Er zijn twee nieuwe termen benoemd ten opzichten van de mario analogie:\n",
    "* *Sparse Rewards*: RL problemen hebben vaak te maken met een belonging die 1-to-many staan (bijv. 1x een reward op elke 1000 zetten/spellen). Dit concept worden dan ook *Sparse rewards* genoemd en vormt een van de uitdagingen van RL ten opzichten van traditionele ML. In Mario en PacMan zijn het halen van levels voorbeelden van *Sparse Rewards*\n",
    "* *Imediate/shaped Rewards*: In bepaalde environments kunnen *imediate/shaped rewards* worden gegeven. In Mario zouden dit muntjes zijn en vijanden verslaan, in PacMan zou dit zijn wanneer de bolletjes en spookjes opgegeten worden. Dit proces van additionele beloningen toekennen kan er voor zorgen dat agent beter leert wat deze moet doen om tot een goed resultaat te komen. Hier schuilt echter ook het gevaar, het algoritme kan te gretig worden en daardoor de voorkeur gaan geven aan vroegtijdige belonginge ten opzichten van de echte beloning aan het einde van het spel.\n",
    "\n",
    "## 1.2 RL formules\n",
    "\n",
    "Om de agent te trainen staan een aantal functies centraal. Namelijk de *Reward function*, *Value funtion* en de *Policy function*. In deze paragraaf bespreken we deze functies in het kort om een beeld te krijgen van wat er onder de motorkap gebeurt:\n",
    "\n",
    "### 1.2.1 *Reward function*\n",
    "de beloning die een agent krijgt voor een bepaalde actie. deze functie kan als onderstaande worden opgeschreven:\n",
    "\n",
    "$$r_{t} = R(s_{t},a_{t},s_{t+1})$$\n",
    "hierin is *r* de beloning, *t* de huidige tijdstap, *R()* een specifieke beloningsfunctie, *s* de huidige obsertvatie/state, *a* de genomen actie. De formule stelt dus \"De huidige beloning, is gelijk aan de functie van de huidige staat de huidige actie en de staat/observatie van de volgende tijdstap.\" Deze functie wordt vaak versimpeld naar R() van de huidige staat of R() van de huidige actie/staat combinatie.\n",
    "\n",
    "Het doel is dus om de cummulatieve beloning over een periode te maximalizeren. De benadering van deze cummulatieve beloning bepaalt de functie van R() hieronder worden twee varianten van de functie besproken.<br> \n",
    "\n",
    "allereerst: **finite-horizon undiscounted return** waarin de functie puur een opsomming is van de belongingen over *n* tijdsstappen.<br>\n",
    "\n",
    "De tweede variant heet: **infinite-horizon discounted return**, hierbij worden de beloningen van alle tijdstappen meegenomen, echter hoeverder de tijdstap (en beloning) in de toekomst ligt, hoe groter de discount. Dit betekent dat een beloning die over 100*t* plaatsvindt minder waard is dan de beloning over 1*t*. De formules voor deze functies zijn respectievelijk:\n",
    "$$R(\\tau) = \\sum_ {t=0}^T\\ r_{t}$$\n",
    "$$R(\\tau) = \\sum_ {t=0}^\\infty \\ r_{t} $$\n",
    "\n",
    "$\\tau$ is in dit geval een opeenvolging van staten en acties: $\\tau = (S_{0},a_{0},S_{1},a_{0}, \\cdots)$ dit wordt ookwel *trajectory, episodes, of rollouts* genoemd. De start staat is hierbij vaak willekeurig gekozen. de volgende staten worden bepaald door de genomen actie van de agent.\n",
    "\n",
    "### 1.2.2 *Value function*\n",
    "De value function kijkt naar de huidige *value* van een staat/observatie. In dit geval wordt met *value* het volgende bedoeld: de verwachte beloning voor de huidige staat of staat-actie combinatie wanneer één policy gekozen wordt en hier niet meer van afgeweken wordt. Van deze value functies zijn er 4 belangrijke te benoemen:\n",
    "\n",
    "1. **On-Policy Value Function**: $V^{\\pi}(s)$ geeft de verwachte return (cummulatieve beloning) wanneer je start in staat *s* en altijd policy $\\pi$ aanhoudt:\n",
    "$$V^{\\pi}(s) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "2. **On-Policy Action-Value Function**: $Q^{\\pi}(s,a)$ de verwachte return wanneer een willekeurige actie wordt genomen vanaf de eerste state endaaropvolgend alleen maar de gekozen policy inzet:\n",
    "$$Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "3. **Optimal Value Function**: $V^{*}(s)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen:\n",
    "$$V^{*}(s) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "4. **Optimal Action-Value Function**: $Q^{*}(s,a)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen na eerst eenmalig een willekeurige actie te hebben gekozen:\n",
    "$$Q^{*}(s,a) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "### 1.2.3 Policy function\n",
    "De policy function (of agent function). Is een functie die aangeeft welke actie een agent neemt. Deze agent en policy termen worden vaak doorelkaar gebruikt. Zo kan policy in: \"de policy probeert de beloning te maximalizeren\" vervangen worden door \"de agent\". Er zijn twee variante van de policy function, namelijk:\n",
    "\n",
    "1. deterministisch, toeval speelt hier geen rol, zijn de begin voorwaarden gelijk dan zal de uitkomst ook gelijk zijn. In dit geval wordt de policy function aangegeven als $a_{t} = \\mu(s_{t})$. Hier staat \"de huidige actie wordt bepaald door de functie $\\mu()$ gegeven de huidige staat\n",
    "\n",
    "2. Stochastisch, in dit geval speelt toeval een rol. De uitkomst is van te voren nog niet bekend. Hier wordt gekeken naar de kans van een actie - vervolg staat . De functie kan opgeschreven worden als: $a_{t} \\sim \\pi(a|s_{t})$ wat betekent dat \"de actie evenredig is met de policy functie $\\pi()$ van een kans gegeven staat *t*.\" ofwel \"kies de actie die de meeste waarschijnlijkheid heeft om te leiden tot de volgende beste staat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "stable-baselines3 1.3.0 requires gym<0.20,>=0.17, but you have gym 0.21.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached gym-0.21.0-py3-none-any.whl\n",
      "Collecting numpy>=1.18.0\n",
      "  Using cached numpy-1.21.4-cp39-cp39-win_amd64.whl (14.0 MB)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: numpy, cloudpickle, gym\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.0\n",
      "    Uninstalling numpy-1.20.0:\n",
      "      Successfully uninstalled numpy-1.20.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.19.0\n",
      "    Uninstalling gym-0.19.0:\n",
      "      Successfully uninstalled gym-0.19.0\n",
      "Successfully installed cloudpickle-2.0.0 gym-0.21.0 numpy-1.21.4\n"
     ]
    }
   ],
   "source": [
    "# Python 3.9.7 is nodig voor het gebruik van deze modules\n",
    "# !pip install stable-baselines3[extra] ## docummentatie voor deze module die RL algoritmes bevat: https://stable-baselines3.readthedocs.io/en/master/\n",
    "# !pip install gym --force\n",
    "# !pip install pyglet ## extra dependency voor OpenAI gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12732/1387558010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[1;31m# importeren voor het omgaan met paths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m \u001b[1;31m# voor het beheren van de omgeving van onze agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m \u001b[1;31m# gebruiken om geen visualisaties te doen van de environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPPO\u001b[0m \u001b[1;31m# een specifiek algorithme voor RL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m \u001b[1;31m# wrapper voor environment beheersing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import os # importeren voor het omgaan met paths\n",
    "import gym # voor het beheren van de omgeving van onze agents\n",
    "from gym import wrappers # gebruiken om geen visualisaties te doen van de environment\n",
    "from stable_baselines3 import PPO # een specifiek algorithme voor RL\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # wrapper voor environment beheersing\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # beoordeling van agent policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Environments\n",
    "Voor de environments maken we gebruik van [OpenAI gym](https://gym.openai.com/envs/#classic_control). In deze environments gaat de agent leren door tijdens een aantal episodes \"random\" acties uit te voeren. De environment is de overkoepelende omgeving waarin de agent acteerd, observaties worden gedaan en waaruit beloning worden gehaald. Een OpenAI gym omgeving kan in verschillende vormen (spaces) voorkomen:\n",
    "* **Box:** n-dimensionele tensor, die een range van waarden bevat --> Box(0, 1, shape=(3,3))\n",
    "* **Discrete:** set van discrete waarden --> Discrete(3)\n",
    "* **Tuple:** Een Tuple van andere spaces --> Tuple((Discrete(3), Box(0, 1, shape=(3,3))))\n",
    "* **Dict:** Dictionary van spaces --> Dict({('height': Discrete(3), 'speed':Box(0, 1, shape=(3,3)))})\n",
    "* **MultiBinary:** One hot encoded binary waarde --> MultiBinary(4) --> [0,0,0,0] of [0,1,0,1] etc.\n",
    "* **MultiDiscrete:** Meerdere discrete waarde --> MultiDiscrete([2,5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inladen van environment\n",
    "environment_name ='CartPole-v0'\n",
    "env = gym.make(environment_name)\n",
    "# Test om te zien wat de environment bevat\n",
    "print(\n",
    "    f'''\n",
    "    env.action_space: {env.action_space}\n",
    "    env.observation_space: {env.observation_space}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CartPole-v0\n",
    "Voor de eerste test wordt de CartPole-v0 environment gebruikt. Deze environment bestaat (zoals hierboven te zien) uit:\n",
    "* Een action space Discrete(2), hierbij is 0 == links en 1 == rechts op een virtuele controller.\n",
    "* de observation space, het scherm dat de speler/agent ziet is een Box() type de eerste array geeft de lower-bound aan van 4 type observaties, de tweede array de upper-bound. een array eenmaal aangeroepen bestaat uit 4 elementen van het type float32\n",
    "\n",
    "### 2.2 De observation space\n",
    "```\n",
    "env.observation_space: \n",
    "    Box(\n",
    "        [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38],\n",
    "        [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38],\n",
    "        (4,),\n",
    "        float32)\n",
    "```\n",
    "\n",
    "dit kan als volgt geïnterpeteerd worden: <br>\n",
    "env.observation_space[0] = [-4.8000002e+00] en [4.8000002e+00] --> dit zijn de min en max posities van het wagentje<br>\n",
    "env.observation_space[1] = [-3.4028235e+38] en [3.4028235e+38] --> de velocity (snelheid) van het wagentje<br>\n",
    "env.observation_space[2] = [-4.1887903e-01] en [4.1887903e-01] --> hoek van de paal ten opzichten van het wagentje<br>\n",
    "env.observation_space[3] = [-3.4028235e+38] en [3.4028235e+38] --> velocity van de paal<br>\n",
    "\n",
    "binnen deze observation space wordt er per env.observation_space[*x*] een waarde teruggeven voor de agent om op te acteren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    " # het aantal keer dat de agent het spel speelt.\n",
    "\n",
    "for episode in range(0, episodes): # loop over de episodes.\n",
    "    state = env.reset() # stel de omgeving opnieuw in.\n",
    "    done = False # zodra het spel is afgelopen verandert dit naar done, en begint dit op false.\n",
    "    score = 0 # de score die onze agent moet verhogen.\n",
    "\n",
    "    while not done: # not done == not False == True.\n",
    "        env.render() #render het spel in een omgeving. normaal gesproken staat dit uit voor snelheid.\n",
    "        action = env.action_space.sample() # van onze action space kies 1 actie random (links of rechts).\n",
    "        n_state, reward, done, info = env.step(action) # neem een stap (frame/actie), dit returned 4 waarden. een nieuwe Box(), de beloning, False/True (het spel is afgelopen), en eventueel extra info.\n",
    "        score += reward # tel de beloning voor deze actie op bij de score van deze episode.\n",
    "    print(f'Episode: {episode+1} | Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Algorithmes\n",
    "\n",
    "Er zijn diverse algorithmes algoritmes beschikbaar. Binnen deze notebook wordt gebruik gemaakt van Model-Free RL. dit zijn algorithmes die zich minder/niet bezig houden met het voorspellen van de volgende state maar puur kijken naar een een set regels die de agent maakt op basis van ervaring. Zo kan de agent een regel opstellen dat het wagentje naar links moet als de stok een beetje links kantelt. In onderstaande afbeelding zijn de diverse opties te zien. In dit geval ligt de focus op PPO (Proximal Policy Optimization) dit is een policy algoritme die minder kans heeft om vast te komen zitten in een minima. Normaal gesproken leert policy based algoritmes van de huidige staat en kiest een nieuwe policy. Met PPO gebeurt dit ook maar wordt het verschil tussen policy *p<sub>t</sub>* en *p<sub>t-1</sub>* geminimaliseerd. <br>\n",
    "\n",
    "![algorithmes](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Metrics\n",
    "Voor elk type algoritme zijn diverse metrics beschikbaar. deze kunnen in 4 categoriën worden opgedeeld:\n",
    "* Evaluation: beschrijving van de lengte en reward voor een episode\n",
    "* Time: alles wat betreft tijd besteding\n",
    "* Loss: loss functie om het model te scoren\n",
    "* Overig: deze worden toeglicht wanneer deze aanbod komen\n",
    "\n",
    "De metrics worden in apparte mappen opgeslagen als log bestanden en zijn uit te lezen op diverse manieren. In dit notebook wordt gekeken naar de tensorboard visualisaties en .npz files die zullen worden omgezet naar een dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bepaal de locatie voor het opslaan van logs en models\n",
    "log_path = os.path.join('Training', 'Evaluations')\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "training_path = os.path.join('Training', 'Training')\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name) # herhaal het maken van de environment\n",
    "env = DummyVecEnv([lambda:env]) # wrapper om de environment in een dummy vector te plaatsen\n",
    "model = PPO('MlpPolicy', env, verbose= 1, tensorboard_log= log_path) # een standaard NeuralNetwork (Multilayer Perceptron), verbose= 1 laat ons de resultaten loggen, tensorboard_log = locatie voor log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps= 20000) # train het model voor x timesteps\n",
    "model.save(PPO_path) # Sla het model op in de locatie: Training\\Saved Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# het model kan geladen worden voor het geval er iets mis is gegaan\n",
    "\n",
    "try: #probeer eerst het bestaande model te verwijderen uit de variable list en laad dan een opgeslagen model\n",
    "    del model #verwijder het bestaande model\n",
    "    model = PPO.load(PPO_path, env=env) # laad het model\n",
    "except NameError: # als het model niet bestaat bij de \"del model\" functie catch the exception en laad het model\n",
    "    model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "print(f'Model is ingeladen!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evalueren van model en tensorboard\n",
    "In de onderstaande cellen gaan we het model evalueren en de de geloggde gegevens in een tensorboard plaatsen. \n",
    "Voor nu zijn de belangrijkste metrics om in de gaten te houden de *average Reward* en *average episode length*.\n",
    "Aangezien deze niet aanwezig zijn in de tensorboard worden deze appart gevisualiseerd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalueer het model op basis van een aantal parameters (average punten, standard deviation) --> max punten = 200.\n",
    "evaluate_policy(model, env, n_eval_episodes= 10, render=True) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om een agent in een specifieke environment te draaien\n",
    "def run_agent(env, model = False, episodes= 5, render = False):\n",
    "    '''\n",
    "    runs an agent (model) through the environment for n episodes, if render is set to False no window is rendered.\n",
    "    at the end of the runs a tensorlog file is written to a predetermined directory\n",
    "    '''\n",
    "    for episode in range(0, episodes): \n",
    "        obs = env.reset() \n",
    "        done = False \n",
    "        score = 0 \n",
    "\n",
    "        while not done:\n",
    "            if render: \n",
    "                env.render(mode='human') \n",
    "            if model:\n",
    "                action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()\n",
    "\n",
    "# draai nu de zelfde environment met een getrainde agent\n",
    "run_agent(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorlog ophalen van een locatie\n",
    "train_log_path = os.path.join(log_path, 'PPO_3')\n",
    "\n",
    "#klik de play button hieronder\n",
    "%tensorboard --logdir= train_log_path # klik de play button hierboven voor de Tensorboard evaluations grafieken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evalueren op basis van score\n",
    "\n",
    "In de onderstaande cellen wordt de agent gescoord op zijn vaardigheden. Gezien de omgeving redelijk simpel is vindt de agent over het algemeen met gemak een optimale oplossing na verloop van het trainings proces.\n",
    "\n",
    "De eerst volgende 2 cellen beoordelen een volledig getrained model. Het is duidelijk dat hier weinig interessants uitkomt. Het model scoort namelijk in 4 van de 5 episodes maximaal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad het model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "episodes = 5 \n",
    "lst_score = []\n",
    "lst_episode = []\n",
    "\n",
    "#Draai het model\n",
    "for episode in range(0, episodes): \n",
    "    obs = env.reset() \n",
    "    done = False \n",
    "    score = 0 \n",
    "\n",
    "    while not done: \n",
    "        # env.render() \n",
    "        action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    lst_score.append(score[0])\n",
    "    lst_episode.append(episode + 1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dataframe en manipulatie\n",
    "import seaborn as sns # visualisatie\n",
    "from matplotlib import pyplot as plt # extra functionaliteit voor visualisatie\n",
    "\n",
    "d = {\"episode\": lst_episode, \"score\": lst_score}\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "\n",
    "sns.lineplot(x= df[\"episode\"], y=df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evalueren van meerdere agents\n",
    "\n",
    "Hieronder worden 2 nieuwe agents getrained. Wordt de evaluatie van de agent tijdens het trainen gelogged naar een .npz file. In deze log komen alleen de scores en de duratie van de episodes terug.\n",
    "\n",
    "Door twee verschillende modellen te trainen kunnen we het effect van PPO policy duidelijk terug zien. Het is namelijk zo dat de PPO minimale aanpassingen doet in de voorgaande strategie (timestep). Deze afwijkingen zorgen doorgaans voor een langzaam stijgende lijn en voorkomt in de meeste gevallen een foute strategie door een te grote policy wijziging. In de 3e cell hieronder is dit ook te zien. PPO zorgt voor een doorgaans stijgende lijn en afwijkingen in de policy worden minimaal gemaakt hierdoor zie je dat een agent niet snel in een local optimum komt of naar een minima toe beweegt. Hier moet wel gemeld worden dat dit een zeer simpele omgeving betreft waardoor het niet vreemd is dat agents in deze omgeving een globaal optimum bereiken.\n",
    "\n",
    "![model1](visualisatie\\model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nieuw model voor loggen trainings informatie\n",
    "# env = wrappers.Monitor(env, video_callable=False ,force=True)\n",
    "model2 = PPO('MlpPolicy', env)\n",
    "model2.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # gebruiken voor wiskundige operaties en npz files te lezen\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# functie voor het laden en transformeren van npz files\n",
    "def npz_to_dataframe(npz):\n",
    "    '''\n",
    "    Takes an npz file object loaded using numpy.load(path to .npz file) and returns a formatted dataframe\n",
    "    '''\n",
    "    # maak een data frame van de npz file op basis van de kolommen\n",
    "    df = pd.DataFrame.from_dict({col: npz[col] for col in npz.files}, orient='index')\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # transpose de dataframe gezien deze verkeerd om staat\n",
    "    df =df.T\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # bereken de gemiddeldes voor iedere cel\n",
    "    df = df.applymap(np.mean)\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_training_score = npz_to_dataframe(npz)\n",
    "df_training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een lineplot om de prestaties van het model na diverse timesteps te zien\n",
    "fig = sns.lineplot(x= df_training_score[\"timesteps\"], y=df_training_score[\"results\"])\n",
    "display(fig)\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vergelijk de training met een tweede model\n",
    "model3 = PPO('MlpPolicy', env)\n",
    "model3.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)\n",
    "\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# Roep de eerder gemaakte functie aan\n",
    "df_training_score2 = npz_to_dataframe(npz)\n",
    "\n",
    "#voeg aan beide data frames een kolom toe met de agent naam\n",
    "df_training_score[\"model\"] = \"model 1\"\n",
    "df_training_score2[\"model\"] = \"model 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder worden de twee agents tijdens het trainen met elkaar vergeleken. Het is goed te zien dat de twee agents andere start punten hebben. Door kleine aanpassingen in de strategie kan de score in het begin sterk stijgen/dalen om vervolgens na een redelijk stijgende lijn in een bepaald optimum te settlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_training_score.append(df_training_score2)\n",
    "fig = sns.lineplot(x= df_compare[\"timesteps\"], y=df_compare[\"results\"], hue= df_compare['model'])\n",
    "display(fig)\n",
    "\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1vsmodel2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Callback en earlystopping\n",
    "Vooral bij grote modellen kan tijdens het trainen het model onstabiel worden. Om dit te verhelpen kunnen we het trainen stoppen wanneer dit een bepaalde grenswaarden bereikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "best_model_path = \".\\Training\\Saved Models\"\n",
    "\n",
    "# StopTrainingOnRewardThreshold --> voorwaarden wanneer het trainen moet stoppen\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold= 200, verbose= 1)\n",
    "\n",
    "# EvalCallback --> evalueert het trainingproces en controlleert de treshhold\n",
    "eval_callback = EvalCallback(env, callback_on_new_best= stop_callback, eval_freq= 1000, best_model_save_path= best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Andere policies en algoritmes\n",
    "\n",
    "In de onderstaande cellen gaan we verder met het opstellen van twee alternatieve modellen. \n",
    "\n",
    "De eerste stap gaat zijn om een PPO model op te stellen met een zelf samengesteld Neural Network. Hoewel Stable Baselines goed afgestelde modellen heeft is het goed om te weten dat deze nog verder verfijnd kunnen worden. In dit geval maken we gebruik van een neural network voor de agent/decision functie (*pi*) en de value function (*vf*). beider netwerken zullen bestaan uit 4 lagen met iedere laag 128 neuronen.\n",
    "\n",
    "In de tweede cel laden we kort een ander algorithme, DQN (Deep Q Network). Dit model is voor de volgende spaces te gebruiken:\n",
    "\n",
    "|Spaces | Action | Observation |\n",
    "|---|---|---|\n",
    "|  Discrete | ✔️ | ✔️ |\n",
    "|  Box | ❌ | ✔️ |\n",
    "|  MultiDiscrete | ❌ | ✔️ |\n",
    "|  MultiBinary | ❌ | ✔️ |\n",
    "\n",
    "Dit model lijkt zich dus goed te lenen voor het Carpole probleem. Hier hebben we namelijk te maken met een **discrete action space** en een **box observation space**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Een neuralnetwork architectuur voor agent (pi) en valuefunction (vf)\n",
    "NN_arch = [dict(pi= [128,128,128,128], vf= [128,128,128,128] )]\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch': NN_arch})\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Project 1: Breakout\n",
    "\n",
    "In deze sectie gaan we proberen het spel Breakout te halen. Breakout is een simpel spel waarbij we met een peddle onder aan het scherm proberen een balletje te raken om hiermee blokken boven in het scherm kappot te maken. Om dit tot een succes te laten verlopen, doorlopen we ongeveer dezelfde stappen als in de voorgaande secties:\n",
    "1. Omgeving inladen\n",
    "2. Omgeving verkennen\n",
    "3. Geschikte algoritmes zoeken\n",
    "4. Agent trainen\n",
    "5. Beoordelen agent\n",
    "6. Hyperparamter tuning????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import gym\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os\n",
    "\n",
    "# nodig om atari spellen te mogen/kunnen gebruiken\n",
    "!python -m atari_py.import_roms \"Roms\\Roms\\ROMS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 de omgeving\n",
    "\n",
    "We hebben dus te maken met een box observation space en een discrete action space. De observation space bestaat uit drie dimensies (Rood, Groen, Blauw waarde) en heeft een afmeting van 210 height x 160 width. Daarnaast bestaat de action space uit 4 discrete acties 'NOOP', 'FIRE', 'RIGHT', en 'LEFT'. Hierbij staat 'NOOP' voor het nemen van geen actie, 'FIRE' wordt gebruikt om het spel op te starten.\n",
    "\n",
    "Voor dit probleem is er dus een algoritme nodig dat gebruik kan maken van een een discrete action space. In de [documentatie van Stable Baselines](https://stable-baselines.readthedocs.io/en/master/guide/algos.html) is te zien dat er een aantal algoritmes in aanmerking komen, namelijk: A2C, ACER, ACKTR, DQN, HER, GAIL en PPO. Deze algoritmes kunnen allemaal omgaan met een discrete action space.\n",
    "\n",
    "We gaan om te beginnen gebruik maken van A2C, DQN, en PPO. Dit verkleind de scope en geeft een beeld van het effect dat diverse modellen zullen hebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 | Score: 2.0\n",
      "Episode: 2 | Score: 0.0\n",
      "Episode: 3 | Score: 1.0\n",
      "Episode: 4 | Score: 2.0\n",
      "Episode: 5 | Score: 2.0\n"
     ]
    }
   ],
   "source": [
    "# aanmaken environment en testen van een sample agent\n",
    "environment_name = 'Breakout-v0'\n",
    "env = gym.make(environment_name)\n",
    "run_agent(env, render= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    observation space type: <class 'gym.spaces.box.Box'>\n",
      "    observation space shape: (210, 160, 3)\n",
      "    action space: Discrete(4)\n",
      "    aanwezige actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#verkennen van de environment\n",
    "print(\n",
    "    f'''\n",
    "    observation space type: {type(env.observation_space)}\n",
    "    observation space shape: {env.observation_space.shape}\n",
    "    action space: {env.action_space}\n",
    "    aanwezige actions: {env.unwrapped.get_action_meanings()}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 De Agents\n",
    "We proberen dus een agent te trainen op het spel Breakout. Ook hebben we gezien dat de observation space een afbeelding is (rgb van 210 bij 160). Dit betekent dat het Mlp-model niet gaat werken, of in iedergeval veel minder effectief zal zijn. In dit geval proberen we een Convolutional Neural Network (CNN) te trainen. Dergelijke netwerken zijn beter in het behandelen van afbeeldingen.\n",
    "\n",
    "### 6.2.1 A2C\n",
    "[Advantage Actor Critic](https://openai.com/blog/baselines-acktr-a2c/), A2C, is een variatie op het A3C (Asynchronous Advantage Actor Critic) algoritme. A2C heeft de mogelijkheid om tegelijk meerdere workers in te zetten op de environment.\n",
    "\n",
    "A3C combineert een aantal belangrijke elementen:\n",
    "* een update aan het algorithme gebeurt na een vast aantal timesteps. Over dit segment worden estimators berkend op de return en advantage functies.\n",
    "* Lagen van de netwerk architectuur worden gedeeld tussen de policy function (pi) en de value function(vf)\n",
    "* Asynchronysche updates\n",
    "\n",
    "Waar A3C de werkers los van elkaar update doet wacht A2C met een update van de werkers totdat deze allemaal klaar zijn met een segment. Vervolgens wordt er een average van alle workers gepakt voor de update. Binnen Stable Baselines en OpenAI lijkt de performance van A2C een stuk beter dan A3C.\n",
    "\n",
    "### 6.2.2 DQN\n",
    "\n",
    "### 6.2.3 PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 5\n",
    "MODEL_PATH = 'Breakout_model\\Saved_Model'\n",
    "LOG_PATH = 'Breakout_model\\Logging'\n",
    "\n",
    "# Parallel environments voor snellere training\n",
    "env = make_atari_env('Breakout-v0', n_envs = 4, seed= 13)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# probeer het model te laden \n",
    "try:\n",
    "    A2C_25K_model = A2C.load(MODEL_PATH + '\\\\a2c_Breakout_25k.zip')\n",
    "    print('model loaded!')\n",
    "except FileNotFoundError:\n",
    "    A2C_25K_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_25k')\n",
    "    model.learn(total_timesteps=25000)\n",
    "    model.save(MODEL_PATH + \"\\\\a2c_Breakout_25k\")\n",
    "    print('model saved!')\n",
    "\n",
    "# maak een enkele environment voor visualisatie\n",
    "env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "run_agent(env, A2C_25K_model, render= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    A2C_1M_model = A2C.load(MODEL_PATH + '\\\\a2c_Breakout_1M.zip')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    env = make_atari_env('Breakout-v0', n_envs = 4, seed= 13)\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "    model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_1M')\n",
    "    model.learn(total_timesteps=1000000)\n",
    "    model.save(MODEL_PATH + '\\\\a2c_Breakout_1M.zip')\n",
    "\n",
    "env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "run_agent(env, A2C_1M_model, render= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.0 Bronnen\n",
    "[3 hour course](https://www.youtube.com/watch?v=Mut_u40Sqz4)<br>\n",
    "[baseline docummentatie](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5041ba25690102580176b8f7b4720df17ebf4bc4d40fd191e3f1a5800ffdb29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
