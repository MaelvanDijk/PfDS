{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voorwoord en opdracht\n",
    "Hoe ontsluit je data en verwerk je het tot bruikbare inzichten? Python biedt heel veel mogelijkheden \n",
    "om data te ontsluiten, te prepareren en te analyseren. Aan al deze mogelijkheden heb je alleen wat als \n",
    "je ze ook echt toe kunt passen. Aan jou de opdracht om de technieken die je tijdens de colleges hebt \n",
    "geleerd in de praktijk te brengen. Dit doe je door een eigen dataproject uit te voeren: zelf relevante \n",
    "data zoeken en inladen, deze data opschonen en combineren tot relevante inzichten.  \n",
    "\n",
    "## Beoordeling \n",
    "We zijn daarom op zoek naar een originele toepassing waarmee je de inhoud van de college kan \n",
    "vertalen in goed werkende en netjes gedocumenteerde python code. De specifieke scores en weging \n",
    "van deze criteria kun je op de volgende pagina vinden. \n",
    "\n",
    "\n",
    "| **Originaliteit** | **Inhoud** | **Code**| **Documentatie**|\n",
    "|---|---|---|---|\n",
    "| Het project omvat een unieke toepassing die in college niet aan bod is gekomen en de student weet hiermee perfect gebruik te maken van de kracht van python. | Alle data-analysestappen worden doorlopen Hierbij wordt aandacht geschonken aan alle geleerde onderwerpen. Geleerde packages worden verrijkt met code uit meerdere andere bronnen | De gehele code werkt en geeft de juiste resultaten weer. Fouten worden automatisch en correct afgehandeld. De code is volledig modulair en werkt met minimale aanpassingen ook op andere datasets. | De code, documentatie en bijpassende titels en teksten maken van het project een duidelijk geheel dat door anderen zonder verdere uitleg zou kunnen worden begrepen |\n",
    "## Deadline \n",
    "We ontvangen je project graag uiterlijk 1 februari, voor (14:00) via Canvas. Mocht je project tot een \n",
    "onvoldoende beoordeling leiden, dan heb je nog een keer de kans om een herkansing in te leveren. \n",
    "Het is je eigen verantwoordelijkheid om op tijd met je project te starten, we zullen tussendoor dus niet \n",
    "controleren hoe ver je bent of gedeeltelijke projecten beoordelen. \n",
    "Tijdens het laatste college vragen we je om collega studenten een korte presentatie te geven over je \n",
    "project. Die presentatie wordt zelf niet beoordeeld maar geeft je wel de kans om te laten zien hoe je \n",
    "met criteria zoals originaliteit of onvoorziene tegenslagen bent omgegaan.   \n",
    "We raden je aan om de code in Jupyter notebooks te schrijven, maar het staat je vrij om daar een \n",
    "ander programma voor te kiezen zolang wij aan het eind van het project de code en resultaten kunnen \n",
    "beoordelen.  \n",
    "\n",
    "## Onderwerp \n",
    "Omdat we het leuk vinden als je de geleerde python vaardigheden meteen in de praktijk kan \n",
    "toepassen, willen we je vragen om zelf een onderwerp te kiezen. Tijdens de colleges zullen we een \n",
    "aantal voorbeelden geven van eerdere projecten. \n",
    "Mocht je nog vragen hebben over het project dan kun je je vragen het best stellen op de Canvas-\n",
    "cursuspagina. Natuurlijk kun je ons ook altijd mailen of tijdens het college aanspreken.\n",
    "\n",
    "## Wat moet er aan de pas komen\n",
    "\n",
    "### Masterclass 0\n",
    "- ~~Variables~~\n",
    "- ~~Calculations / Arithmetics~~\n",
    "- Statements & Expressions\n",
    "- Datatypes\n",
    "- Operators\n",
    "\n",
    "### Masterclass 1\n",
    "- if, else and elif-statements\n",
    "- One-line if-statements\n",
    "- Conditional expression\n",
    "- ~~While-loop~~\n",
    "- ~~For-loop~~\n",
    "\n",
    "### Masterclass 2\n",
    "- ~~functions, parameters and arguments~~\n",
    "- ~~docstrings~~\n",
    "- classes (niet verplicht)\n",
    "- ~~imports~~\n",
    "\n",
    "\n",
    "### Masterclass 3\n",
    "- ~~Fruitfull functions (with return statement)~~\n",
    "- ~~Fruitless functions (without return statement)~~\n",
    "- ~~Docstring~~\n",
    "- Argument tuple packing (*args, **kwargs)\n",
    "- clasess (not required)\n",
    "- ~~importing libraries~~\n",
    "\n",
    "### Masterclass 4\n",
    "- ~~numpy~~\n",
    "- ~~lists~~\n",
    "- ~~dicts~~\n",
    "- tuple\n",
    "\n",
    "### Masterclass 5\n",
    "- api\n",
    "- ~~visualization~~\n",
    "- ~~error handling (try, except,~~ else, finally)\n",
    "\n",
    "### Masterclass 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opdracht introductie: Reinforcement Learning\n",
    "\n",
    "Voor het vaak Programming for Data Science 2020-2021 is het de bedoeling een programmeer project aan te leveren bij voorkeur in Jupyter notebook format. De eisen zijn om in iedergeval gebruik te maken van verschillende fundamenten in het programeren (data-types, functions, imports, etc.) Het onderwerp van de gekozen opdracht is het toepassen van **Reinforcement Learning (RL).**\n",
    "\n",
    "Het notebook is al volgt opgebouwd:\n",
    "1. Uitleg en introductie over RL\n",
    "2. verkennen van de centrale libraries\n",
    "3. toepassen op een *complexe* omgeving\n",
    "\n",
    "Alleen deel 3 doornemen is genoeg om te zien wat Reinforcement Learning is. Om echter tot het toepassings niveau te komen kunnen deel 1 en 2 worden gelezen. Hierin worden de berekeningen toegelicht en worden de functies en dergelijke verder uitgediept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Wat is Reinforcement Learning (RL)?\n",
    "\n",
    "Reinforcement Learning, RL in het kort, is een tak van machinelearning/deeplearning. Waar andere machinelearning takken (historische) data nodig hebben om te voorspellen, clustteren, clasificeren, etc. Is Reinforcement Learning een onderdeel van machinelearning waarbij de machine zijn eigen data creeërt en op basis hiervan leert. RL kan daardoor gebruikt worden met minimale initiële data en zal dit gaandeweg het trainingsproces zelf maken. Dit kan tijd en kosten besparen. In plaats van een robot te trainen in een fysieke omgeving kan deze omgeving en robot in een digitale omgeving gesimuleerd en getrained worden. Dit beperkt de benodigde middelen en maakt ontwikkelen van robots relatief goedkoop.<br><br>\n",
    "![Machinelearning catagories](https://d6vdma9166ldh.cloudfront.net/media/images/1561029079805-Machine-Learning.jpg)\n",
    "\n",
    "### 1.1 Wat is RL in het kort?\n",
    "Met reinforcement Learning wordt geprobeerd een **agent** te trainen de juiste **beslissingen** te maken. Beslissingen worden gemaakt in een bepaalde **omgeving** (bijvoorbeeld een lab of een videospel). Om de analogie van het videospel aan te houden: de agent (mario) voert een actie uit (springen) en maakt een **observatie** (een muntje komt uit een blokje). Deze observaties en acties worden aan elkaar gekoppeld doormiddel van een beloningssysteem. Gedurende veel itteraties zal de agent trainen en het beste resultaat proberen te behalen.\n",
    "\n",
    "Binnen RL doen we een aantal aannamens of stellen we een paar beperkingen:\n",
    "* Het probleem is (zeer) complex, anders is RL overkill.\n",
    "* Het betreft een Markov achtige omgeving --> elke actie heeft een reactie ofwel observaties en acties volgen elkaar op.\n",
    "* trainen kan lang duren en is niet altijd stabiel.\n",
    "\n",
    "\n",
    "### 1.2 RL raamwerk\n",
    "In onderstaande afbeelding is schematisch weergegeven hoe het raamwerk van Reinforcement Learning inwerking treed. De agent (Pacman) voert een actie uit (linksaf, rechtsaf, omlaag, omhoog), hij eet daardoor meer bolletjes en zijn scoren stijgt (imediate/shaped reward). De spookjes bewegen ook, de algehele staat van het speelveld komt als observatie binnen bij de agent die vervolgens weer een actie neemt. Deze cyclus herhaalt zich tot de agent het level heeft gehaald of af is gegaan. Zodra de agent het level haalt zal deze nog een reward krijgen (sparse rewards). Dit geheel vormt het raamwerk voor RL.<br><br>\n",
    "![Reinforcement Learning Framework](https://miro.medium.com/max/1400/1*CjLFVeYssOIJaeijrxPHPg.png)\n",
    "\n",
    "Er zijn twee nieuwe termen benoemd ten opzichten van de mario analogie:\n",
    "* *Sparse Rewards*: RL problemen hebben vaak te maken met een belonging die 1-to-many staan (bijv. 1x een reward op elke 1000 zetten/spellen). Deze worden dan ook *Sparse rewards* genoemd en vormt een van de uitdagingen van RL ten opzichten van traditionele ML. In Mario en PacMan zijn het halen van levels voorbeelden van *Sparse Rewards*.\n",
    "* *Imediate/shaped Rewards*: In bepaalde environments kunnen *imediate/shaped rewards* worden gegeven. In Mario zouden dit muntjes zijn en vijanden verslaan, in PacMan zou dit zijn wanneer de bolletjes en spookjes opgegeten worden. Dit proces van additionele beloningen toekennen kan er voor zorgen dat agent beter leert wat deze moet doen om tot een goed resultaat te komen. Hier schuilt echter ook het gevaar, het algoritme kan te gretig worden en daardoor de voorkeur gaan geven aan vroegtijdige belonginge ten opzichten van de echte beloning aan het einde van het spel.\n",
    "\n",
    "### 1.3 RL formules\n",
    "\n",
    "Om de agent te trainen staan een aantal functies centraal. Namelijk de *Reward function*, *Value funtion* en de *Policy function*. In deze paragraaf bespreken we deze functies in het kort om een beeld te krijgen van wat er onder de motorkap gebeurt. De functies zijn vrij technisch en niet vereist om RL toe te passen. Maar om de kern van RL te doorgronden zijn deze formules wel belangrijk.\n",
    "\n",
    "#### 1.3.1 *Reward function*\n",
    "de beloning die een agent krijgt voor een bepaalde actie. deze functie kan als onderstaande worden opgeschreven:\n",
    "\n",
    "$$r_{t} = R(s_{t},a_{t},s_{t+1})$$\n",
    "hierin is *r* de beloning, *t* de huidige tijdstap, *R()* een specifieke beloningsfunctie, *s* de huidige obsertvatie/state, *a* de genomen actie. De formule stelt dus \"De huidige beloning, is gelijk aan de functie van de huidige staat, de huidige actie en de staat/observatie van de volgende tijdstap.\" Deze functie wordt vaak versimpeld naar $R(s_{t})$ (de huidige staat) of $R(s_{t}, a_{t})$ (de huidige actie/staat combinatie).\n",
    "\n",
    "Het doel is dus om de cummulatieve beloning over een periode te maximalizeren. De benadering van deze cummulatieve beloning bepaalt de functie van R() hieronder worden twee varianten van de functie besproken.<br> \n",
    "\n",
    "allereerst: **finite-horizon undiscounted return** waarin de functie puur een opsomming is van de belongingen over *T* tijdsstappen:\n",
    " $$R(\\tau) = \\sum_ {t=0}^T\\ r_{t}$$\n",
    " <br>\n",
    "\n",
    "De tweede variant heet: **infinite-horizon discounted return**, hierbij worden de beloningen van alle tijdstappen meegenomen, echter hoeverder de tijdstap (en beloning) in de toekomst ligt, hoe groter de discount. Dit betekent dat een beloning die over 100*t* plaatsvindt minder waard is dan de beloning over 1*t*:\n",
    "\n",
    "$$R(\\tau) = \\sum_ {t=0}^\\infty \\ r_{t} $$\n",
    "<br>\n",
    "\n",
    "$\\tau$ (*tau*) is in dit geval een opeenvolging van staten en acties: $\\tau = (S_{0},a_{0},S_{1},a_{0}, \\cdots)$ dit wordt ookwel *trajectory, episodes, of rollouts* genoemd. De start staat is hierbij vaak willekeurig gekozen. de volgende staten worden bepaald door de genomen actie van de agent.\n",
    "\n",
    "#### 1.3.2 *Value function*\n",
    "De value function kijkt naar de huidige *value* van een staat/observatie. In dit geval wordt met *value* het volgende bedoeld: de verwachte beloning voor de huidige staat of staat-actie combinatie wanneer één policy gekozen wordt en hier niet meer van afgeweken wordt. Van deze value functies zijn er 4 belangrijke te benoemen:\n",
    "\n",
    "1. **On-Policy Value Function**: $V^{\\pi}(s)$ geeft de verwachte return (cummulatieve beloning) wanneer je start in staat *s* en altijd policy $\\pi$ aanhoudt:\n",
    "$$V^{\\pi}(s) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "2. **On-Policy Action-Value Function**: $Q^{\\pi}(s,a)$ de verwachte return wanneer een willekeurige actie wordt genomen vanaf de eerste state en daarop volgend alleen maar de gekozen policy inzet:\n",
    "$$Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "3. **Optimal Value Function**: $V^{*}(s)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen:\n",
    "$$V^{*}(s) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "4. **Optimal Action-Value Function**: $Q^{*}(s,a)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen na eerst eenmalig een willekeurige actie te hebben gekozen:\n",
    "$$Q^{*}(s,a) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "#### 1.3.3 Policy function\n",
    "De policy function (of agent function). Is een functie die aangeeft welke actie een agent neemt. Deze agent en policy termen worden vaak doorelkaar gebruikt. Zo kan policy in: \"de policy probeert de beloning te maximalizeren\" vervangen worden door \"de agent\". Er zijn twee variante van de policy function, namelijk:\n",
    "\n",
    "1. deterministisch, toeval speelt hier geen rol, zijn de begin voorwaarden gelijk dan zal de uitkomst ook gelijk zijn. In dit geval wordt de policy function aangegeven als $a_{t} = \\mu(s_{t})$. Hier staat \"de huidige actie wordt bepaald door de functie $\\mu()$ (*mu*) gegeven de huidige staat. \n",
    "<br><br>\n",
    "\n",
    "2. Stochastisch, in dit geval speelt toeval een rol. De uitkomst is van te voren nog niet bekend. Hier wordt gekeken naar de kans van een actie - vervolg staat . De functie kan opgeschreven worden als: $a_{t} \\sim \\pi(a|s_{t})$ wat betekent dat \"de actie evenredig is met de policy functie $\\pi()$ van een kans gegeven staat *t*.\" ofwel \"kies de actie die de meeste waarschijnlijkheid heeft om te leiden tot de volgende beste staat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL in kleine stapjes\n",
    "In de voorgaande sectie is RL op theoretisch niveau beschreven. Nu is het tijd om daadwerlijk met RL aan de slag te gaan. In dit project staan 2 libraries centraal:\n",
    "1. [gym](https://gym.openai.com/docs/): Gym is een library geschreven door OpenAI en bevat de bouwstenen om een agent binnen een environment te laten opereren.\n",
    "2. [Stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/): Bevat verschillende algoritmes om een agent te trainen en volgens o.a. de eerder genoemde formules te laten handelen.\n",
    "<br><br>\n",
    "\n",
    "Verder zijn er nog een aantal andere libraries die worden ingeladen, namelijk:\n",
    "* os: is nodig om systeem operaties te doen (zoals locaties van files ophalen)\n",
    "* pandas: voor het werken met data in tabel format\n",
    "* seaborn: voor visualisatie\n",
    "* matplotlib: ligt onder seaborn en brengt extra functionaliteiten mee\n",
    "* numpy: ligt onder pandas en brengt extra functionaliteiten\n",
    "* torch: Deeplearning library, wordt in dit geval gebruikt om CUDA GPU's te benutten voor RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python 3.9.7 is nodig voor het gebruik van deze modules\n",
    "# !pip install stable-baselines3[extra] ## docummentatie voor deze module die RL algoritmes bevat: https://stable-baselines3.readthedocs.io/en/master/\n",
    "# !pip install gym\n",
    "# !pip install pyglet ## extra dependency voor OpenAI gym mocht deze ontbreken\n",
    "!pip list -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying adventure.bin from Roms\\Roms\\ROMS\\Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\adventure.bin\n",
      "copying air_raid.bin from Roms\\Roms\\ROMS\\Air Raid (Men-A-Vision) (PAL) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\air_raid.bin\n",
      "copying alien.bin from Roms\\Roms\\ROMS\\Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\alien.bin\n",
      "copying amidar.bin from Roms\\Roms\\ROMS\\Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\amidar.bin\n",
      "copying assault.bin from Roms\\Roms\\ROMS\\Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\assault.bin\n",
      "copying asterix.bin from Roms\\Roms\\ROMS\\Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\asterix.bin\n",
      "copying atlantis.bin from Roms\\Roms\\ROMS\\Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\atlantis.bin\n",
      "copying bank_heist.bin from Roms\\Roms\\ROMS\\Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\bank_heist.bin\n",
      "copying battle_zone.bin from Roms\\Roms\\ROMS\\Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\battle_zone.bin\n",
      "copying beam_rider.bin from Roms\\Roms\\ROMS\\Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\beam_rider.bin\n",
      "copying berzerk.bin from Roms\\Roms\\ROMS\\Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\berzerk.bin\n",
      "copying bowling.bin from Roms\\Roms\\ROMS\\Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\bowling.bin\n",
      "copying boxing.bin from Roms\\Roms\\ROMS\\Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\boxing.bin\n",
      "copying breakout.bin from Roms\\Roms\\ROMS\\Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\breakout.bin\n",
      "copying carnival.bin from Roms\\Roms\\ROMS\\Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\carnival.bin\n",
      "copying centipede.bin from Roms\\Roms\\ROMS\\Centipede (1983) (Atari - GCC) (CX2676) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\centipede.bin\n",
      "copying chopper_command.bin from Roms\\Roms\\ROMS\\Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\chopper_command.bin\n",
      "copying crazy_climber.bin from Roms\\Roms\\ROMS\\Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\crazy_climber.bin\n",
      "copying defender.bin from Roms\\Roms\\ROMS\\Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\defender.bin\n",
      "copying demon_attack.bin from Roms\\Roms\\ROMS\\Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\demon_attack.bin\n",
      "copying donkey_kong.bin from Roms\\Roms\\ROMS\\Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\donkey_kong.bin\n",
      "copying double_dunk.bin from Roms\\Roms\\ROMS\\Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\double_dunk.bin\n",
      "copying elevator_action.bin from Roms\\Roms\\ROMS\\Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\elevator_action.bin\n",
      "copying enduro.bin from Roms\\Roms\\ROMS\\Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\enduro.bin\n",
      "copying fishing_derby.bin from Roms\\Roms\\ROMS\\Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\fishing_derby.bin\n",
      "copying freeway.bin from Roms\\Roms\\ROMS\\Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\freeway.bin\n",
      "copying frogger.bin from Roms\\Roms\\ROMS\\Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\frogger.bin\n",
      "copying frostbite.bin from Roms\\Roms\\ROMS\\Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\frostbite.bin\n",
      "copying galaxian.bin from Roms\\Roms\\ROMS\\Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\galaxian.bin\n",
      "copying gopher.bin from Roms\\Roms\\ROMS\\Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\gopher.bin\n",
      "copying gravitar.bin from Roms\\Roms\\ROMS\\Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\gravitar.bin\n",
      "copying hero.bin from Roms\\Roms\\ROMS\\H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\hero.bin\n",
      "copying ice_hockey.bin from Roms\\Roms\\ROMS\\Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\ice_hockey.bin\n",
      "copying jamesbond.bin from Roms\\Roms\\ROMS\\James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\jamesbond.bin\n",
      "copying journey_escape.bin from Roms\\Roms\\ROMS\\Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\journey_escape.bin\n",
      "copying kaboom.bin from Roms\\Roms\\ROMS\\Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\kaboom.bin\n",
      "copying kangaroo.bin from Roms\\Roms\\ROMS\\Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\kangaroo.bin\n",
      "copying keystone_kapers.bin from Roms\\Roms\\ROMS\\Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\keystone_kapers.bin\n",
      "copying king_kong.bin from Roms\\Roms\\ROMS\\King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\king_kong.bin\n",
      "copying koolaid.bin from Roms\\Roms\\ROMS\\Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\koolaid.bin\n",
      "copying krull.bin from Roms\\Roms\\ROMS\\Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\krull.bin\n",
      "copying kung_fu_master.bin from Roms\\Roms\\ROMS\\Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\kung_fu_master.bin\n",
      "copying laser_gates.bin from Roms\\Roms\\ROMS\\Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\laser_gates.bin\n",
      "copying montezuma_revenge.bin from Roms\\Roms\\ROMS\\Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\montezuma_revenge.bin\n",
      "copying mr_do.bin from Roms\\Roms\\ROMS\\Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\mr_do.bin\n",
      "copying ms_pacman.bin from Roms\\Roms\\ROMS\\Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\ms_pacman.bin\n",
      "copying name_this_game.bin from Roms\\Roms\\ROMS\\Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\name_this_game.bin\n",
      "copying pacman.bin from Roms\\Roms\\ROMS\\Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pacman.bin\n",
      "copying phoenix.bin from Roms\\Roms\\ROMS\\Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\phoenix.bin\n",
      "copying video_pinball.bin from Roms\\Roms\\ROMS\\Pinball (AKA Video Pinball) (Zellers).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\video_pinball.bin\n",
      "copying pitfall.bin from Roms\\Roms\\ROMS\\Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pitfall.bin\n",
      "copying pooyan.bin from Roms\\Roms\\ROMS\\Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pooyan.bin\n",
      "copying private_eye.bin from Roms\\Roms\\ROMS\\Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\private_eye.bin\n",
      "copying qbert.bin from Roms\\Roms\\ROMS\\Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\qbert.bin\n",
      "copying riverraid.bin from Roms\\Roms\\ROMS\\River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\riverraid.bin\n",
      "copying road_runner.bin from patched version of Roms\\Roms\\ROMS\\Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\road_runner.bin\n",
      "copying robotank.bin from Roms\\Roms\\ROMS\\Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\robotank.bin\n",
      "copying seaquest.bin from Roms\\Roms\\ROMS\\Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\seaquest.bin\n",
      "copying sir_lancelot.bin from Roms\\Roms\\ROMS\\Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\sir_lancelot.bin\n",
      "copying skiing.bin from Roms\\Roms\\ROMS\\Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\skiing.bin\n",
      "copying solaris.bin from Roms\\Roms\\ROMS\\Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\solaris.bin\n",
      "copying space_invaders.bin from Roms\\Roms\\ROMS\\Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\space_invaders.bin\n",
      "copying star_gunner.bin from Roms\\Roms\\ROMS\\Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\star_gunner.bin\n",
      "copying surround.bin from Roms\\Roms\\ROMS\\Surround (32 in 1) (Bit Corporation) (R320).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\surround.bin\n",
      "copying tennis.bin from Roms\\Roms\\ROMS\\Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\tennis.bin\n",
      "copying time_pilot.bin from Roms\\Roms\\ROMS\\Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\time_pilot.bin\n",
      "copying trondead.bin from Roms\\Roms\\ROMS\\TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\trondead.bin\n",
      "copying tutankham.bin from Roms\\Roms\\ROMS\\Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\tutankham.bin\n",
      "copying up_n_down.bin from Roms\\Roms\\ROMS\\Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\up_n_down.bin\n",
      "copying venture.bin from Roms\\Roms\\ROMS\\Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\venture.bin\n",
      "copying pong.bin from Roms\\Roms\\ROMS\\Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pong.bin\n",
      "copying wizard_of_wor.bin from Roms\\Roms\\ROMS\\Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\wizard_of_wor.bin\n",
      "copying yars_revenge.bin from Roms\\Roms\\ROMS\\Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\yars_revenge.bin\n",
      "copying zaxxon.bin from Roms\\Roms\\ROMS\\Zaxxon (1983) (Coleco) (2454) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\zaxxon.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "# from gym import wrappers # gebruiken om geen visualisaties te doen van de environment\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C # Specifieke algoritme voor RL\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # wrapper voor environment beheersing\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # beoordeling van agent policies\n",
    "from stable_baselines3.common.vec_env import VecFrameStack # gebruikt voor visual based learners om frames opelkaar te leggen\n",
    "from stable_baselines3.common.env_util import make_atari_env #specifieke wrapper voor atari environments\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CheckpointCallback # callbacks die aangeroepen worden tijdens learning\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# nodig om atari spellen te mogen/kunnen gebruiken\n",
    "!python -m atari_py.import_roms \"Roms\\Roms\\ROMS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Environments\n",
    "\n",
    "OpenAI geeft met de gym library toegang tot het benutten van RL environments. Zoals eerder uitgelegd is de environment een omgeving waar de agent zicht in beweegt, observaties uit haalt en beloningen binnen krijgt.<br><br>\n",
    "\n",
    "Gym environments zijn op te delen in een aantal types, namelijk:\n",
    "* **Box:** n-dimensionele tensor, die een range van waarden bevat --> ```Box(0, 1, shape=(3,3))```\n",
    "* **Discrete:** set van discrete waarden --> ```Discrete(3)```\n",
    "* **Tuple:** Een Tuple van andere spaces --> ```Tuple((Discrete(3), Box(0, 1, shape=(3,3))))```\n",
    "* **Dict:** Dictionary van spaces --> ```Dict({('height': Discrete(3), 'speed':Box(0, 1, shape=(3,3)))})```\n",
    "* **MultiBinary:** One hot encoded binary waarde --> ```MultiBinary(4)``` --> [0,0,0,0] of [0,1,0,1] etc.\n",
    "* **MultiDiscrete:** Meerdere discrete waarde --> ```MultiDiscrete([2,5,2])```\n",
    "\n",
    "In onderstaande code gaan we de action space en de observation space ophalen van de ```CartPole-v0``` omgeving. Deze twee spaces worden na de code cell verder uigelegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Action space van CartPole-v0 environment: Discrete(2)\n",
      "    Observation space van CartPole-v0 environment: Box(4,)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# inladen van environment\n",
    "environment_name ='CartPole-v0'\n",
    "env = gym.make(environment_name)\n",
    "\n",
    "# Test om te zien wat de environment bevat\n",
    "print(\n",
    "    f'''\n",
    "    Action space van {environment_name} environment: {env.action_space}\n",
    "    Observation space van {environment_name} environment: {env.observation_space}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CartPole-v0\n",
    "Voor de eerste test wordt de CartPole-v0 environment gebruikt. Deze environment bestaat (zoals hierboven te zien) uit:\n",
    "* Action space Discrete(2), waarbij 0 de cart naar links stuurt en 1 de cart naar rechts stuurt.\n",
    "* Observation space, het scherm dat de speler/agent ziet is een Box(), een array van in dit geval 4 elementen. elk van deze vier elementen hebben een lower-bound en een upper-bound. Elk element is van het type float32. Hieronder wordt er nog wat dieper ingegaan op de observation space.\n",
    "\n",
    "### 2.2 De observation space\n",
    "```\n",
    "env.observation_space: \n",
    "    Box(\n",
    "        [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38],\n",
    "        [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38],\n",
    "        (4,),\n",
    "        float32)\n",
    "```\n",
    "\n",
    "dit kan als volgt geïnterpeteerd worden: <br>\n",
    "env.observation_space[0] = [-4.8000002e+00] en [4.8000002e+00] --> dit zijn de min en max posities van het wagentje<br>\n",
    "env.observation_space[1] = [-3.4028235e+38] en [3.4028235e+38] --> de velocity (snelheid) van het wagentje<br>\n",
    "env.observation_space[2] = [-4.1887903e-01] en [4.1887903e-01] --> hoek van de paal ten opzichten van het wagentje<br>\n",
    "env.observation_space[3] = [-3.4028235e+38] en [3.4028235e+38] --> velocity van de paal<br>\n",
    "\n",
    "binnen deze observation space wordt er per env.observation_space[*x*] een waarde teruggeven voor de agent om op te acteren. Zie bijvoorbeeld onderstaande snippit:\n",
    "```\n",
    "env.observation_space.sample()\n",
    "--> array([-1.1144775e-02,  2.3990196e+38,  2.5578210e-01,  3.1761389e+38],\n",
    "      dtype=float32)\n",
    "```\n",
    "\n",
    "Bovenstaande geeft dus de locatie van de wagen, de snelheid / richting van de wagen, de hoek van de paal en de snelheid/richting van de paal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Een simulatie doen\n",
    "\n",
    "We hebben dus een omgeving waaruit we informatie kunnen ophalen, en we hebben een action space waaruit de agent acties kan kiezen. Door middel van een een loop kunnen we een aantal keer het spel laten spelen door de computer.  <br>\n",
    "\n",
    "In onderstaande code gaan we 5 rondes (episodes) van het spel spelen. Elke episode wordt het spel gereset en beginnen we weer op het startpunt ```env.reset()```. We geven reseten ook de score en geven aan dat het spel nog niet klaar is. <br>\n",
    "\n",
    "In de inner loop draaien we het spel totdat de ```env.step()``` function aangeeft dat het spel afgelopen is. Deze step functie neemt een 'stap' in de omgeving door een actie uit te voeren en geeft hier voor informatie voor terug waaronder de reward. De actie kan random zijn ```.sample()``` of een keuze op basis van learnings door het model. De geïnformeerde keuze wordt later toeglicht. <br>\n",
    "\n",
    "Aan het einde van elke episode (outer loop) printen we de episode en de bijbehorende score. Zodra we klaar zijn met alle episodes maken we de omgeving schoon om problemen te voorkomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# het aantal keer dat de agent het spel speelt.\n",
    "EPISODES = 5\n",
    "\n",
    "\n",
    "for episode in range(0, EPISODES): # loop over de episodes.\n",
    "    state = env.reset() # stel de omgeving opnieuw in.\n",
    "    done = False # zodra het spel is afgelopen verandert dit naar done, en begint dit op false.\n",
    "    score = 0 # de score die onze agent moet verhogen.\n",
    "\n",
    "    while not done: # not done == not False == True.\n",
    "        env.render() #render het spel in een omgeving. normaal gesproken staat dit uit voor snelheid.\n",
    "        action = env.action_space.sample() # van onze action space kies 1 actie random (links of rechts).\n",
    "        n_state, reward, done, info = env.step(action) # neem een stap (frame/actie), dit returned 4 waarden. een nieuwe Box(), de beloning, False/True (het spel is afgelopen), en eventueel extra info.\n",
    "        score += reward # tel de beloning voor deze actie op bij de score van deze episode.\n",
    "    print(f'Episode: {episode+1} | Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Algoritmes\n",
    "\n",
    "Er zijn diverse algoritmes beschikbaar. Binnen dit notebook wordt gebruik gemaakt van Model-Free RL. dit zijn algorithmes die zich minder/niet bezig houden met het voorspellen van de volgende state maar puur kijken naar een een set regels die de agent maakt op basis van ervaring. Zo kan de agent een regel opstellen dat het wagentje naar links moet als de stok een beetje links kantelt. In onderstaande afbeelding zijn de diverse opties te zien. In dit geval ligt de focus op PPO (Proximal Policy Optimization) dit is een policy algoritme die minder kans heeft om vast te komen zitten in een minima. Normaal gesproken leert policy based algoritmes van de huidige staat en kiest een nieuwe policy. Met PPO gebeurt dit ook maar wordt het verschil tussen policy *p<sub>t</sub>* en *p<sub>t-1</sub>* klein gehouden. <br>\n",
    "\n",
    "![algorithmes](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Metrics\n",
    "Voor elk type algoritme zijn diverse metrics beschikbaar. deze kunnen in 4 categoriën worden opgedeeld:\n",
    "* Evaluation: beschrijving van de lengte en reward voor een episode\n",
    "* Time: alles wat betreft tijd besteding\n",
    "* Loss: loss functie om het model te scoren\n",
    "* Overig: deze worden toeglicht wanneer deze aanbod komen\n",
    "\n",
    "De metrics worden in apparte mappen opgeslagen als log bestanden en zijn uit te lezen op diverse manieren. In dit notebook wordt gekeken naar de tensorboard visualisaties en .npz files die zullen worden omgezet naar een dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Een agent maken\n",
    "\n",
    "Het maken van een simpele agent met Stable_Baselines3 is makkelijk om te doen. Met twee regels code kan een agent ingesteld en getrained worden. De environment voorbereiden kost verder nog 2 regels code. Hierdoor is een base agent binnen 4 a 5 regels code klaar. In onderstaande cellen worden een aantal extra stappen toegevoegd naast hier boven genoemde 5 regels code. De volledige code in onderstaande snippit wordt eerst uitgelegd zodat duidelijk is wat er gebeurt:\n",
    "\n",
    "```\n",
    "log_path = os.path.join('Training', 'Evaluations')\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "training_path = os.path.join('Training', 'Training')\n",
    "\n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose= 1,\n",
    "    tensorboard_log= log_path\n",
    "    )\n",
    "\n",
    "model.learn(total_timesteps= 20000, eval_log_path = training_path)\n",
    "model.save(PPO_path)\n",
    "```\n",
    "\n",
    "Allereerst worden 3 bestandslocaties opgeslagen in variabele. Deze locaties zijn nodig voor het opslaan van tensorboard logging informatie (log_path), opslaan van het model (PPO_path) en opslaan van numpy trainings informatie (training_path).\n",
    "\n",
    "In de volgende 2 regels, wordt een environment gemaakt zoals we eerder hebben gedaan. Daarna zetten we de environment om in een vector. Voor bepaalde modellen is het goed om environments te stacken (de agent op 2 of meer environments tegelijk te trainen). Voor simpele omgevingen zoals de CartPole-v0 omgeving is het niet nodig om omgevingen te stacken. Wel heeft het PPO model een vector nodig vandaar dat we de environment \"wrappen\" in een dummy vector (vector met een laag). \n",
    "\n",
    "in de 6e regel initialiseren we een agent. We geven de agent de variabele naam \"model\" en stellen een aantal parameters in. We kiezen een Multilayered Perceptron (MlP) als onderliggend netwerk. Dit is een heel simpele Neurale Network waarbij getallen aan de ene kant worden ingevoerd (de dummy vector van de omgeving) aan het einde komt er een output uit (de actie). Het network zal vervolgens aangepast worden zodat de verzameling van input (observaties) en output (actie) de best mogelijk reward geven. <br><br>\n",
    "![een voorbeeld van een MlP](https://www.researchgate.net/profile/Mohamed-Zahran-16/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n",
    "<br><br>\n",
    "\n",
    "we geven ook een locatie voor onze tensorboard mee. Tensorboard files bevatten informatie over het model tijdens / na trainen. Deze informatie wordt met ```verbose = 1``` beperkt, wanneer 0 wordt ingevoerd wordt er niks gelogged en bij 2 wordt er veel informatie gelogged (dit kan veel ruimte kosten).\n",
    "\n",
    "In de laatste 2 regels trainen we de agent om te handelen op basis van bepaalde regels. Dit doen we voor 20.000 timesteps (episodes / games) en we slaan training informatie op in de aangeven locatie. Vervolgens kan het model worden opgeslagen om later opgehaald te worden zodra deze nodig is.\n",
    "\n",
    "In de onderstaande cellen worden deze stappen achtereen uitgevoerd. en wordt een bestaand model ingeladen als deze aanwezig is. Anders wordt er een nieuw model getrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "model loaded!\n",
      "Episode: 1 | Score: [200.]\n",
      "Episode: 2 | Score: [200.]\n",
      "Episode: 3 | Score: [200.]\n",
      "Episode: 4 | Score: [200.]\n",
      "Episode: 5 | Score: [200.]\n"
     ]
    }
   ],
   "source": [
    "# bepaal de locatie voor het opslaan van logs en models\n",
    "log_path = os.path.join('Training', 'Evaluations')\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "training_path = os.path.join('Training', 'Training')\n",
    "log_path\n",
    "\n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda:env]) # wrapper om de environment in een dummy vector te plaatsen\n",
    "\n",
    "# Kiezen van een model\n",
    "model = PPO(\n",
    "    'MlpPolicy', # MlpPolicy (Multilayer Perceptron), een neuraalnetwerk policy\n",
    "    env,\n",
    "    verbose= 1, # verbose = 0, 1, of 2 --> 1 is loggen ban belangrijkste gegevens.\n",
    "    tensorboard_log= log_path # waar de tensorboard file wordt opgeslagen\n",
    "    )\n",
    "\n",
    "try:\n",
    "    # als er een model aanwezig is wordt deze ingeladen\n",
    "    model = PPO.load(r'Training\\Saved Models\\PPO_Model_Cartpole.zip')\n",
    "    print('model loaded!')\n",
    "\n",
    "except:\n",
    "    # onbreekt dit model dan wordt een nieuw model getrained\n",
    "    print('model not found... training new model')\n",
    "    model.learn(total_timesteps= 20000, eval_log_path = training_path) # train het model voor x timesteps\n",
    "    model.save(PPO_path) # Sla het model op in de locatie: Training\\Saved Models \n",
    "    print(f'training finished, model saved to {PPO_path}')\n",
    "\n",
    "finally:\n",
    "    # Zodra een model is geladen of getrained wordt het model hieronder in de environment losgelaten (dit is hetzelfde als met de random sample van de environment)\n",
    "    for episode in range(0, 5):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action, _ = model.predict(obs) # Laat het model een actie uitvoeren op basis van de huidige observatie \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evalueren van model en tensorboard\n",
    "In de onderstaande cellen gaan we het model evalueren en de de geloggde gegevens in een tensorboard plaatsen. \n",
    "Voor nu zijn de belangrijkste metrics om in de gaten te houden de *average Reward* en *average episode length*.\n",
    "Aangezien deze niet aanwezig zijn in de tensorboard worden deze appart gevisualiseerd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalueer het model op basis van een aantal parameters (average punten, standard deviation) --> max punten = 200.\n",
    "evaluate_policy(model, env, n_eval_episodes= 5, render=True) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om een agent in een specifieke environment te draaien\n",
    "def run_agent(env, model = False, episodes= 5, render = False):\n",
    "    '''\n",
    "    runs an agent (model) through the environment for n episodes, if render is set to False no window is rendered.\n",
    "    at the end of the runs a tensorlog file is written to a predetermined directory\n",
    "    '''\n",
    "    for episode in range(0, episodes): \n",
    "        obs = env.reset() \n",
    "        done = False \n",
    "        score = 0 \n",
    "\n",
    "        while not done:\n",
    "            if render: \n",
    "                env.render(mode='human') \n",
    "            if model:\n",
    "                action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draai nu de zelfde environment met een getrainde agent\n",
    "run_agent(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorlog ophalen van een locatie\n",
    "train_log_path = os.path.join(log_path, 'PPO_3')\n",
    "\n",
    "#klik de play button hieronder\n",
    "%tensorboard --logdir= train_log_path # klik de play button hierboven voor de Tensorboard evaluations grafieken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evalueren op basis van score\n",
    "\n",
    "In de onderstaande cellen wordt de agent gescoord op zijn vaardigheden. Gezien de omgeving redelijk simpel is vindt de agent over het algemeen met gemak een optimale oplossing na verloop van het trainings proces.\n",
    "\n",
    "De eerst volgende 2 cellen beoordelen een volledig getrained model. Het is duidelijk dat hier weinig interessants uitkomt. Het model scoort namelijk in 4 van de 5 episodes maximaal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad het model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "episodes = 5 \n",
    "lst_score = []\n",
    "lst_episode = []\n",
    "\n",
    "#Draai het model\n",
    "for episode in range(0, episodes): \n",
    "    obs = env.reset() \n",
    "    done = False \n",
    "    score = 0 \n",
    "\n",
    "    while not done: \n",
    "        # env.render() \n",
    "        action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    lst_score.append(score[0])\n",
    "    lst_episode.append(episode + 1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"episode\": lst_episode, \"score\": lst_score}\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "\n",
    "sns.lineplot(x= df[\"episode\"], y=df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evalueren van meerdere agents\n",
    "\n",
    "Hieronder worden 2 nieuwe agents getrained. Wordt de evaluatie van de agent tijdens het trainen gelogged naar een .npz file. In deze log komen alleen de scores en de duratie van de episodes terug.\n",
    "\n",
    "Door twee verschillende modellen te trainen kunnen we het effect van PPO policy duidelijk terug zien. Het is namelijk zo dat de PPO minimale aanpassingen doet in de voorgaande strategie (timestep). Deze afwijkingen zorgen doorgaans voor een langzaam stijgende lijn en voorkomt in de meeste gevallen een foute strategie door een te grote policy wijziging. In de 3e cell hieronder is dit ook te zien. PPO zorgt voor een doorgaans stijgende lijn en afwijkingen in de policy worden minimaal gemaakt hierdoor zie je dat een agent niet snel in een local optimum komt of naar een minima toe beweegt. Hier moet wel gemeld worden dat dit een zeer simpele omgeving betreft waardoor het niet vreemd is dat agents in deze omgeving een globaal optimum bereiken.\n",
    "\n",
    "![model1](visualisatie\\model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nieuw model voor loggen trainings informatie\n",
    "# env = wrappers.Monitor(env, video_callable=False ,force=True)\n",
    "model2 = PPO('MlpPolicy', env)\n",
    "model2.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# functie voor het laden en transformeren van npz files\n",
    "def npz_to_dataframe(npz):\n",
    "    '''\n",
    "    Takes an npz file object loaded using numpy.load(path to .npz file) and returns a formatted dataframe\n",
    "    '''\n",
    "    # maak een data frame van de npz file op basis van de kolommen\n",
    "    df = pd.DataFrame.from_dict({col: npz[col] for col in npz.files}, orient='index')\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # transpose de dataframe gezien deze verkeerd om staat\n",
    "    df =df.T\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # bereken de gemiddeldes voor iedere cel\n",
    "    df = df.applymap(np.mean)\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_training_score = npz_to_dataframe(npz)\n",
    "df_training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een lineplot om de prestaties van het model na diverse timesteps te zien\n",
    "fig = sns.lineplot(x= df_training_score[\"timesteps\"], y=df_training_score[\"results\"])\n",
    "display(fig)\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vergelijk de training met een tweede model\n",
    "model3 = PPO('MlpPolicy', env)\n",
    "model3.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)\n",
    "\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# Roep de eerder gemaakte functie aan\n",
    "df_training_score2 = npz_to_dataframe(npz)\n",
    "\n",
    "#voeg aan beide data frames een kolom toe met de agent naam\n",
    "df_training_score[\"model\"] = \"model 1\"\n",
    "df_training_score2[\"model\"] = \"model 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder worden de twee agents tijdens het trainen met elkaar vergeleken. Het is goed te zien dat de twee agents andere start punten hebben. Door kleine aanpassingen in de strategie kan de score in het begin sterk stijgen/dalen om vervolgens na een redelijk stijgende lijn in een bepaald optimum te settlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_training_score.append(df_training_score2)\n",
    "fig = sns.lineplot(x= df_compare[\"timesteps\"], y=df_compare[\"results\"], hue= df_compare['model'])\n",
    "display(fig)\n",
    "\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1vsmodel2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Callback en earlystopping\n",
    "Vooral bij grote modellen kan tijdens het trainen het model onstabiel worden. Om dit te verhelpen kunnen we het trainen stoppen wanneer dit een bepaalde grenswaarden bereikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_path = \".\\Training\\Saved Models\"\n",
    "\n",
    "# StopTrainingOnRewardThreshold --> voorwaarden wanneer het trainen moet stoppen\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold= 200, verbose= 1)\n",
    "\n",
    "# EvalCallback --> evalueert het trainingproces en controlleert de treshhold\n",
    "eval_callback = EvalCallback(env, callback_on_new_best= stop_callback, eval_freq= 1000, best_model_save_path= best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Andere policies en algoritmes\n",
    "\n",
    "In de onderstaande cellen gaan we verder met het opstellen van twee alternatieve modellen. \n",
    "\n",
    "De eerste stap gaat zijn om een PPO model op te stellen met een zelf samengesteld Neural Network. Hoewel Stable Baselines goed afgestelde modellen heeft is het goed om te weten dat deze nog verder verfijnd kunnen worden. In dit geval maken we gebruik van een neural network voor de agent/decision functie (*pi*) en de value function (*vf*). beider netwerken zullen bestaan uit 4 lagen met iedere laag 128 neuronen.\n",
    "\n",
    "In de tweede cel laden we kort een ander algorithme, DQN (Deep Q Network). Dit model is voor de volgende spaces te gebruiken:\n",
    "\n",
    "|Spaces | Action | Observation |\n",
    "|---|---|---|\n",
    "|  Discrete | ✔️ | ✔️ |\n",
    "|  Box | ❌ | ✔️ |\n",
    "|  MultiDiscrete | ❌ | ✔️ |\n",
    "|  MultiBinary | ❌ | ✔️ |\n",
    "\n",
    "Dit model lijkt zich dus goed te lenen voor het Carpole probleem. Hier hebben we namelijk te maken met een **discrete action space** en een **box observation space**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Een neuralnetwork architectuur voor agent (pi) en valuefunction (vf)\n",
    "NN_arch = [dict(pi= [128,128,128,128], vf= [128,128,128,128] )]\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch': NN_arch})\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Project 1: Breakout\n",
    "\n",
    "In deze sectie gaan we proberen het spel Breakout te halen. Breakout is een simpel spel waarbij we met een peddle onder aan het scherm proberen een balletje te raken om hiermee blokken boven in het scherm kappot te maken. Om dit tot een succes te laten verlopen, doorlopen we ongeveer dezelfde stappen als in de voorgaande secties:\n",
    "1. Omgeving inladen\n",
    "2. Omgeving verkennen\n",
    "3. Geschikte algoritmes zoeken\n",
    "4. Agent trainen\n",
    "5. Beoordelen agent\n",
    "6. Hyperparamter tuning????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0.1 Functies klaarzetten\n",
    "Om met deze omgeving aan de slag te gaan maken we eerst een aantal functies aan die gebruikt kunnen worden. De functies worden zoveel mogelijk opgeknipt zodat het duidelijk is wat een functie doet zonder enige vorm van dubbelzinnigheid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_info(env):\n",
    "    \"\"\"returns information about the environments type, observation an daction space and action meanings.\n",
    "\n",
    "    arguments:\n",
    "        env -- environment created using gym.make()\n",
    "    \n",
    "    Return:\n",
    "        environment information printed to consol\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "    f'''\n",
    "    observation space type: {type(env.observation_space)}\n",
    "    observation space shape: {env.observation_space.shape}\n",
    "    action space: {env.action_space}\n",
    "    aanwezige actions: {env.unwrapped.get_action_meanings()}\n",
    "    '''\n",
    ")\n",
    "\n",
    "def npz_to_dataframe(npz):\n",
    "    '''\n",
    "    Takes an npz file object loaded using numpy.load(path to .npz file) and returns a formatted dataframe\n",
    "\n",
    "    Arguments:\n",
    "        npz -- .npz file like object containing training information of specific trained model\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # maak een data frame van de npz file op basis van de kolommen\n",
    "    df = pd.DataFrame.from_dict({col: npz[col] for col in npz.files}, orient='index')\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # transpose de dataframe gezien deze verkeerd om staat\n",
    "    df =df.T\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # bereken de gemiddeldes voor iedere cel\n",
    "    df = df.applymap(np.mean)\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "    \n",
    "    return df\n",
    "\n",
    "def device_status():\n",
    "    '''\n",
    "    prints torch cuda device information to consol\n",
    "    '''\n",
    "\n",
    "    print(\n",
    "        f'''\n",
    "        Cuda enabled device: {torch.cuda.is_available()}\n",
    "        Select device index: {torch.cuda.current_device()}\n",
    "        Device location: {torch.cuda.device(0)}\n",
    "        Number of devices: {torch.cuda.device_count()}\n",
    "        Device name: {torch.cuda.get_device_name(0)}\n",
    "        Allocated memory: {round(torch.cuda.memory_allocated(0)/1024**3,1)} 'GB')\n",
    "        Cached memory: {round(torch.cuda.memory_reserved(0)/1024**3,1)} 'GB')\n",
    "        '''\n",
    "    )\n",
    "\n",
    "def run_agent(env, model = False, episodes= 5, render = False):\n",
    "    '''\n",
    "    runs an agent (model) through the environment for n episodes, if render is set to False no window is rendered.\n",
    "    at the end of the runs a tensorlog file is written to a predetermined directory\n",
    "\n",
    "    arguments:\n",
    "        env -- supply environment where the agent should be run\n",
    "        model (optional) -- supply a trained model/agent to run through the environment\n",
    "        episodes (optional) -- set the number of episodes for which to train the model\n",
    "        render (optional) -- set to True to show the agent in a pop-up playing the environment\n",
    "\n",
    "    return: prints episode and score to console\n",
    "\n",
    "    '''\n",
    "\n",
    "    for episode in range(0, episodes): \n",
    "        obs = env.reset() \n",
    "        done = False \n",
    "        score = 0 \n",
    "\n",
    "        while not done:\n",
    "            if render: \n",
    "                env.render(mode='human') \n",
    "            if model:\n",
    "                action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 de omgeving\n",
    "\n",
    "We hebben dus te maken met een box observation space en een discrete action space. De observation space bestaat uit drie dimensies (Rood, Groen, Blauw waarde) en heeft een afmeting van 210 height x 160 width. Daarnaast bestaat de action space uit 4 discrete acties 'NOOP', 'FIRE', 'RIGHT', en 'LEFT'. Hierbij staat 'NOOP' voor het nemen van geen actie, 'FIRE' wordt gebruikt om het spel op te starten.\n",
    "\n",
    "Voor dit probleem is er dus een algoritme nodig dat gebruik kan maken van een een discrete action space. In de [documentatie van Stable Baselines](https://stable-baselines.readthedocs.io/en/master/guide/algos.html) is te zien dat er een aantal algoritmes in aanmerking komen, namelijk: A2C, ACER, ACKTR, DQN, HER, GAIL en PPO. Deze algoritmes kunnen allemaal omgaan met een discrete action space.\n",
    "\n",
    "We gaan om te beginnen gebruik maken van A2C, DQN, en PPO. Dit verkleind de scope en geeft een beeld van het effect dat diverse modellen zullen hebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPISODES = 5\n",
    "MODEL_PATH = 'Breakout_model\\Saved_Model'\n",
    "LOG_PATH = 'Breakout_model\\Logging'\n",
    "TRAIN_LOG_PATH = 'Breakout_model\\Training_logs'\n",
    "\n",
    "# controleren van eventuele GPU/CPU device\n",
    "device_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aanmaken environment en testen van een sample agent\n",
    "environment_name = 'Breakout-v0'\n",
    "env = gym.make(environment_name)\n",
    "env_info(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(env, render= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 De Agents\n",
    "We proberen dus een agent te trainen op het spel Breakout. Ook hebben we gezien dat de observation space een afbeelding is (rgb van 210 bij 160). Dit betekent dat het Mlp-model niet gaat werken, of in iedergeval veel minder effectief zal zijn. In dit geval proberen we een Convolutional Neural Network (CNN) te trainen. Dergelijke netwerken zijn beter in het behandelen van afbeeldingen.\n",
    "\n",
    "### 6.2.1 A2C\n",
    "[Advantage Actor Critic](https://openai.com/blog/baselines-acktr-a2c/), A2C, is een variatie op het A3C (Asynchronous Advantage Actor Critic) algoritme. A2C heeft de mogelijkheid om tegelijk meerdere workers in te zetten op de environment.\n",
    "\n",
    "A3C combineert een aantal belangrijke elementen:\n",
    "* een update aan het algorithme gebeurt na een vast aantal timesteps. Over dit segment worden estimators berkend op de return en advantage functies.\n",
    "* Lagen van de netwerk architectuur worden gedeeld tussen de policy function (pi) en de value function(vf)\n",
    "* Asynchronysche updates\n",
    "\n",
    "Waar A3C de werkers los van elkaar update doet wacht A2C met een update van de werkers totdat deze allemaal klaar zijn met een segment. Vervolgens wordt er een average van alle workers gepakt voor de update. Binnen Stable Baselines en OpenAI lijkt de performance van A2C een stuk beter dan A3C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probeer het model te laden \n",
    "try:\n",
    "    A2C_5K_model = A2C.load(r'Breakout_model\\Saved_Model\\a2c_Breakout_5k')\n",
    "    print('model loaded!')\n",
    "except (FileNotFoundError, AttributeError) as e:\n",
    "    # Parallel environments voor snellere training\n",
    "    train_env = make_atari_env('Breakout-v0', n_envs = 4, seed= 13)\n",
    "    train_env = VecFrameStack(train_env, n_stack=4)\n",
    "    A2C_5K_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_5k')\n",
    "    A2C_5K_model.learn(total_timesteps=5000, eval_log_path= TRAIN_LOG_PATH, eval_env=env)\n",
    "    A2C_5K_model.save(MODEL_PATH + \"\\\\a2c_Breakout_5k\")\n",
    "    print('model saved!')\n",
    "\n",
    "\n",
    "# maak een enkele environment voor visualisatie\n",
    "\n",
    "# eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "# eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "# evaluate_policy(A2C_5K_model, eval_env, n_eval_episodes= 5 )\n",
    "\n",
    "# env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "# env = VecFrameStack(env, n_stack=4)\n",
    "# A2C_25K_model = A2C('CnnPolicy', env, verbose=1)\n",
    "\n",
    "# evaluate_policy(A2C_25K_model, env, n_eval_episodes= 5, render= True )\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "# evaluate_policy(A2C_5K_model, eval_env, n_eval_episodes= 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALLEEN GEBRUIKEN ALS JE +1 DAG WILT WACHTEN OP DE TRAINING CYCLUS\n",
    "\n",
    "env = make_atari_env('Breakout-v0', n_envs = 8, seed= 13)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "# maak een callback aan elke x timesteps en save dit in \n",
    "checkpoint_callback = CheckpointCallback(save_freq=100000, save_path=r'Breakout_model\\Saved_Model', name_prefix='rl_model')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=r'Breakout_model\\best_model',log_path=r'Breakout_model\\Logging\\a2c_Breakout_tuned', eval_freq=500)\n",
    "\n",
    "\n",
    "tuned_A2C_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_tuned_5M', gamma= 0.0001, learning_rate=0.00005)\n",
    "tuned_A2C_model.learn(5000000, eval_log_path= TRAIN_LOG_PATH, eval_env=eval_env, callback=[checkpoint_callback, eval_callback])\n",
    "tuned_A2C_model.save(MODEL_PATH + \"\\\\a2c_Breakout_tuned_5M\")\n",
    "\n",
    "\n",
    "# evaluate_policy(tuned_A2C_model, eval_env, n_eval_episodes= 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(r'Breakout_model\\best_model\\best_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALLEEN GEBRUIKEN ALS JE 5 UUR WILT WACHTEN OP DE TRAINING CYCLUS\n",
    "\n",
    "# try:\n",
    "#     A2C_1M_model = A2C.load(MODEL_PATH + '\\\\a2c_Breakout_1M.zip')\n",
    "\n",
    "# except (FileNotFoundError, AttributeError) as e:\n",
    "#     env = make_atari_env('Breakout-v0', n_envs = 4, seed= 13)\n",
    "#     env = VecFrameStack(env, n_stack=4)\n",
    "#     model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_1M')\n",
    "#     model.learn(total_timesteps=1000000)\n",
    "#     model.save(MODEL_PATH + '\\\\a2c_Breakout_1M.zip')\n",
    "\n",
    "# env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "# env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# run_agent(env, A2C_1M_model, render= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 DQN\n",
    "\n",
    "Een DQN, Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = DQN.load(r'Breakout_model\\Saved_Model\\DQN_Breakout_100k')\n",
    "    print('model loaded!')\n",
    "except (FileNotFoundError, AttributeError) as e:\n",
    "    print('model not found... \\n training new model')\n",
    "    eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "    train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "    train_env = VecFrameStack(train_env, n_stack=1)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=r'Breakout_model\\Saved_Model', name_prefix='rl_model')\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path=r'Breakout_model\\best_model',log_path=r'Breakout_model\\Logging\\DQN_Breakout_tuned_100K', eval_freq=500)\n",
    "    model = DQN('CnnPolicy', train_env, verbose= 1, tensorboard_log= LOG_PATH + '\\\\DQN_Breakout_100k', device= 'cuda',  buffer_size= 200)\n",
    "    model.learn(total_timesteps=100000, eval_log_path= TRAIN_LOG_PATH, eval_env=eval_env)\n",
    "    model.save(MODEL_PATH + \"\\\\DQN_Breakout_100k\")\n",
    "    print('model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for episode in range(0, EPISODES): \n",
    "  done = False \n",
    "  score = 0 \n",
    "\n",
    "  while not done: \n",
    "      # env.render() \n",
    "      action, _ = model.predict(obs, deterministic=True)\n",
    "      obs, reward, done, info = env.step(action)\n",
    "      score += reward\n",
    "  print(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# Roep de eerder gemaakte functie aan\n",
    "df_training_score2 = npz_to_dataframe(npz)\n",
    "\n",
    "#voeg aan beide data frames een kolom toe met de agent naam\n",
    "df_training_score[\"model\"] = \"model 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2.3 PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.0 Bronnen\n",
    "[3 hour course](https://www.youtube.com/watch?v=Mut_u40Sqz4)<br>\n",
    "[baseline docummentatie](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5041ba25690102580176b8f7b4720df17ebf4bc4d40fd191e3f1a5800ffdb29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
