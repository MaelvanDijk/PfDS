{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voorwoord en opdracht\n",
    "Hoe ontsluit je data en verwerk je het tot bruikbare inzichten? Python biedt heel veel mogelijkheden \n",
    "om data te ontsluiten, te prepareren en te analyseren. Aan al deze mogelijkheden heb je alleen wat als \n",
    "je ze ook echt toe kunt passen. Aan jou de opdracht om de technieken die je tijdens de colleges hebt \n",
    "geleerd in de praktijk te brengen. Dit doe je door een eigen dataproject uit te voeren: zelf relevante \n",
    "data zoeken en inladen, deze data opschonen en combineren tot relevante inzichten.  \n",
    "\n",
    "## Beoordeling \n",
    "We zijn daarom op zoek naar een originele toepassing waarmee je de inhoud van de college kan \n",
    "vertalen in goed werkende en netjes gedocumenteerde python code. De specifieke scores en weging \n",
    "van deze criteria kun je op de volgende pagina vinden. \n",
    "\n",
    "\n",
    "| **Originaliteit** | **Inhoud** | **Code**| **Documentatie**|\n",
    "|---|---|---|---|\n",
    "| Het project omvat een unieke toepassing die in college niet aan bod is gekomen en de student weet hiermee perfect gebruik te maken van de kracht van python. | Alle data-analysestappen worden doorlopen Hierbij wordt aandacht geschonken aan alle geleerde onderwerpen. Geleerde packages worden verrijkt met code uit meerdere andere bronnen | De gehele code werkt en geeft de juiste resultaten weer. Fouten worden automatisch en correct afgehandeld. De code is volledig modulair en werkt met minimale aanpassingen ook op andere datasets. | De code, documentatie en bijpassende titels en teksten maken van het project een duidelijk geheel dat door anderen zonder verdere uitleg zou kunnen worden begrepen |\n",
    "## Deadline \n",
    "We ontvangen je project graag uiterlijk 1 februari, voor (14:00) via Canvas. Mocht je project tot een \n",
    "onvoldoende beoordeling leiden, dan heb je nog een keer de kans om een herkansing in te leveren. \n",
    "Het is je eigen verantwoordelijkheid om op tijd met je project te starten, we zullen tussendoor dus niet \n",
    "controleren hoe ver je bent of gedeeltelijke projecten beoordelen. \n",
    "Tijdens het laatste college vragen we je om collega studenten een korte presentatie te geven over je \n",
    "project. Die presentatie wordt zelf niet beoordeeld maar geeft je wel de kans om te laten zien hoe je \n",
    "met criteria zoals originaliteit of onvoorziene tegenslagen bent omgegaan.   \n",
    "We raden je aan om de code in Jupyter notebooks te schrijven, maar het staat je vrij om daar een \n",
    "ander programma voor te kiezen zolang wij aan het eind van het project de code en resultaten kunnen \n",
    "beoordelen.  \n",
    "\n",
    "## Onderwerp \n",
    "Omdat we het leuk vinden als je de geleerde python vaardigheden meteen in de praktijk kan \n",
    "toepassen, willen we je vragen om zelf een onderwerp te kiezen. Tijdens de colleges zullen we een \n",
    "aantal voorbeelden geven van eerdere projecten. \n",
    "Mocht je nog vragen hebben over het project dan kun je je vragen het best stellen op de Canvas-\n",
    "cursuspagina. Natuurlijk kun je ons ook altijd mailen of tijdens het college aanspreken.\n",
    "\n",
    "## Wat moet er aan de pas komen\n",
    "\n",
    "### Masterclass 0\n",
    "- ~~Variables~~\n",
    "- ~~Calculations / Arithmetics~~\n",
    "- Statements & Expressions\n",
    "- Datatypes\n",
    "- Operators\n",
    "\n",
    "### Masterclass 1\n",
    "- if, else and elif-statements\n",
    "- One-line if-statements\n",
    "- Conditional expression\n",
    "- ~~While-loop~~\n",
    "- ~~For-loop~~\n",
    "\n",
    "### Masterclass 2\n",
    "- ~~functions, parameters and arguments~~\n",
    "- ~~docstrings~~\n",
    "- classes (niet verplicht)\n",
    "- ~~imports~~\n",
    "\n",
    "\n",
    "### Masterclass 3\n",
    "- ~~Fruitfull functions (with return statement)~~\n",
    "- ~~Fruitless functions (without return statement)~~\n",
    "- ~~Docstring~~\n",
    "- Argument tuple packing (*args, **kwargs)\n",
    "- clasess (not required)\n",
    "- ~~importing libraries~~\n",
    "\n",
    "### Masterclass 4\n",
    "- ~~numpy~~\n",
    "- ~~lists~~\n",
    "- ~~dicts~~\n",
    "- tuple\n",
    "\n",
    "### Masterclass 5\n",
    "- api\n",
    "- ~~visualization~~\n",
    "- ~~error handling (try, except,~~ else, finally)\n",
    "\n",
    "### Masterclass 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opdracht introductie: Reinforcement Learning\n",
    "\n",
    "Voor het vaak Programming for Data Science 2020-2021 is het de bedoeling een programmeer project aan te leveren bij voorkeur in Jupyter notebook format. De eisen zijn om in iedergeval gebruik te maken van verschillende fundamenten in het programeren (data-types, functions, imports, etc.) Het onderwerp van de gekozen opdracht is het toepassen van **Reinforcement Learning (RL).**\n",
    "\n",
    "Het notebook is al volgt opgebouwd:\n",
    "1. Uitleg en introductie over RL\n",
    "2. verkennen van de centrale libraries\n",
    "3. toepassen op een *complexe* omgeving\n",
    "\n",
    "Alleen deel 3 doornemen is genoeg om te zien wat Reinforcement Learning is. Om echter tot het toepassings niveau te komen kunnen deel 1 en 2 worden gelezen. Hierin worden de berekeningen toegelicht en worden de functies en dergelijke verder uitgediept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Wat is Reinforcement Learning (RL)?\n",
    "\n",
    "Reinforcement Learning, RL in het kort, is een tak van machinelearning/deeplearning. Waar andere machinelearning takken (historische) data nodig hebben om te voorspellen, clustteren, clasificeren, etc. Is Reinforcement Learning een onderdeel van machinelearning waarbij de machine zijn eigen data creeërt en op basis hiervan leert. RL kan daardoor gebruikt worden met minimale initiële data en zal dit gaandeweg het trainingsproces zelf maken. Dit kan tijd en kosten besparen. In plaats van een robot te trainen in een fysieke omgeving kan deze omgeving en robot in een digitale omgeving gesimuleerd en getrained worden. Dit beperkt de benodigde middelen en maakt ontwikkelen van robots relatief goedkoop.<br><br>\n",
    "![Machinelearning catagories](https://d6vdma9166ldh.cloudfront.net/media/images/1561029079805-Machine-Learning.jpg)\n",
    "\n",
    "### 1.1 Wat is RL in het kort?\n",
    "Met reinforcement Learning wordt geprobeerd een **agent** te trainen de juiste **beslissingen** te maken. Beslissingen worden gemaakt in een bepaalde **omgeving** (bijvoorbeeld een lab of een videospel). Om de analogie van het videospel aan te houden: de agent (mario) voert een actie uit (springen) en maakt een **observatie** (een muntje komt uit een blokje). Deze observaties en acties worden aan elkaar gekoppeld doormiddel van een beloningssysteem. Gedurende veel itteraties zal de agent trainen en het beste resultaat proberen te behalen.\n",
    "\n",
    "Binnen RL doen we een aantal aannamens of stellen we een paar beperkingen:\n",
    "* Het probleem is (zeer) complex, anders is RL overkill.\n",
    "* Het betreft een Markov achtige omgeving --> elke actie heeft een reactie ofwel observaties en acties volgen elkaar op.\n",
    "* trainen kan lang duren en is niet altijd stabiel.\n",
    "\n",
    "\n",
    "### 1.2 RL raamwerk\n",
    "In onderstaande afbeelding is schematisch weergegeven hoe het raamwerk van Reinforcement Learning inwerking treed. De agent (Pacman) voert een actie uit (linksaf, rechtsaf, omlaag, omhoog), hij eet daardoor meer bolletjes en zijn scoren stijgt (imediate/shaped reward). De spookjes bewegen ook, de algehele staat van het speelveld komt als observatie binnen bij de agent die vervolgens weer een actie neemt. Deze cyclus herhaalt zich tot de agent het level heeft gehaald of af is gegaan. Zodra de agent het level haalt zal deze nog een reward krijgen (sparse rewards). Dit geheel vormt het raamwerk voor RL.<br><br>\n",
    "![Reinforcement Learning Framework](https://miro.medium.com/max/1400/1*CjLFVeYssOIJaeijrxPHPg.png)\n",
    "\n",
    "Er zijn twee nieuwe termen benoemd ten opzichten van de mario analogie:\n",
    "* *Sparse Rewards*: RL problemen hebben vaak te maken met een belonging die 1-to-many staan (bijv. 1x een reward op elke 1000 zetten/spellen). Deze worden dan ook *Sparse rewards* genoemd en vormt een van de uitdagingen van RL ten opzichten van traditionele ML. In Mario en PacMan zijn het halen van levels voorbeelden van *Sparse Rewards*.\n",
    "* *Imediate/shaped Rewards*: In bepaalde environments kunnen *imediate/shaped rewards* worden gegeven. In Mario zouden dit muntjes zijn en vijanden verslaan, in PacMan zou dit zijn wanneer de bolletjes en spookjes opgegeten worden. Dit proces van additionele beloningen toekennen kan er voor zorgen dat agent beter leert wat deze moet doen om tot een goed resultaat te komen. Hier schuilt echter ook het gevaar, het algoritme kan te gretig worden en daardoor de voorkeur gaan geven aan vroegtijdige belonginge ten opzichten van de echte beloning aan het einde van het spel.\n",
    "\n",
    "### 1.3 RL formules\n",
    "\n",
    "Om de agent te trainen staan een aantal functies centraal. Namelijk de *Reward function*, *Value funtion* en de *Policy function*. In deze paragraaf bespreken we deze functies in het kort om een beeld te krijgen van wat er onder de motorkap gebeurt. De functies zijn vrij technisch en niet vereist om RL toe te passen. Maar om de kern van RL te doorgronden zijn deze formules wel belangrijk.\n",
    "\n",
    "#### 1.3.1 *Reward function*\n",
    "de beloning die een agent krijgt voor een bepaalde actie. deze functie kan als onderstaande worden opgeschreven:\n",
    "\n",
    "$$r_{t} = R(s_{t},a_{t},s_{t+1})$$\n",
    "hierin is *r* de beloning, *t* de huidige tijdstap, *R()* een specifieke beloningsfunctie, *s* de huidige obsertvatie/state, *a* de genomen actie. De formule stelt dus \"De huidige beloning, is gelijk aan de functie van de huidige staat, de huidige actie en de staat/observatie van de volgende tijdstap.\" Deze functie wordt vaak versimpeld naar $R(s_{t})$ (de huidige staat) of $R(s_{t}, a_{t})$ (de huidige actie/staat combinatie).\n",
    "\n",
    "Het doel is dus om de cummulatieve beloning over een periode te maximalizeren. De benadering van deze cummulatieve beloning bepaalt de functie van R() hieronder worden twee varianten van de functie besproken.<br> \n",
    "\n",
    "allereerst: **finite-horizon undiscounted return** waarin de functie puur een opsomming is van de belongingen over *T* tijdsstappen:\n",
    " $$R(\\tau) = \\sum_ {t=0}^T\\ r_{t}$$\n",
    " <br>\n",
    "\n",
    "De tweede variant heet: **infinite-horizon discounted return**, hierbij worden de beloningen van alle tijdstappen meegenomen, echter hoeverder de tijdstap (en beloning) in de toekomst ligt, hoe groter de discount. Dit betekent dat een beloning die over 100*t* plaatsvindt minder waard is dan de beloning over 1*t*:\n",
    "\n",
    "$$R(\\tau) = \\sum_ {t=0}^\\infty \\ r_{t} $$\n",
    "<br>\n",
    "\n",
    "$\\tau$ (*tau*) is in dit geval een opeenvolging van staten en acties: $\\tau = (S_{0},a_{0},S_{1},a_{0}, \\cdots)$ dit wordt ookwel *trajectory, episodes, of rollouts* genoemd. De start staat is hierbij vaak willekeurig gekozen. de volgende staten worden bepaald door de genomen actie van de agent.\n",
    "\n",
    "#### 1.3.2 *Value function*\n",
    "De value function kijkt naar de huidige *value* van een staat/observatie. In dit geval wordt met *value* het volgende bedoeld: de verwachte beloning voor de huidige staat of staat-actie combinatie wanneer één policy gekozen wordt en hier niet meer van afgeweken wordt. Van deze value functies zijn er 4 belangrijke te benoemen:\n",
    "\n",
    "1. **On-Policy Value Function**: $V^{\\pi}(s)$ geeft de verwachte return (cummulatieve beloning) wanneer je start in staat *s* en altijd policy $\\pi$ aanhoudt:\n",
    "$$V^{\\pi}(s) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "2. **On-Policy Action-Value Function**: $Q^{\\pi}(s,a)$ de verwachte return wanneer een willekeurige actie wordt genomen vanaf de eerste state en daarop volgend alleen maar de gekozen policy inzet:\n",
    "$$Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "3. **Optimal Value Function**: $V^{*}(s)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen:\n",
    "$$V^{*}(s) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "4. **Optimal Action-Value Function**: $Q^{*}(s,a)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen na eerst eenmalig een willekeurige actie te hebben gekozen:\n",
    "$$Q^{*}(s,a) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "#### 1.3.3 Policy function\n",
    "De policy function (of agent function). Is een functie die aangeeft welke actie een agent neemt. Deze agent en policy termen worden vaak doorelkaar gebruikt. Zo kan policy in: \"de policy probeert de beloning te maximalizeren\" vervangen worden door \"de agent\". Er zijn twee variante van de policy function, namelijk:\n",
    "\n",
    "1. deterministisch, toeval speelt hier geen rol, zijn de begin voorwaarden gelijk dan zal de uitkomst ook gelijk zijn. In dit geval wordt de policy function aangegeven als $a_{t} = \\mu(s_{t})$. Hier staat \"de huidige actie wordt bepaald door de functie $\\mu()$ (*mu*) gegeven de huidige staat. \n",
    "<br><br>\n",
    "\n",
    "2. Stochastisch, in dit geval speelt toeval een rol. De uitkomst is van te voren nog niet bekend. Hier wordt gekeken naar de kans van een actie - vervolg staat . De functie kan opgeschreven worden als: $a_{t} \\sim \\pi(a|s_{t})$ wat betekent dat \"de actie evenredig is met de policy functie $\\pi()$ van een kans gegeven staat *t*.\" ofwel \"kies de actie die de meeste waarschijnlijkheid heeft om te leiden tot de volgende beste staat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL in kleine stapjes\n",
    "In de voorgaande sectie is RL op theoretisch niveau beschreven. Nu is het tijd om daadwerlijk met RL aan de slag te gaan. In dit project staan 2 libraries centraal:\n",
    "1. [gym](https://gym.openai.com/docs/): Gym is een library geschreven door OpenAI en bevat de bouwstenen om een agent binnen een environment te laten opereren.\n",
    "2. [Stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/): Bevat verschillende algoritmes om een agent te trainen en volgens o.a. de eerder genoemde formules te laten handelen.\n",
    "<br><br>\n",
    "\n",
    "Verder zijn er nog een aantal andere libraries die worden ingeladen, namelijk:\n",
    "* os: is nodig om systeem operaties te doen (zoals locaties van files ophalen)\n",
    "* pandas: voor het werken met data in tabel format\n",
    "* seaborn: voor visualisatie\n",
    "* matplotlib: ligt onder seaborn en brengt extra functionaliteiten mee\n",
    "* numpy: ligt onder pandas en brengt extra functionaliteiten\n",
    "* torch: Deeplearning library, wordt in dit geval gebruikt om CUDA GPU's te benutten voor RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python 3.9.7 is nodig voor het gebruik van deze modules\n",
    "# !pip install stable-baselines3[extra] ## docummentatie voor deze module die RL algoritmes bevat: https://stable-baselines3.readthedocs.io/en/master/\n",
    "# !pip install gym\n",
    "# !pip install pyglet ## extra dependency voor OpenAI gym mocht deze ontbreken\n",
    "!pip list -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "# from gym import wrappers # gebruiken om geen visualisaties te doen van de environment\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C # Specifieke algoritme voor RL\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # wrapper voor environment beheersing\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # beoordeling van agent policies\n",
    "from stable_baselines3.common.vec_env import VecFrameStack # gebruikt voor visual based learners om frames opelkaar te leggen\n",
    "from stable_baselines3.common.env_util import make_atari_env #specifieke wrapper voor atari environments\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CheckpointCallback # callbacks die aangeroepen worden tijdens learning\n",
    "\n",
    "# packages voor het werken met dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualisatie packages\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# deeplearning library (voor het controleren van gpu status)\n",
    "import torch\n",
    "\n",
    "# nodig om atari spellen te mogen/kunnen gebruiken\n",
    "# ';' voorkomt dat er een output komt\n",
    "!python -m atari_py.import_roms \"Roms\\Roms\\ROMS\";\n",
    "\n",
    "print('Roms verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Environments\n",
    "\n",
    "OpenAI geeft met de gym library toegang tot het benutten van RL environments. Zoals eerder uitgelegd is de environment een omgeving waar de agent zicht in beweegt, observaties uit haalt en beloningen binnen krijgt.<br><br>\n",
    "\n",
    "Gym environments zijn op te delen in een aantal types, namelijk:\n",
    "* **Box:** n-dimensionele tensor, die een range van waarden bevat --> ```Box(0, 1, shape=(3,3))```\n",
    "* **Discrete:** set van discrete waarden --> ```Discrete(3)```\n",
    "* **Tuple:** Een Tuple van andere spaces --> ```Tuple((Discrete(3), Box(0, 1, shape=(3,3))))```\n",
    "* **Dict:** Dictionary van spaces --> ```Dict({('height': Discrete(3), 'speed':Box(0, 1, shape=(3,3)))})```\n",
    "* **MultiBinary:** One hot encoded binary waarde --> ```MultiBinary(4)``` --> [0,0,0,0] of [0,1,0,1] etc.\n",
    "* **MultiDiscrete:** Meerdere discrete waarde --> ```MultiDiscrete([2,5,2])```\n",
    "\n",
    "In onderstaande code gaan we de action space en de observation space ophalen van de ```CartPole-v0``` omgeving. Deze twee spaces worden na de code cell verder uigelegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inladen van environment\n",
    "environment_name ='CartPole-v0'\n",
    "env = gym.make(environment_name)\n",
    "\n",
    "# Test om te zien wat de environment bevat\n",
    "print(\n",
    "    f'''\n",
    "    Action space van {environment_name} environment: {env.action_space}\n",
    "    Observation space van {environment_name} environment: {env.observation_space}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CartPole-v0\n",
    "Voor de eerste test wordt de CartPole-v0 environment gebruikt. Deze environment bestaat (zoals hierboven te zien) uit:\n",
    "* Action space Discrete(2), waarbij 0 de cart naar links stuurt en 1 de cart naar rechts stuurt.\n",
    "* Observation space, het scherm dat de speler/agent ziet is een Box(), een array van in dit geval 4 elementen. elk van deze vier elementen hebben een lower-bound en een upper-bound. Elk element is van het type float32. Hieronder wordt er nog wat dieper ingegaan op de observation space.\n",
    "\n",
    "### 2.2 De observation space\n",
    "```\n",
    "env.observation_space: \n",
    "    Box(\n",
    "        [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38],\n",
    "        [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38],\n",
    "        (4,),\n",
    "        float32)\n",
    "```\n",
    "\n",
    "dit kan als volgt geïnterpeteerd worden: <br>\n",
    "env.observation_space[0] = [-4.8000002e+00] en [4.8000002e+00] --> dit zijn de min en max posities van het wagentje<br>\n",
    "env.observation_space[1] = [-3.4028235e+38] en [3.4028235e+38] --> de velocity (snelheid) van het wagentje<br>\n",
    "env.observation_space[2] = [-4.1887903e-01] en [4.1887903e-01] --> hoek van de paal ten opzichten van het wagentje<br>\n",
    "env.observation_space[3] = [-3.4028235e+38] en [3.4028235e+38] --> velocity van de paal<br>\n",
    "\n",
    "binnen deze observation space wordt er per env.observation_space[*x*] een waarde teruggeven voor de agent om op te acteren. Zie bijvoorbeeld onderstaande snippit:\n",
    "```\n",
    "env.observation_space.sample()\n",
    "--> array([-1.1144775e-02,  2.3990196e+38,  2.5578210e-01,  3.1761389e+38],\n",
    "      dtype=float32)\n",
    "```\n",
    "\n",
    "Bovenstaande geeft dus de locatie van de wagen, de snelheid / richting van de wagen, de hoek van de paal en de snelheid/richting van de paal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neem een mogelijke vorm van de observation space\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Een simulatie doen\n",
    "\n",
    "We hebben dus een omgeving waaruit we informatie kunnen ophalen, en we hebben een action space waaruit de agent acties kan kiezen. Door middel van een een loop kunnen we een aantal keer het spel laten spelen door de computer.  <br>\n",
    "\n",
    "In onderstaande code gaan we 5 rondes (episodes) van het spel spelen. Elke episode wordt het spel gereset en beginnen we weer op het startpunt ```env.reset()```. We geven reseten ook de score en geven aan dat het spel nog niet klaar is. <br>\n",
    "\n",
    "In de inner loop draaien we het spel totdat de ```env.step()``` function aangeeft dat het spel afgelopen is. Deze step functie neemt een 'stap' in de omgeving door een actie uit te voeren en geeft hier voor informatie voor terug waaronder de reward. De actie kan random zijn ```.sample()``` of een keuze op basis van learnings door het model. De geïnformeerde keuze wordt later toeglicht. <br>\n",
    "\n",
    "Aan het einde van elke episode (outer loop) printen we de episode en de bijbehorende score. Zodra we klaar zijn met alle episodes maken we de omgeving schoon om problemen te voorkomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# het aantal keer dat de agent het spel speelt.\n",
    "EPISODES = 5\n",
    "\n",
    "\n",
    "for episode in range(0, EPISODES): # loop over de episodes.\n",
    "    state = env.reset() # stel de omgeving opnieuw in.\n",
    "    done = False # zodra het spel is afgelopen verandert dit naar done, en begint dit op false.\n",
    "    score = 0 # de score die onze agent moet verhogen.\n",
    "\n",
    "    while not done: # not done == not False == True --> het model blijft dus doorgaan zolang de done variable de waarde 'False' heeft\n",
    "        env.render() #render het spel in een omgeving. normaal gesproken staat dit uit voor snelheid.\n",
    "        action = env.action_space.sample() # van onze action space kies 1 actie random (links of rechts).\n",
    "        n_state, reward, done, info = env.step(action) # neem een stap (frame/actie), dit returned 4 waarden. een nieuwe Box(), de beloning, False/True (het spel is afgelopen), en eventueel extra info.\n",
    "        score += reward # tel de beloning voor deze actie op bij de score van deze episode.\n",
    "    print(f'Episode: {episode+1} | Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Algoritmes\n",
    "\n",
    "Er zijn diverse algoritmes beschikbaar. Binnen dit notebook wordt gebruik gemaakt van Model-Free RL. dit zijn algorithmes die zich minder/niet bezig houden met het voorspellen van de volgende state maar puur kijken naar een een set regels die de agent maakt op basis van ervaring. Zo kan de agent een regel opstellen dat het wagentje naar links moet als de stok een beetje links kantelt. In onderstaande afbeelding zijn de diverse opties te zien. In dit geval ligt de focus op PPO (Proximal Policy Optimization) dit is een policy algoritme die minder kans heeft om vast te komen zitten in een minima. Normaal gesproken leert policy based algoritmes van de huidige staat en kiest een nieuwe policy. Met PPO gebeurt dit ook maar wordt het verschil tussen policy *p<sub>t</sub>* en *p<sub>t-1</sub>* klein gehouden. <br>\n",
    "\n",
    "![algorithmes](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Metrics\n",
    "Voor elk type algoritme zijn diverse metrics beschikbaar. deze kunnen in 4 categoriën worden opgedeeld:\n",
    "* Evaluation: beschrijving van de lengte en reward voor een episode\n",
    "* Time: alles wat betreft tijd besteding\n",
    "* Loss: loss functie om het model te scoren\n",
    "* Overig: deze worden toeglicht wanneer deze aanbod komen\n",
    "\n",
    "De metrics worden in apparte mappen opgeslagen als log bestanden en zijn uit te lezen op diverse manieren. In dit notebook wordt gekeken naar de tensorboard visualisaties en .npz files die zullen worden omgezet naar een dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Een agent maken\n",
    "\n",
    "Het maken van een simpele agent met Stable_Baselines3 is makkelijk om te doen. Met twee regels code kan een agent ingesteld en getrained worden. De environment voorbereiden kost verder nog 2 regels code. Hierdoor is een base agent binnen 4 a 5 regels code klaar. In onderstaande cellen worden een aantal extra stappen toegevoegd naast hier boven genoemde 5 regels code. De volledige code in onderstaande snippit wordt eerst uitgelegd zodat duidelijk is wat er gebeurt:\n",
    "\n",
    "```\n",
    "log_path = os.path.join('Training', 'Evaluations')\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "training_path = os.path.join('Training', 'Training')\n",
    "\n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose= 1,\n",
    "    tensorboard_log= log_path\n",
    "    )\n",
    "\n",
    "model.learn(total_timesteps= 20000, eval_log_path = training_path)\n",
    "model.save(PPO_path)\n",
    "```\n",
    "\n",
    "Allereerst worden 3 bestandslocaties opgeslagen in variabele. Deze locaties zijn nodig voor het opslaan van tensorboard logging informatie (log_path), opslaan van het model (PPO_path) en opslaan van numpy trainings informatie (training_path).\n",
    "\n",
    "In de volgende 2 regels, wordt een environment gemaakt zoals we eerder hebben gedaan. Daarna zetten we de environment om in een vector. Voor bepaalde modellen is het goed om environments te stacken (de agent op 2 of meer environments tegelijk te trainen). Voor simpele omgevingen zoals de CartPole-v0 omgeving is het niet nodig om omgevingen te stacken. Wel heeft het PPO model een vector nodig vandaar dat we de environment \"wrappen\" in een dummy vector (vector met een laag). \n",
    "\n",
    "in de 6e regel initialiseren we een agent. We geven de agent de variabele naam \"model\" en stellen een aantal parameters in. We kiezen een Multilayered Perceptron (MlP) als onderliggend netwerk. Dit is een heel simpele Neurale Network waarbij getallen aan de ene kant worden ingevoerd (de dummy vector van de omgeving) aan het einde komt er een output uit (de actie). Het network zal vervolgens aangepast worden zodat de verzameling van input (observaties) en output (actie) de best mogelijk reward geven. <br><br>\n",
    "![een voorbeeld van een MlP](https://www.researchgate.net/profile/Mohamed-Zahran-16/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n",
    "<br><br>\n",
    "\n",
    "we geven ook een locatie voor onze tensorboard mee. Tensorboard files bevatten informatie over het model tijdens / na trainen. Deze informatie wordt met ```verbose = 1``` beperkt, wanneer 0 wordt ingevoerd wordt er niks gelogged en bij 2 wordt er veel informatie gelogged (dit kan veel ruimte kosten).\n",
    "\n",
    "In de laatste 2 regels trainen we de agent om te handelen op basis van bepaalde regels. Dit doen we voor 20.000 timesteps (episodes / games) en we slaan training informatie op in de aangeven locatie. Vervolgens kan het model worden opgeslagen om later opgehaald te worden zodra deze nodig is.\n",
    "\n",
    "In de onderstaande cellen worden deze stappen achtereen uitgevoerd. en wordt een bestaand model ingeladen als deze aanwezig is. Anders wordt er een nieuw model getrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bepaal de locatie voor het opslaan van logs en models\n",
    "log_path = os.path.join('Training', 'Evaluations')\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "training_path = os.path.join('Training', 'Training')\n",
    "log_path\n",
    "\n",
    "# maak een omgeving \n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda:env]) # wrapper om de environment in een dummy vector te plaatsen\n",
    "\n",
    "# Initialiseer een PPO-agent \n",
    "model = PPO(\n",
    "    'MlpPolicy', # MlpPolicy (Multilayer Perceptron), een neuraalnetwerk policy\n",
    "    env,\n",
    "    verbose= 1, # verbose = 0, 1, of 2 --> 1 is loggen ban belangrijkste gegevens.\n",
    "    tensorboard_log= log_path # waar de tensorboard file wordt opgeslagen\n",
    "    )\n",
    "\n",
    "try:\n",
    "    # als er een model aanwezig is wordt deze ingeladen\n",
    "    model = PPO.load(r'Training\\Saved Models\\PPO_Model_Cartpole.zip')\n",
    "    print('Pre-trained model loaded!')\n",
    "\n",
    "except:\n",
    "    # onbreekt dit model dan wordt een nieuw model getrained\n",
    "    print('Pre-trained model not found... training new model')\n",
    "    model.learn(total_timesteps= 20000, eval_log_path = training_path) # train het model voor 20000 timesteps en schrijf de training informatie weg\n",
    "    model.save(PPO_path) # Sla het model op in de locatie: Training\\Saved Models \n",
    "    print(f'Model training finished, model saved to {PPO_path}')\n",
    "\n",
    "finally:\n",
    "    # Zodra een model is geladen of getrained wordt het model hieronder in de environment losgelaten (dit is hetzelfde als met de random sample van de environment)\n",
    "    for episode in range(0, 5):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action, _ = model.predict(obs) # Laat het model een actie uitvoeren op basis van de huidige observatie (ipv een sample uit de actie environment)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evalueren van model en tensorboard\n",
    "\n",
    "Er zijn drie manieren om een Stable Baslines model te beoorderen. Bij het aanmaken van het model kan een tensorboard log path worden meegeven. Hierin wordt informatie opgeslagen die gevisualiseerd kan worden via de ```%tensorboard``` magic commando. Dit opent een nieuw scherm (dit scherm moet mogelijk herladen worden om alle data binnen te halen) en geeft verschillende gegevens weer over de learnings van een model. In de afbeelding hieronder is de episode reward en length (in dit geval gelijk) gevisualiseerd voor een aantal PPO modellen die getrained zijn:\n",
    "<br><br>\n",
    "![Voorbeeld tensorlog](visualisatie\\tensorboard_example.png)\n",
    "<br><br>\n",
    "\n",
    "Gezien de omgeving relatief simpel is wordt het plafond van 200 (y-as) in een klein aantal trainingrondes (x-as) bereikt. De onderstaande 3 cellen doen het volgende:\n",
    "1. Open een tensorboard en laad de log gegevens in.\n",
    "2. Maak een functie aan die gebruikt kan worden om een agent (getrained of op basis van action-samples) in een omgeving te draaien.\n",
    "3. Test de functie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorlog ophalen van een locatie\n",
    "train_log_path = os.path.join(log_path, 'PPO_3')\n",
    "\n",
    "#klik de play button hieronder\n",
    "%tensorboard --logdir= train_log_path \n",
    "# klik de play button hierboven voor de Tensorboard evaluations grafieken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om een agent in een specifieke environment te draaien\n",
    "def run_agent(env, model = False, episodes= 5, render = False):\n",
    "    '''\n",
    "    runs an agent (model) through the environment for n episodes, if render is set to False no window is rendered.\n",
    "    at the end of the runs a tensorlog file is written to a predetermined directory\n",
    "    '''\n",
    "    for episode in range(0, episodes): \n",
    "        obs = env.reset() \n",
    "        done = False \n",
    "        score = 0 \n",
    "\n",
    "        while not done:\n",
    "            if render: \n",
    "                env.render(mode='human') \n",
    "            if model:\n",
    "                action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draai nu de zelfde environment met een getrainde agent\n",
    "run_agent(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalueer het model op basis van een aantal parameters (average punten, standard deviation) --> max punten = 200.\n",
    "# evaluate_policy(model, env, n_eval_episodes= 5, render=False) \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evalueren op basis van score\n",
    "\n",
    "De tweede manier om een agent te beoordelen is door deze een aantal keer in de omgeving zijn gang te laten gaan, dit weg te schrijven in een lijst (of ander object) en deze lijst vervolgens om te zetten naar een visual.\n",
    "\n",
    "Deze wijzen van visualiseren kan op twee momenten, namelijk: tijdens het trainen of zodra de agent volledig getrained is. Beide geven een ander zicht op de agent en hoewel het mogelijk is om dit zelf te doen worden beide uitvoeringen afgeraden.\n",
    "\n",
    "Bij het zelf loggen van de data tijdens een training  wordt het proces vele malen trager zelfs op een simpele omgeving. Daarnaast vereist dit toevoegen van meerdere regels code waarbij er ingebouwde functionaliteiten zijn om dit sneller te doen. De code hiervoor heb ik uit het notebook verwijderd maar wel getest. Een model waarvan de training in deze simpele omgeving op deze manier beoordeeld zou worden kost minstens 350x de tijd die de ingebouwde loggingsfunctie gebruikt.\n",
    "\n",
    "Om een agent te evalueren na afloop van het training kost minder tijd. bij een 5 tal evaluaties is het wegschrijven van de scoren naar een lijst erg snel. Echter kost ook deze manier meerdere regels code. Tijdsverschil heb ik hier niet gemeten maar mijn vermoeden is dat dit ook langer zal duren bij ingewikkeldere omgevingen en meer beoordelingen.\n",
    "\n",
    "Om toch te laten zien hoe het beoordelen zou gaan met zelf geschreven code kan onderstaande 2 cellen worden gedraaid. Hierin worden twee lijsten geïnitialiseerd eentje de score en een voor het bijhouden van de episode (hoeveelste evaluatie ronde). Vervolgens wordt, in de tweede cell, een dataframe aangemaakt op basis van een lijst naar dictionary transformatie. Dit dataframe wordt vervolgens gevisualiseerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad het model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "# initialeer 2 lege lijsten om score en episode in bij te houden\n",
    "lst_score = []\n",
    "lst_episode = []\n",
    "\n",
    "#Draai het model\n",
    "for episode in range(0, episodes): \n",
    "    obs = env.reset() \n",
    "    done = False \n",
    "    score = 0 \n",
    "\n",
    "    while not done: \n",
    "        # env.render() \n",
    "        action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    lst_score.append(score[0])\n",
    "    lst_episode.append(episode + 1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak een dictiionary op basis van twee lijsten\n",
    "d = {\"episode\": lst_episode, \"score\": lst_score}\n",
    "\n",
    "# laad de dictionary in een dataframe\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "# visualiseer de dataframe\n",
    "sns.barplot(x= df[\"episode\"], y=df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evalueren van meerdere agents op basis van .npz bestanden\n",
    "\n",
    "De laatste manier die hier gebruikt wordt voor het visualiseren van model prestaties is het loggen van basale data naar een .npz bestand. Dit bestandstype kan worden uitgelezen via de Numpy library van python. In onderstaande afbeelding is te zien hoe een .npz bestand als visualisatie er uit kan komen te zien. In dit geval is de ontwikkeling van twee modellen tijdens het trainen bijgehouden:\n",
    "<br><br>\n",
    "![model1 vs model2](visualisatie\\model1vsmodel2.png)\n",
    "<br><br>\n",
    "\n",
    "Zoals te zien lopen beide agents een ander traject af met trainen. Hier is ook tot zekere hoogte het effect van een PPO model te zien. De agent ontwikkelt zich door kleine aanpassingen te doen. Hierdoor is de kans klein dat het in een lokaal optmimum beland. De agent zal soms grote sprongen maken in de ontwikkeling door een kleine aanpassing in de strategie (deze kleine aanpassing is kenmerkend voor PPO). De volgende timestep zal de agent weer een kleine aanpassing doen. Het gevolg is dat de reward stabiel groeit (hopelijk) en een optimale strategie tot stand komt. In sommige gevallen kan de stap drastische score veranderingen veroorzaken. Dit zal het model dan weer met een kleine stap proberen te corrigeren (indien negatief).\n",
    "\n",
    "In de onderstaande 5 cellen gaan we van model tot visualisatie:\n",
    "1. We maken een nieuwe agent aan.\n",
    "2. Een functie wordt aangemaakt om van .npz file tot een dataframe te komen, deze functie wordt ook gelijk toegepast.\n",
    "3. We visualiseren de dataframe om te controleren of alles goed is gegaan, daarna slaan we de afbeelding op.\n",
    "4. Er wordt een tweede model gemaakt, getrained en de informatie van de training wordt in een ander dataframe geladen. \n",
    "5. Vervolgens wordt in een derde dataframe de informatie van beide modellen gecombineert en gevisualiseert.\n",
    "\n",
    "Wanneer onderstaande cellen worden gedraait zal de uitkomst anders zijn dan bovenstaande afbeelding maar de stappen worden hetzelfde doorlopen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nieuw model voor loggen trainings informatie\n",
    "model2 = PPO('MlpPolicy', env)\n",
    "model2.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderstaande functie is ontstaan na een aantal iteraties. In eerste instantie werd elke stap binnen de functie gecontroleerd door het dataframe te printen. hierdoor werd duidelijk dat het dataframe eerst gepivot moest worden door de verkeerde indeling waarmee het .npz bestand gevuld is. Na het transposen bleken de kolommen 'results' en 'ep_lengths' gevuld te zijn met lijsten. Dit komt omdat elke evaluatie 5x draait. Hierover is het gemiddelde berekend om een benadering te krijgen van de verwachte prestaties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# functie voor het laden en transformeren van npz files\n",
    "def npz_to_dataframe(npz):\n",
    "    '''\n",
    "    Takes an npz file object loaded using numpy.load(path to .npz file) and returns a formatted dataframe\n",
    "    '''\n",
    "    # maak een data frame van de npz file op basis van de kolommen\n",
    "    df = pd.DataFrame.from_dict({col: npz[col] for col in npz.files}, orient='index')\n",
    "    \n",
    "    # transpose de dataframe gezien deze verkeerd om staat\n",
    "    df =df.T\n",
    "\n",
    "    # bereken de gemiddeldes voor iedere twee van de drie kolommen bevatten een lijst van waarden (1 waarde per n_eval_episodes in de vorige cell)\n",
    "    df = df.applymap(np.mean)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_training_score = npz_to_dataframe(npz)\n",
    "df_training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een lineplot om de prestaties van het model na diverse timesteps te zien\n",
    "fig = sns.lineplot(x= df_training_score[\"timesteps\"], y=df_training_score[\"results\"])\n",
    "display(fig)\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vergelijk de training met een tweede model\n",
    "model3 = PPO('MlpPolicy', env)\n",
    "model3.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)\n",
    "\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# Roep de eerder gemaakte functie aan\n",
    "df_training_score2 = npz_to_dataframe(npz)\n",
    "\n",
    "#voeg aan beide data frames een kolom toe met de agent naam\n",
    "df_training_score[\"model\"] = \"model 1\"\n",
    "df_training_score2[\"model\"] = \"model 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_training_score.append(df_training_score2)\n",
    "fig = sns.lineplot(x= df_compare[\"timesteps\"], y=df_compare[\"results\"], hue= df_compare['model'])\n",
    "display(fig)\n",
    "\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1vsmodel2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Callback en earlystopping\n",
    "Vooral bij grote modellen kan tijdens het trainen het model onstabiel worden. Om dit te verhelpen kunnen we het trainen stoppen wanneer dit een bepaalde grenswaarden bereikt. Een andere reden om het trainen van een model te 'onderbreken' is een lang draaiende training. Wanneer een model 30 uur aan het trainen is en de computer crashed wil je dat het model is opgeslagen in de tussentijd om niet alle waardevolle resources verspild te hebben.\n",
    "In onderstaande twee cellen gaan we het model elke 1000 stappen evalueren en bij een score van 200 breken we de training af om het model op te slaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Waar het model opgeslagen wordt\n",
    "best_model_path = \".\\Training\\Saved Models\"\n",
    "\n",
    "# StopTrainingOnRewardThreshold --> voorwaarden wanneer het trainen moet stoppen\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold= 200, verbose= 1)\n",
    "\n",
    "# EvalCallback --> evalueert het trainingproces en controlleert de treshhold\n",
    "eval_callback = EvalCallback(env, callback_on_new_best= stop_callback, eval_freq= 1000, best_model_save_path= best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Andere policies en algoritmes\n",
    "\n",
    "In de onderstaande cellen gaan we verder met het opstellen van twee alternatieve modellen. \n",
    "\n",
    "De eerste stap gaat zijn om een PPO model op te stellen met een zelf samengesteld Neural Network. Hoewel Stable Baselines goed afgestelde modellen heeft is het goed om te weten dat deze nog verder verfijnd kunnen worden. In dit geval maken we gebruik van een neural network voor de agent/decision functie (*pi*) en de value function (*vf*). beider netwerken zullen bestaan uit 4 lagen met iedere laag 128 neuronen.<br>\n",
    "\n",
    "In de tweede cel laden we kort een ander algorithme, DQN (Deep Q Network). Dit model is voor de volgende spaces te gebruiken:\n",
    "\n",
    "|Spaces | Action | Observation |\n",
    "|---|---|---|\n",
    "|  Discrete | ✔️ | ✔️ |\n",
    "|  Box | ❌ | ✔️ |\n",
    "|  MultiDiscrete | ❌ | ✔️ |\n",
    "|  MultiBinary | ❌ | ✔️ |\n",
    "\n",
    "Dit model lijkt zich dus goed te lenen voor het Carpole probleem. Hier hebben we namelijk te maken met een **discrete action space** en een **box observation space**.<br>\n",
    "\n",
    "Het opzetten van deze twee modellen is alleen ter demonstratie. Het illustreert hoe simpel het is om een ander algoritme te kiezen en hoe, nagenoeg, alle arguments methods binnen de functies gelijk blijven.\n",
    "\n",
    "In **Sectie 6** worden verschillende agents met elkaar vergeleken op een complexere omgeving. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Een neuralnetwork architectuur voor agent (pi) en valuefunction (vf)\n",
    "NN_arch = [dict(pi= [128,128,128,128], vf= [128,128,128,128] )]\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch': NN_arch})\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Project 1: Breakout\n",
    "\n",
    "In deze sectie gaan we proberen het spel Breakout te halen. Breakout is een simpel spel waarbij we met een peddle onder aan het scherm proberen een balletje te raken om hiermee blokken boven in het scherm kappot te maken. Om dit tot een succes te laten verlopen, worden modellen getrained en vergeleken met de kennis die we in de voorgaande secties hebben verkregen. De volgende onderdelen zullen aanbod komen:\n",
    "1. Benodigde functies\n",
    "2. Inladen van de omgeving\n",
    "3. Algortimes vergelijken\n",
    "4. Hyperparameters\n",
    "6. Een model trainen die naar verwachting goed zal presteren\n",
    "\n",
    "We beginnen ook met dezelfde imports als aan het begin van deze notebook. Hierdoor weten we zeker dat alles werkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "# from gym import wrappers # gebruiken om geen visualisaties te doen van de environment\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C # Specifieke algoritme voor RL\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # wrapper voor environment beheersing\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # beoordeling van agent policies\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage # gebruikt voor visual based learners om frames opelkaar te leggen\n",
    "from stable_baselines3.common.env_util import make_atari_env #specifieke wrapper voor atari environments\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CheckpointCallback # callbacks die aangeroepen worden tijdens learning\n",
    "\n",
    "import pandas as pd # voor het werken met dataframes\n",
    "import seaborn as sns # voor visualiseren\n",
    "from matplotlib import pyplot as plt # aanpassingen van visuals\n",
    "import numpy as np # npz files inlezen\n",
    "\n",
    "import torch # Ophalen van gegevens van de videokaart\n",
    "\n",
    "# nodig om atari spellen te mogen/kunnen gebruiken\n",
    "# ';' voorkomt dat er een output komt\n",
    "!python -m atari_py.import_roms \"Roms\\Roms\\ROMS\";\n",
    "\n",
    "print('Roms verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0.1 Functies klaarzetten\n",
    "Om met deze omgeving aan de slag te gaan maken we eerst een aantal functies aan die gebruikt kunnen worden. De functies worden zoveel mogelijk opgeknipt zodat het duidelijk is wat een functie doet. Deze functies zijn iteratief en met wat test werk vooraf aangemaakt. Zo was het nodig om bij het maken van de functie ```npz_to_dataframe()``` elke tussenstap te printen om te zien wat er in de file zat en hoe dit werd omgezet naar een pandas dataframe. ```compare_trainings()``` heeft ook een aantal wijzigingen gehad. Waaronder de mogelijkheid om nu meerdere modellen naast elkaar te leggen en het een fruitfull function te maken zodat de image opgeslagen kan worden met een latere method call.\n",
    "\n",
    "De functies zijn veelal gebruikt om sneller en duidelijker code te kunnen uitvoeren. En deze functies maken gebruik van een docstrings om informatie voor de user beschikbaar te hebben. Type hinting is toegevoegd voorgebruiksgemak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_info(env: gym.Env):\n",
    "    \"\"\"returns information about the environments type, observation an daction space and action meanings.\n",
    "\n",
    "    arguments:\n",
    "        env -- environment created using gym.make()\n",
    "    \n",
    "    Return:\n",
    "        environment information printed to console\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "    f'''\n",
    "    observation space type: {type(env.observation_space)}\n",
    "    observation space shape: {env.observation_space.shape}\n",
    "    action space: {env.action_space}\n",
    "    aanwezige actions: {env.unwrapped.get_action_meanings()}\n",
    "    '''\n",
    ")\n",
    "\n",
    "def npz_to_dataframe(npz: np.ndarray):\n",
    "    '''\n",
    "    Takes an npz file object loaded using numpy.load(path to .npz file) and returns a formatted dataframe\n",
    "\n",
    "    Arguments:\n",
    "        npz -- .npz file like object containing training information of specific trained model\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # maak een data frame van de npz file op basis van de kolommen\n",
    "    df = pd.DataFrame.from_dict({col: npz[col] for col in npz.files}, orient='index')\n",
    "    \n",
    "    # transpose de dataframe gezien deze verkeerd om staat\n",
    "    df =df.T\n",
    "    \n",
    "    # bereken de gemiddeldes voor iedere cel\n",
    "    df = df.applymap(np.mean)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def device_status():\n",
    "    '''\n",
    "    prints torch cuda device information to console\n",
    "    '''\n",
    "\n",
    "    print(\n",
    "        f'''\n",
    "        Cuda enabled device:    {torch.cuda.is_available()}\n",
    "        Select device index:    {torch.cuda.current_device()}\n",
    "        Device location:        {torch.cuda.device(0)}\n",
    "        Number of devices:      {torch.cuda.device_count()}\n",
    "        Device name:            {torch.cuda.get_device_name(0)}\n",
    "        Allocated memory:       {round(torch.cuda.memory_allocated(0)/1024**3,1)} 'GB')\n",
    "        Cached memory:          {round(torch.cuda.memory_reserved(0)/1024**3,1)} 'GB')\n",
    "        '''\n",
    "    )\n",
    "\n",
    "def run_agent(env: gym.Env, model = False, episodes: int = 5, render: bool = False):\n",
    "    '''\n",
    "    runs an agent (model) through the environment for n episodes, if render is set to False no window is rendered.\n",
    "    at the end of the runs a tensorlog file is written to a predetermined directory\n",
    "\n",
    "    arguments:\n",
    "        env -- supply environment where the agent should be run\n",
    "        model (optional) -- supply a trained model/agent to run through the environment\n",
    "        episodes (optional) -- set the number of episodes for which to train the model\n",
    "        render (optional) -- set to True to show the agent in a pop-up playing the environment\n",
    "\n",
    "    return: prints episode and score to console\n",
    "\n",
    "    '''\n",
    "\n",
    "    for episode in range(0, episodes): \n",
    "        obs = env.reset() \n",
    "        done = False \n",
    "        score = 0 \n",
    "\n",
    "        while not done:\n",
    "            if render: \n",
    "                env.render(mode='human') \n",
    "            if model:\n",
    "                action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()\n",
    "\n",
    "def compare_trainings(dataframes: pd.DataFrame, model_names: list):\n",
    "    '''\n",
    "    appends supplied dataframes containing timesteps and results of training and adds a column with the model name.\n",
    "    Then creates, shows and returns a lineplot with the 3 model results compared over time \n",
    "\n",
    "    arguments:\n",
    "        dataframes -- a list of dataframes containing the columns 'timesteps' and 'results'\n",
    "        model_names -- a list of names for the models in the same order as the dataframes\n",
    "\n",
    "    return: seaborn figure with a line per model scores on the y-axis and time on the y-axis\n",
    "\n",
    "    '''\n",
    "    master_df = pd.DataFrame(columns=['timesteps', 'results', 'model'])\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        df['model'] = f'{model_names[i]}'\n",
    "        master_df = master_df.append(df)\n",
    "    \n",
    "    fig = sns.lineplot(x= master_df[\"timesteps\"], y=master_df[\"results\"], hue= master_df['model'])\n",
    "    return fig  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 de omgeving\n",
    "\n",
    "We hebben dus te maken met een box observation space en een discrete action space. De observation space bestaat uit drie dimensies (Rood, Groen, Blauw waarde) en heeft een afmeting van 210 height x 160 width. Daarnaast bestaat de action space uit 4 discrete acties 'NOOP', 'FIRE', 'RIGHT', en 'LEFT'. Hierbij staat 'NOOP' voor het nemen van geen actie, 'FIRE' wordt gebruikt om het spel op te starten.\n",
    "\n",
    "Voor dit probleem is er dus een algoritme nodig dat gebruik kan maken van een een discrete action space. In de [documentatie van Stable Baselines](https://stable-baselines.readthedocs.io/en/master/guide/algos.html) is te zien dat er een aantal algoritmes in aanmerking komen, namelijk: A2C, ACER, ACKTR, DQN, HER, GAIL en PPO. Deze algoritmes kunnen allemaal omgaan met een discrete action space.\n",
    "\n",
    "We gaan om te beginnen gebruik maken van A2C, DQN, en PPO. Dit verkleind de scope en geeft een beeld van het effect dat diverse modellen zullen hebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPISODES = 5\n",
    "MODEL_PATH = r'Breakout_model\\Saved_Model' # locatie van het model\n",
    "TENSOR_LOG_PATH = r'Breakout_model\\Logging' # locatie van tensor fils\n",
    "TRAIN_LOG_PATH = r'Breakout_model\\Training_evaluation_npz' # locatie .npz training bestanden voor model_comparison() functie\n",
    "BEST_MODELS_PATH = r'Breakout_model\\Best_model' # plek waar het beste model tijdens de trining wordt opgeslagen\n",
    "MODEL_CHECKPOINT_PATH = r'Breakout_model\\Checkpoints' # opslaan van checkpoint models (als bijvoorbeeld het systeem crashed)\n",
    "\n",
    "# controleren van eventuele GPU/CPU device\n",
    "device_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aanmaken environment en info van de omgeving printen\n",
    "environment_name = 'Breakout-v0'\n",
    "env = gym.make(environment_name)\n",
    "env_info(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of alles werkt d.m.v aangemaakte functie\n",
    "run_agent(env, render= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 De Agents\n",
    "We proberen dus een agent te trainen op het spel Breakout. Ook hebben we gezien dat de observation space een afbeelding is (rgb van 210 bij 160). Dit betekent dat het Mlp-model niet gaat werken, of in iedergeval veel minder effectief zal zijn. In dit geval proberen we een Convolutional Neural Network (CNN) te trainen. Dergelijke netwerken zijn beter in het behandelen van afbeeldingen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 A2C\n",
    "[Advantage Actor Critic](https://openai.com/blog/baselines-acktr-a2c/), A2C, is een variatie op het A3C (Asynchronous Advantage Actor Critic) algoritme. A2C heeft de mogelijkheid om tegelijk meerdere workers in te zetten op de environment.\n",
    "\n",
    "A3C combineert een aantal belangrijke elementen:\n",
    "* een update aan het algorithme gebeurt na een vast aantal timesteps. Over dit segment worden estimators berkend op de return en advantage functies.\n",
    "* Lagen van de netwerk architectuur worden gedeeld tussen de policy function (pi) en de value function(vf)\n",
    "* Asynchronysche updates\n",
    "\n",
    "Waar A3C de werkers los van elkaar update wacht A2C met een update van de werkers totdat deze allemaal klaar zijn met een segment. Vervolgens wordt er een average van alle workers gepakt voor de update. Binnen Stable Baselines en OpenAI lijkt de performance van A2C een stuk beter dan A3C.\n",
    "\n",
    "Hieronder trainen (of laden) we een A2C agent. We loggen hiervan een aantal gegevens om later te vergelijken met de andere drie modellen om uiteindelijk een keuze te maken wel model het meest geschikt lijkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probeer een model in te laden en evalueren, anders wordt deze getrained\n",
    "try:\n",
    "    A2C_model = A2C.load(BEST_MODELS_PATH + r'\\A2C\\best_model.zip') # inladen model\n",
    "    print('model loaded!')\n",
    "    print('model evaluating...')\n",
    "\n",
    "    eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13) # omgeving aanmaken\n",
    "    eval_env = VecFrameStack(eval_env, n_stack=4) # omgeving stacken\n",
    "\n",
    "    A2C_eval = evaluate_policy(A2C_model, eval_env, n_eval_episodes= 10, render= True ) # model evalueren\n",
    "    eval_env.close() # omgeving sluiten\n",
    "    print(f'model mean score: {A2C_eval[0]}') # ophalen reward\n",
    "\n",
    "# als dit model niet bestaat vraag of de gebruiker een model wilt trainen  \n",
    "except (FileNotFoundError, AttributeError) as e:\n",
    "    model_answer = input('Model could not be found. Do you want to train a model? This will take a few minutes y/n')\n",
    "\n",
    "    # als er iets anders is meegegeven dan y train het model niet\n",
    "    if model_answer != 'y':\n",
    "        print('You have chosen not to train the model, ending operations')\n",
    "    \n",
    "    # bij input y train een model voor een aantal timesteps\n",
    "    elif model_answer == 'y':\n",
    "        print('.... Alright sit back, relax model will start training, see you in a few...')\n",
    "        \n",
    "        # maak training environment en stack deze\n",
    "        env = make_atari_env('Breakout-v0', n_envs = 8, seed= 13)\n",
    "        env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "        # maak evaluatie environment en stack deze\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "        eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "\n",
    "        # maak twee callbacks, een voor het opslaan van een tussentijds model, de ander voor het evalueren van het model\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=25000, save_path= MODEL_CHECKPOINT_PATH +  r'\\A2C', name_prefix='A2C')\n",
    "        eval_callback = EvalCallback(env, best_model_save_path= BEST_MODELS_PATH + r'\\A2C', log_path= TRAIN_LOG_PATH + r'\\A2C', eval_freq=500, verbose= 0)\n",
    "\n",
    "        A2C_callbacks = [checkpoint_callback, eval_callback]\n",
    "\n",
    "        # initialiseer het model met de callbacks en verbose=0 om minder info naar de terminal te printen\n",
    "        A2C_model = A2C('CnnPolicy', env, verbose=0, tensorboard_log= TENSOR_LOG_PATH + r'\\A2C')\n",
    "\n",
    "        # Train het model en geef de callbacks mee\n",
    "        A2C_model.learn(50000, eval_env= eval_env, callback= A2C_callbacks)\n",
    "\n",
    "        # sla het model op\n",
    "        A2C_model.save(path = MODEL_PATH + r\"\\A2C\")\n",
    "\n",
    "        # print de locaties waar het model en gegevens zijn opgeslagen\n",
    "        print('Last Model saved to:        '+ f'{MODEL_PATH}' + r'\\A2C')\n",
    "        print('Best Model saved to:        '+ f'{BEST_MODELS_PATH}' + r'\\A2C')\n",
    "        print('Training log saved to:      '+ f'{TRAIN_LOG_PATH}' + r'\\A2C')\n",
    "        print('Tensor log saved to:        '+ f'{TENSOR_LOG_PATH}' + r'\\A2C')\n",
    "        print('Model Checkpoints saved to: '+ f'{MODEL_CHECKPOINT_PATH}' + r'\\A2C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 DQN\n",
    "\n",
    "Een DQN, Deep Q Network, is een agent gebasseerd op een neuraal netwerk waarbij uitgegaan wordt van het optimaliseren van de Q-value functie. Hierbij wordt gebruik gemaakt van een replay memory, het netwerk leert op dit moment niet van de actie, staat, actie, staat in een lineaire lijn. De replay memory verzameld ad random actie, transitie (verandering in frame) paren om hiervan te leren. Dit stabilizeerd het leer proces van de agent.\n",
    "\n",
    "De [Q-value function (Bellman equation for discounted rewards)](https://www.geeksforgeeks.org/bellman-equation/) is een greedy algoritme en wordt als volgt genoteerd: $Q(s,a) = r(s,a) + \\gamma (maxQ(s',a))$ <br>\n",
    "Wat hier staat is: \"De Q-value functie is gegeven actie ($a$) en staat ($s$) de beloning $r(s,a)$ met daarbij opgeteld de maximale Q voor de state t+1 ($s'$) en actie ($a$). Deze $Qmax$ maakt dat het een algoritme greedy is. Het kijkt alleen naar de huidige actie en de daaropvolgende staat om het hoogst mogelijke resultaat te bereiken. Deze functie kijkt echter niet verder dan 1 timestep(frame) waardoor de functie op lange termijn minder grote beloningen kan halen.\n",
    "\n",
    "Het stukje $Qmax(s',a)$ haalt de maximale future state en actie waarde op uit de zogenoemde Q-table. Deze Q-table is een matrix met alle mogelijke acties en states en een score. Hier wordt ook de $\\gamma()$ (gamma) functie op toegepast. De gamma functie kan een waarde van 0 tot 1 mee te krijgen waar 0 gelijk staat aan directe beloningen en 1 op langere termijn beloningen van toepassing is. \n",
    "\n",
    "Hieronder nog een voorbeeld van een Q-table, hierin is actie 3 state 3 de hoogste waarde:\n",
    "\n",
    "| | Action 1 | Action 2 | Action 3 |\n",
    "|---|---|---|---|\n",
    "|  State 1 | 0.01 | 0.5 | 0.6 |\n",
    "|  State 2 | 0.01 | 0.7 | 0.003 |\n",
    "|  State 3 | 0.5 | 0.55 | 0.86 |\n",
    "\n",
    "Als we hier het algoritme op los laten die weet dat onze future state 2 zal zijn dan wordt actie 2 gekozen en de waarde 0.7 geretourneert.\n",
    "\n",
    "In onderstaande code trainen (of laden) we een DQN agent. Tijdens het trainen worden rewards gelogged om een idee te geven van de meest waardevolle agent om mee door te gaan. Waarbij de andere modellen nog geen hyperparamaters worden ingesteld doen we dit voor de DQN agent wel. Hier verlagen we de buffer_size en learning_starts in ```.DQN()```, de GPU van mijn laptop heeft namelijk niet de capaciteit voor een hogere buffer_size. Deze buffer grootte beïnvloed hoevel frames (als afbeelding) en actie combinaties worden vastgehouden in memory. Deze verzameling states en acties vormen de pool waar het model ad random samples pakt om van te leren. Hoe kleiner deze pool hoe lager het vermogen om te leren, wat een beperking is voor DQN in deze opdracht. Omdat we ook minder lang trainen in kader van de opdracht moet het leren eerder beginnen en verlagen we dit naar de helft van de totale time_steps meegegeven in ```.learn()```. Ook dit beperkt voor nu de kwaliteit van het model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probeer een model in te laden en evalueren, anders wordt deze getrained\n",
    "try:\n",
    "    # create new environment\n",
    "    eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "    train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "\n",
    "    # load model\n",
    "    DQN_model = DQN.load(BEST_MODELS_PATH +  r'\\DQN\\best_model.zip')\n",
    "    print('model loaded!')\n",
    "    print('model evaluating...')\n",
    "\n",
    "    # evaluate model\n",
    "    DQN_eval = evaluate_policy(DQN_model, eval_env, n_eval_episodes= 10, render= True )\n",
    "    eval_env.close()\n",
    "    print(f'model mean score: {DQN_eval[0]}')\n",
    "\n",
    "# als dit model niet bestaat vraag of de gebruiker een model wilt trainen  \n",
    "except (FileNotFoundError, AttributeError) as e:\n",
    "    model_answer = input('Model could not be found. Do you want to train a model? This will take a few minutes y/n')\n",
    "\n",
    "    # als er iets anders is meegegeven dan y train het model niet\n",
    "    if model_answer != 'y':\n",
    "        print('You have chosen not to train the model, ending operations')\n",
    "    \n",
    "    # bij input y train een model voor een aantal timesteps\n",
    "    elif model_answer == 'y':\n",
    "        print('.... Alright sit back, relax model will start training, see you in a few...')\n",
    "        \n",
    "        # maak environment en stack deze\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "        train_env = VecFrameStack(train_env, n_stack=1)\n",
    "        \n",
    "        # maak twee callbacks, een voor het opslaan van een tussentijds model, de ander voor het evalueren van het model\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=25000, save_path= MODEL_CHECKPOINT_PATH +  r'\\DQN', name_prefix='DQN')\n",
    "        eval_callback = EvalCallback(eval_env, best_model_save_path= BEST_MODELS_PATH + r'\\DQN', log_path= TRAIN_LOG_PATH + r'\\DQN', eval_freq=500, verbose = 0)\n",
    "        \n",
    "        DQN_callbacks = [checkpoint_callback, eval_callback]\n",
    "\n",
    "        # initialiseer het model met de callbacks en verbose=0 om minder info naar de terminal te printen\n",
    "        DQN_model = DQN('CnnPolicy', train_env, verbose= 0, tensorboard_log= TENSOR_LOG_PATH + r'\\DQN', device= 'cuda',  buffer_size= 20000, learning_starts = 25000)\n",
    "\n",
    "        # Train het model en geef de callbacks mee\n",
    "        DQN_model.learn(total_timesteps=50000, eval_env=eval_env, callback= DQN_callbacks)\n",
    "        \n",
    "        # sla het model op\n",
    "        DQN_model.save(path = MODEL_PATH + r\"\\DQN\")\n",
    "\n",
    "        # print de locaties waar het model en gegevens zijn opgeslagen\n",
    "        print('Last Model saved to:        '+ f'{MODEL_PATH}' + r'\\DQN')\n",
    "        print('Best Model saved to:        '+ f'{BEST_MODELS_PATH}' + r'\\DQN')\n",
    "        print('Training log saved to:      '+ f'{TRAIN_LOG_PATH}' + r'\\DQN')\n",
    "        print('Tensor log saved to:        '+ f'{TENSOR_LOG_PATH}' + r'\\DQN')\n",
    "        print('Model Checkpoints saved to: '+ f'{MODEL_CHECKPOINT_PATH}' + r'\\DQN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 PPO\n",
    "We hebben het al kort gehad over een [PPO model](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) in sectie 3.1. In het kort probeert een PPO model de policy function in kleine stappen aan te passen. Waar andere modellen de functie met grotere stappen aanpast, streeft PPO er naar om de wijzingen in de policy zo minimaal mogelijk te houden. Dit moet ervoor zorgen dat de training van het model stabieler verloopt. Het model blijkt niet alleen op simpele robot/spellen goed te werken maar ook op een ingewikkeldere omgeving zoals [DOTA2](https://www.youtube.com/watch?v=pkGa8ICQJS8).\n",
    "\n",
    "Hoogover kijkt zorgt een PPO agent er dus voor dat tijdens het trainen van het netwerk de policy (de functie die de actie bepaalt) minimaal verandert. Dit wordt gedaan omdat er veel noise (toevallige data) in de reward functions kan zitten. Om te voorkomen dat het model denkt dat een bepaalde policy erg goed was terwijl deze voornamelijk bestond uit noise wordt de uitkomst geclipped. Dit clippen zorgt ervoor dat de verandering in weigths binnen een bepaalde band breedte blijft en het model niet zijn weights aan gaat passen omdat deze noise binnen krijgt. \n",
    "\n",
    "Een technische maar wat meer verhelderende uitleg wordt gegeven op Youtube door [Arxiv Insights](https://www.youtube.com/watch?v=5P7I-xPq8u8).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probeer een model in te laden en evalueren, anders wordt deze getrained\n",
    "try:\n",
    "    # create new environment\n",
    "    eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "    train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "    \n",
    "    # load model\n",
    "    PPO_model = PPO.load(r'Breakout_model\\Saved_Model\\PPO_Breakout_5k.zip')\n",
    "    print('model loaded!')\n",
    "    print('model evaluating...')\n",
    "        \n",
    "    # evaluate model\n",
    "    PPO_eval = evaluate_policy(PPO_model, eval_env, n_eval_episodes= 5, render= True )\n",
    "    eval_env.close()\n",
    "    print(f'model mean score: {PPO_eval[0]}')\n",
    "\n",
    "# als dit model niet bestaat vraag of de gebruiker een model wilt trainen\n",
    "except (FileNotFoundError, AttributeError) as e:\n",
    "    \n",
    "    model_answer = input('Model could not be found. Do you want to train a model? This will take a few minutes y/n')\n",
    "    \n",
    "    # als er iets anders is meegegeven dan y train het model niet\n",
    "    if model_answer != 'y':\n",
    "        print('You have chosen not to train the model, ending operations')\n",
    "    \n",
    "    # bij input y train een model voor een aantal timesteps\n",
    "    elif model_answer == 'y':\n",
    "        print('.... Alright sit back, relax model will start training, see you in a few...')\n",
    "\n",
    "        # create new environment en stack deze\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "        train_env = VecFrameStack(train_env, n_stack=4)\n",
    "\n",
    "        # maak twee callbacks, een voor het opslaan van een tussentijds model, de ander voor het evalueren van het model\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=25000, save_path= MODEL_CHECKPOINT_PATH + r'\\PPO', name_prefix='PPO')\n",
    "        eval_callback = EvalCallback(eval_env, best_model_save_path= BEST_MODELS_PATH + r'\\PPO', log_path= TRAIN_LOG_PATH + r'\\PPO', eval_freq=500, verbose= 0)\n",
    "\n",
    "        PPO_callbacks = [checkpoint_callback, eval_callback]\n",
    "\n",
    "        # initialiseer het model met de callbacks en verbose=0 om minder info naar de terminal te printen\n",
    "        PPO_model = PPO('CnnPolicy', train_env, verbose= 0, tensorboard_log= TENSOR_LOG_PATH + r'\\PPO', device= 'cuda')\n",
    "\n",
    "        # Train het model en geef de callbacks mee\n",
    "        PPO_model.learn(total_timesteps=50000, eval_env=eval_env, callback= PPO_callbacks)\n",
    "\n",
    "        # sla het model op\n",
    "        PPO_model.save(path = MODEL_PATH + r\"\\PPO\")\n",
    "        \n",
    "        # print de locaties waar het model en gegevens zijn opgeslagen\n",
    "        print('Last Model saved to:        '+ f'{MODEL_PATH}' + r'\\PPO')\n",
    "        print('Best Model saved to:        '+ f'{BEST_MODELS_PATH}' + r'\\PPO')\n",
    "        print('Training log saved to:      '+ f'{TRAIN_LOG_PATH}' + r'\\PPO')\n",
    "        print('Tensor log saved to:        '+ f'{TENSOR_LOG_PATH}' + r'\\PPO')\n",
    "        print('Model Checkpoints saved to: '+ f'{MODEL_CHECKPOINT_PATH}' + r'\\PPO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Modellen vergelijken\n",
    "\n",
    "Er zijn nu 3 modellen getrained voor 50.000 timesteps. Persoonlijk had ik de hoogste verwachtingen bij de DQN agent echter blijkt deze het slechtste te presteren. In onderstaande figure is te zien dat het beste model op een korte trainingscyclus zonder tuning de PPO agent is. In het laatste onderdeel van dit hoofdstuk gaan we aan de slag met Hyperparameter tuning om een goed model te trainen. \n",
    "\n",
    "![model comparison](visualisatie\\Breakout_model_comparison.png)\n",
    "\n",
    "Wanneer we nog even stil staan bij de bovenstaande afbeelding zien we dat A2C en PPO eventueel langer getrained kunnen worden om een betere benchmark op te halen. Nu is hier niet voor gekozen wegens de tijdsinvestering. In onderstaande cell wordt deze afbeelding gemaakt aan de hand van eerder aangemaakte functies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual dataframes\n",
    "A2C_df = npz_to_dataframe(np.load(r'Breakout_model\\Training_evaluation_npz\\A2C\\evaluations.npz'))\n",
    "DQN_df = npz_to_dataframe(np.load(r'Breakout_model\\Training_evaluation_npz\\DQN\\evaluations.npz'))\n",
    "PPO_df = npz_to_dataframe(np.load(r'Breakout_model\\Training_evaluation_npz\\PPO\\evaluations.npz'))\n",
    "\n",
    "# put dataframes in list and use another list to supply model names\n",
    "model_dfs = [A2C_df, DQN_df, PPO_df]\n",
    "model_names = ['A2C', 'DQN', 'PPO']\n",
    "\n",
    "# run function to append and show figure of dataframes\n",
    "model_comparision_plt = compare_trainings(model_dfs, model_names);\n",
    "\n",
    "# the object variable is an AxesSubplot. We can extract the figure attribute and save this as .png\n",
    "model_comparision_figure = model_comparision_plt.figure\n",
    "model_comparision_figure.savefig(r'visualisatie\\Breakout_model_comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Hyperparameter Tuning\n",
    "\n",
    "Binnen ML en RL zijn we niet alleen afhankelijk van de data die in het model gaat (garbage in, garbage out is een veel gehoord kreet) maar ook van de specifieke parameters van dit model. Het afstellen van deze parameters is erg belangrijk om het model de juiste richting in te helpen en een betere optimalisatie te verkrijgen. Zo hebben we eerder ```time_steps, replay_buffer, learning_starts``` ingesteld voor het DQN model. Waar deze parameters voornamelijk zijn bedoelt om machine resources juist in te zetten zijn er ook diverse parameters die de onderliggende wiskunde beïnvloeden.\n",
    "\n",
    "Maar welke parameters zijn er nou en welke moet je aanpassen om het gewenste resultaat te krijgen? Hierop is niet direct een juist antwoord en dit zal ondervonden moeten worden door het testen van diverse instellingen.\n",
    "\n",
    "<img align=\"left\" src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimgflip.com%2Fs%2Fmeme%2FTwo-Buttons.jpg&f=1&nofb=1\" alt=\"choices\" style=\"width: 200px; padding-right:25px; padding-bottom:30px\"/>\n",
    "\n",
    "Het uiteindelijke model waar we mee gaan werken is de PPO agent. Deze gaan we opnieuw in korte cycli trainen om de effecten van afzonderlijke parameters te kunnen vergelijken. Aan het einde gaan we het model voor langere tijd trainen in de hoop op een hogere score. DeepMind heeft met experts aan een agent gewerkt dat een score > 300 heeft weten te behalen. Aangezien we zien dat na een halfuur trainen zonder optimalisatie de agent maximaal 10 punten weet te halen stel ik in dit notebook het doel om minimaal 100 punten te scoren. Met de beperkte hardware hoop ik dit nog te kunnen halen.\n",
    "\n",
    "De hyperparameters voor een PPO agent zijn terug te vinden in de [documentatie](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html). Om het process minder afhankelijk te laten zijn van kans gaan we elk parameter die we wijzigen 5 keer testen (doormiddel van 5 dezelfde seeds). Op deze manier kunnen we een deel van de randomness wegnemen en kunnen we kijken naar average ontwikkeling van het model. Om te beginnen gaan we kijken naar drie verschillende parameters, namelijk:\n",
    "* [learning_rate](https://deepchecks.com/glossary/learning-rate-in-machine-learning/): De grootte van de error waarmee de gewichten in een network geüpdate mag worden. Een hogere learning rate zorgt er voor dat een model sneller uitkomt op een (sub)-optimum \n",
    "* [gamma](https://stackoverflow.com/questions/54334315/what-is-the-full-meaning-of-the-discount-factor-%CE%B3-gamma-in-reinforcement-learn): ofwel de discount factor. Hoe hoger de waarde hoe belangrijker de reward van toekomstig states is in verhouding tot de direct reward. Kort gezegd hoelanger het duurt voordat de echte reward komt (het level halen vs 1 blokje kappot maken) hoe hoger de gamma moet zijn\n",
    "* [batch_size](https://deeplizard.com/learn/video/U4WB9p6ODjM): Een batch (of mini-batch) is het aantal inputs dat 'tegelijk' door een netwerk gaat. Krachtige computers kunnen grotere batches tegelijk door het netwerk sturen maar dit is niet altijd beter. Wanneer het netwerk kleinere batches ontvangt kan deze beter leren generaliseren. Het aantal batches, bijvoorbeeld 10, bepaald hoeveel data objecten (zoals een image) per keer door het netwerk gaat. Als er 1.000 afbeeldingen zijn dan bestaat elke batch dus uit 100 afbeeldingen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maken weer de constante aan en voegen hier een lijst met seeds, learning rates, gamma's en batch sizes aan toe waar we met de modellen over gaan itereren\n",
    "SEED_LIST = [13, 26, 39, 52, 65]\n",
    "LEARNING_RATES = [0.0003, 0.003, 0.03, 0.00002, 0.000001]\n",
    "GAMMA_OPTIONS = [0.9999, 0.99, 0.80, 0.5, 0.05]\n",
    "BATCH_SIZES = [16, 32, 64, 128, 256]\n",
    "TIMESTEPS = 10000\n",
    "EPISODES = 5\n",
    "MODEL_PATH = r'Breakout_model\\Saved_Model'\n",
    "TENSOR_LOG_PATH = r'Breakout_model\\Logging'\n",
    "TRAIN_LOG_PATH = r'Breakout_model\\Training_evaluation_npz'\n",
    "BEST_MODELS_PATH = r'Breakout_model\\Best_model'\n",
    "MODEL_CHECKPOINT_PATH = r'Breakout_model\\Checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Learning Rate\n",
    "\n",
    "De [learning_rate](https://deepchecks.com/glossary/learning-rate-in-machine-learning/) is zoals eerder gezegd de snelheid waarmee je model toestaat te leren. Het trainen van een neuralnetwerk houdt in dat dat je de weights langzaam aanpast om naar een optimale uitkomst te werken. In onderstaande afbeelding is het effect van drie verschillende learningrates te zien:\n",
    "\n",
    "![learning rates effect](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
    "\n",
    "Hierin is te zien dat door de learning rate de snelheid bepaalt waarmee het (sub)-optimum wordt bereikt en het effect wanneer deze verkeerd is afgesteld. Zoals aangegeven gebeurt dit door de weights aan te passen in het netwerk op basis van de fouten die zijn gemaakt. De learningrate bepaald de fractie waarmee deze weights worden aangepast. Stel de ideale weight is 0.2 en we hebben momenteel een weight van 0.25. Bij een te grote learning rate wordt dit misschien wel aangepast naar een 0.05. Hierdoor drijft het netwerk verder af van het optimum (rechter grafiek). Is de learning rate te laag dan passen we de weight aan naar 0.2499. Dat betekent dat we erg lang doen over het optimum bereiken (linker grafiek). Het is dus een kunst om de juiste learning rate te kiezen.\n",
    "\n",
    "In onderstaande twee cellen gaan we eerst een netwerk trainen op 5 verschillende random environments en in elk van deze environments worden 5 agents getrained met verschillende learning rates en schrijven we de resultaten weg. Wanneer we vervolgens de trainingen willen visualiseren zien we dat de grafiek bijna onleesbaar is. Met een paar aanpassingen kunnen we dit verhelpen en hebben we een duidelijkere grafiek met een iets duidelijker resultaat!\n",
    "\n",
    "<img align=\"centre-left\" src=\"visualisatie\\PPO_model_learningrate_comparison_cluttered.png\" alt=\"cluttered graph\" style=\"width: 45%; padding-top:20px; background-color:white\"/>\n",
    "<img align=\"centre-right\" src=\"visualisatie\\PPO_model_learningrate_comparison.png\" alt=\"clean graph\" style=\"width: 50%; background-color:white\"/>\n",
    "\n",
    "Wanneer we naar de rechter afbeelding kijken zien we ook gelijk het resultaat van een hogere vs een lagere learning rate. Hoe hoger de learningrate is ingesteld hoe sneller het model in deze kleine steekproef een betere score haalt. Het valt hier wel op dat de blauwe en groene agent (0.0003 en 0.3) langzamer op gang komen. De aannamen hier is dat deze twee agents bij een langere training beter gaan presteren tot zij een plafond bereiken.anderzijds kan het zijn dat ze nu al vast zitten in een sub-optimum en niet meer verder kunnen trainen dan de band breedte van de laatste 2000 timesteps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = input(\n",
    "    '''\n",
    "    Het uitvoeren van deze cell kost +/- 2 uur. \n",
    "    De modellen zijn al een keer getrained en in de volgende cell kan je de resultaten terug vinden zonder dat je het model opnieuw hoeft te trainen.\n",
    "    \n",
    "    wil je deze cell overslaan?\n",
    "    type: \"overslaan\"\n",
    "    \n",
    "    wil je de cell uitvoeren?\n",
    "    type: \"uitvoeren\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "if a == \"overslaan\":\n",
    "    print('Je hebt er voor gekozen de cell niet uit te voeren')\n",
    "\n",
    "elif a == \"uitvoeren\":\n",
    "    print('de cell wordt nu uitgevoerd...')\n",
    "\n",
    "    # Trainen van een PPO agent op basis van learning rate\n",
    "    \n",
    "    # Create an environment based on the current itteration in te seedlist\n",
    "    for i in SEED_LIST:\n",
    "        print(f'working with seed: {i}')\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = VecFrameStack(train_env, n_stack=1)\n",
    "\n",
    "        # Train a model based on the learningrate itteration\n",
    "        for j in LEARNING_RATES:\n",
    "            print(f'working with learning rate: {j}')\n",
    "            # create checkpoints and callbacks\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path= BEST_MODELS_PATH + r'\\PPO',\n",
    "                log_path= TRAIN_LOG_PATH + r'\\PPO' + '_seed_' + str(i) + '_learningrate_' + str(j), # Save trainingdata to location\n",
    "                eval_freq=500,\n",
    "                verbose= 0\n",
    "                )\n",
    "\n",
    "            PPO_callbacks = [eval_callback]\n",
    "\n",
    "            # create and train model\n",
    "            PPO_model = PPO(\n",
    "                'CnnPolicy',\n",
    "                train_env,\n",
    "                verbose= 0,\n",
    "                device= 'cuda',\n",
    "                learning_rate= j # use the current learning rate\n",
    "                )\n",
    "\n",
    "\n",
    "            PPO_model.learn(\n",
    "                total_timesteps= TIMESTEPS,\n",
    "                eval_env= eval_env,\n",
    "                callback= PPO_callbacks\n",
    "                )\n",
    "\n",
    "else:\n",
    "    print(f'\"{a}\" is geen geldige optie, de cell wordt niet uitgevoerd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate grafiek samenstellen\n",
    "Hieronder worden beide grafieken voor het vergelijken van de learning rates opgemaakt. Hiervoor gebruiken we de eerder gemaakte functies om van een .npz file naar een dataframe te komen. In een loop plaatsen we de dataframes in een lijst. Daarna maken we een lijst aan voor de legenda namen en kleuren. Dit moet gebeuren in dezelfde volgorden als de modellen in de lijst zijn geplaatst, anders klopt de grafiek niet.\n",
    "\n",
    "Omdat een grafiek van 5(seeds) x 5(learning rates) = 25(agents) te rommelig is herhalen we een techniek die we ook gebruikt hebben in de ```compare_trainings()``` functie. Er wordt namelijk een master data frame aangemaakt waar we steeds een berekende dataframe in plaatsen. De berekening doen we door voor elke dataframe met dezelfde learning rate te groeperen op timestep. Over elk van deze groepen pakken we de gemeddelde resultaten ('results' en 'ep_lengths'). Dit slaan we op in de master dataframe die we vervolgens gebruiken voor de grafiek. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty list to hold data frames that will be used in compare_trainings()\n",
    "model_dfs = []\n",
    "\n",
    "# Create individual dataframes based on nested for loop and append to list\n",
    "for i in SEED_LIST:\n",
    "    for j in LEARNING_RATES:\n",
    "        df_lr = npz_to_dataframe(np.load(fr'Breakout_model\\Training_evaluation_npz\\PPO_seed_{i}_learningrate_{j}\\evaluations.npz'))\n",
    "        model_dfs.append(df_lr)\n",
    "\n",
    "# use for loop to determine model names\n",
    "model_names = [f'SEED {i} LR {j}' for i in SEED_LIST for j in LEARNING_RATES ]\n",
    "model_names\n",
    "\n",
    "# run function to append and show figure of dataframes also setting figure size --> leaves a cluttered graph\n",
    "plt.figure(figsize=(15,4))\n",
    "compare_trainings(model_dfs, model_names).get_figure();\n",
    "plt.legend(ncol=5, loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Master dataframe containing the final columns\n",
    "m_df = pd.DataFrame(columns=['timesteps', 'results', 'ep_lengths'])\n",
    "\n",
    "# Empty list to hold aggregated dataframes\n",
    "model_dfs_cleaned = []\n",
    "\n",
    "\n",
    "for j in LEARNING_RATES:\n",
    "\n",
    "    for i in SEED_LIST:\n",
    "        # Loop over seeds and append the master dataframe with reults of all seeds for one learningrate\n",
    "        df_lr = npz_to_dataframe(np.load(fr'Breakout_model\\Training_evaluation_npz\\PPO_seed_{i}_learningrate_{j}\\evaluations.npz'))\n",
    "        m_df = m_df.append(df_lr)\n",
    "    \n",
    "    # After the inner loop group the resulting frame on timestep end get averages \n",
    "    df_cleaned = m_df.groupby(['timesteps']).mean().reset_index()\n",
    "\n",
    "    # put this in the final dataframe used for the graph\n",
    "    model_dfs_cleaned.append(df_cleaned)\n",
    "\n",
    "# create legend names\n",
    "model_names_cleaned = [f'Average of seeds for LR {j}' for j in LEARNING_RATES ]\n",
    "\n",
    "#construct and save figure --> returns the cleaned graph\n",
    "plt.figure(figsize=(15,4))\n",
    "PPO_model_learningrate_comparison = compare_trainings(model_dfs_cleaned, model_names_cleaned).figure;\n",
    "PPO_model_learningrate_comparison.savefig(r'visualisatie\\PPO_model_learningrate_comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Gamma instellen\n",
    "\n",
    "[gamma](https://stackoverflow.com/questions/54334315/what-is-the-full-meaning-of-the-discount-factor-%CE%B3-gamma-in-reinforcement-learn) ($\\gamma$) betreft de discount functie. Hierbij willen we het model in meer of mindere maten belonen voor de uitslag van toekomstige states (frames). Hoe hoger de gamma hoe belangrijker de toekomstige state in verhouding tot de directe state. Voor een simpele omgeving zoals het cartpole probleem in de introductie maakt $\\gamma()$ minder uit. Hierbij is naarmelijk het oneindig overeind blijven van de stok ongeveer gelijk aan het op dit moment overeind houden van de stok. Zodra er een moeilijker probleem optreed wordt $\\gamma()$ bepalen een stuk lastiger.\n",
    "\n",
    "Een manier om dit duidelijk te maken is met een simpel voorbeeld. Wanneer de horizon van het probleem (het aantal timesteps in de toekomst waarin het probleem opgelost moet worden) 20 is en we gebruiken $\\gamma = 0.9$ dan wordt de discount op deze toekomstige reward $\\gamma^20 = 0.12$. Doen we dit zelfde met $\\gamma = 0.99$ dan komen we op $0.99^20 = 0.82$. Vervolgens wordt deze waarde van de discount function $\\gamma()$ maal de future reward gedaan. Dus een lagere $\\gamma$ betekent een kleiner deel van de reward, hieronder een volledige uitwerking wanneer de reward 20 punten is:\n",
    "* $\\gamma = 0.9$&emsp;-->  $\\gamma^20 = 0.12$   &nbsp;  --> $0.12 * 20 = 2.4$\n",
    "* $\\gamma = 0.99$&emsp;  -->  $\\gamma^20 = 0.82$  &nbsp;   --> $0.82 * 20 = 16.4$\n",
    "\n",
    "Zo zie je dat de toekomstige reward bij een hogere gamma meer waard wordt. Maar zien we dit nou direct terug in de testen die hieronder worden gedaan? Het korte antwoord, nee. Er lijk niet direct een groot verschil te ontstaan door andere $\\gamma$ waarden te gebruiken. Wel is het te zien dat een hogere $\\gamma$, dus het belangrijk maken van de toekomst. Iets minder presteert, dit zou kunnen betekenen dat we met een $\\gamma < 9$ moeten gaan werken.\n",
    "\n",
    "![gamma training](visualisatie\\PPO_model_gamma_comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waarschuwing en aanvraag bevestiging user, resultaten zijn al beschikbaar dus er hoeft niet getrained te worden\n",
    "a = input(\n",
    "    '''\n",
    "    Het uitvoeren van deze cell kost +/- 2 uur. \n",
    "    De modellen zijn al een keer getrained en in de volgende cell kan je de resultaten terug vinden zonder dat je het model opnieuw heoft te trainen.\n",
    "    \n",
    "    wil je deze cell overslaan?\n",
    "    type: \"overslaan\"\n",
    "    \n",
    "    wil je de cell uitvoeren?\n",
    "    type: \"uitvoeren\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "if a == \"overslaan\":\n",
    "    print('Je hebt er voor gekozen de cell niet uit te voeren')\n",
    "\n",
    "elif a == \"uitvoeren\":\n",
    "    print('de cell wordt nu uitgevoerd...')\n",
    "\n",
    "    # Trainen van een PPO agent op basis van learning rate\n",
    "\n",
    "    # Create an environment based on the current itteration in te seedlist\n",
    "    for i in SEED_LIST:\n",
    "        print(f'working with seed: {i}')\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = VecFrameStack(train_env, n_stack=1)\n",
    "\n",
    "        # Loop over de gamma waarden en train elke keer een model\n",
    "        for j in GAMMA_OPTIONS:\n",
    "            print(f'working with gamma: {j}')\n",
    "            # create checkpoints and callbacks\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path= BEST_MODELS_PATH + r'\\PPO',\n",
    "                log_path= TRAIN_LOG_PATH + r'\\PPO' + '_seed_' + str(i) + '_gamma_' + str(j), # save trainingdata op juiste locatie\n",
    "                eval_freq=500,\n",
    "                verbose= 0\n",
    "                )\n",
    "\n",
    "            PPO_callbacks = [eval_callback]\n",
    "\n",
    "            # create and train model\n",
    "            PPO_model = PPO(\n",
    "                'CnnPolicy',\n",
    "                train_env,\n",
    "                verbose= 0,\n",
    "                device= 'cuda',\n",
    "                gamma= j # gebruik huidige gamma\n",
    "                )\n",
    "\n",
    "\n",
    "            PPO_model.learn(\n",
    "                total_timesteps= TIMESTEPS,\n",
    "                eval_env= eval_env,\n",
    "                callback= PPO_callbacks\n",
    "                )\n",
    "\n",
    "else:\n",
    "    print(f'\"{a}\" is geen geldige optie, de cell wordt niet uitgevoerd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder gaan we nu opnieuw een grafiek samenstellen. Dit keer van de gamma opties en de resultaten. In de voorgaande paragraaf Is het verschil in de twee codes (rommelige en opgeruimde grafiek al getoond. Hieronder gaan we daarom alleen maar de opgeruimde grafiek uitvoeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a list of dataframes with averages over the different seeds\n",
    "m_df = pd.DataFrame(columns=['timesteps', 'results', 'ep_lengths'])\n",
    "model_dfs_cleaned = []\n",
    "\n",
    "# nested loop om alle mogelijke combinaties van gamma en seeds op te halen\n",
    "for j in GAMMA_OPTIONS:\n",
    "    for i in SEED_LIST:\n",
    "        df_lr = npz_to_dataframe(np.load(fr'Breakout_model\\Training_evaluation_npz\\PPO_seed_{i}_gamma_{j}\\evaluations.npz')) # zoek naar de file op basis van loop\n",
    "        m_df = m_df.append(df_lr) # voeg de resulterende dataframe toe aan de master list\n",
    "\n",
    "    df_cleaned = m_df.groupby(['timesteps']).mean().reset_index() # zet het model in de juist vorm en bereken de gemiddelde over elke timestep\n",
    "    model_dfs_cleaned.append(df_cleaned) # voeg dit toe aan de finale dataframe lijst\n",
    "\n",
    "# itereer over de mogelijke namen voor de modellen\n",
    "model_names_cleaned = [f'Average of seeds for γ {j}' for j in GAMMA_OPTIONS ]\n",
    "\n",
    "# maak het figuur met de aangemaakt functie en sla deze daarna op\n",
    "plt.figure(figsize=(15,4))\n",
    "PPO_model_learningrate_comparison = compare_trainings(model_dfs_cleaned, model_names_cleaned).figure; # .figure is nodig om de figure object op te halen, hierop kan .savefig() worden gebruikt\n",
    "PPO_model_learningrate_comparison.savefig(r'visualisatie\\PPO_model_gamma_comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 Batch sizes instellen\n",
    "[batch size](https://deeplizard.com/learn/video/U4WB9p6ODjM) bepaalt de data die door het netwerk gaat voordat de gewichten worden aangepast. Wanneer we een netwerk trainen zijn een aantal termen die gebruikt worden met betrekking tot de lengte en de hoveelheid data. De afbeelding hieronder geeft duidelijk weer wat in iedergeval drie van deze termen inhouden:\n",
    "\n",
    "<img align=\"left\" src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F2020%2F1*AOiD8LEDWrWy5l_f9qgweQ%402x.jpeg&f=1&nofb=1\" alt=\"itteration vs batch vs epoch\" style=\"width: 22%; padding-right: 25px\"/>\n",
    "\n",
    "* **Epoch**: een cyclus waarin alle data door het netwerk is gegaag\n",
    "* **Iteration**: het aantal batches dat nodig is om tot 1 volledige epoch te gaan\n",
    "* **Batch**: een subset van de data, zodra deze door het netwerk is geggaan worden de gewichten aangepast\n",
    "\n",
    "Daarnaast kennen we ook de term **timestep**. Elke timestep is gelijk aan één itteratie, dus aan één batch door het netwerk sturen. De gebruikte terminology kan soms verschillen van package tot package maar deze beschrijving is voor nu dekkend.\n",
    "\n",
    "Maar wat is nu het effect van een kleinere of grotere batch size? Allereerst zal een model sneller zijn met een grotere batch. Alle data wordt in een keer ingeladen, de loss wordt in een keer berekend en alle gewichten worden aangepast. Dit gaat in verhouding tot een kleine batch erg snel. Bij kleine batches wordt het netwerk op kleinere stukjes van de data aangepast waardoor er meer berekeneningen nodig zijn. Dus voor tijdswinst kies je een grote batch size.\n",
    "\n",
    "Het tweede verschil is de benadering van het netwerk. Bij een kleinere batch size moet het netwerk vaker zijn gewichten aanpassen omdat deze meer 'fouten' zal maken en deze wilt corrigeren. Bij een grote batch leert het netwerk alleen over de gemiddelde fout van alle data. Deze fout zal niet veel verschillen per keer en het netwerk zal daarom alleen op de training data kunnen werken. Wanneer er met kleinere batches wordt gewerkt wordt het netwerk gedwongen om te generaliseren over meerdere (sub)datasets. Hier kan ook gelijk een randomness aan toegevoegd worden waardoor het netwerk eventueel een tijdsverband en volgordelijkheid loslaat.<br><br>\n",
    "\n",
    "\n",
    "Onderstaande afbeelding is het resultaat van het trainen van een netwerk met verschillende batche grootte en over de 5 random environments het gemiddelde te nemen. Binnen deze kleine proef is inderdaad de reactie van een lagere batchsize te zien op de score van het model. Natuurlijk weten we niet hoe dit bij langere trainingen zal zijn maar ook hier doen we de aannamen dat dit patroon zich voortzet. Voor het uiteindelijke model lijkt een batch grootte van 16 of 32 wenselijk. Dit betekent dat het trainen van een model dus langer zal kunnen gaan duren.\n",
    "\n",
    "![batch size models](visualisatie\\PPO_model_batchsize_comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in onderstaande code gebeurt eenzelfde als het trainen voor gamma en learning_rates.\n",
    "a = input(\n",
    "    '''\n",
    "    Het uitvoeren van deze cell kost +/- 2 uur. \n",
    "    De modellen zijn al een keer getrained en in de volgende cell kan je de resultaten terug vinden zonder dat je het model opnieuw heoft te trainen.\n",
    "    \n",
    "    wil je deze cell overslaan?\n",
    "    type: \"overslaan\"\n",
    "    \n",
    "    wil je de cell uitvoeren?\n",
    "    type: \"uitvoeren\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "if a == \"overslaan\":\n",
    "    print('Je hebt er voor gekozen de cell niet uit te voeren')\n",
    "\n",
    "elif a == \"uitvoeren\":\n",
    "    print('de cell wordt nu uitgevoerd...')\n",
    "\n",
    "    # Trainen van een PPO agent op basis van batch size\n",
    "    for i in SEED_LIST:\n",
    "        print(f'working with seed: {i}')\n",
    "        # Create an environment based on the current itteration in te seedlist\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = VecFrameStack(train_env, n_stack=1)\n",
    "\n",
    "        # bepaal batchsize op basis van loop\n",
    "        for j in BATCH_SIZES:\n",
    "            print(f'working with batch size: {j}')\n",
    "            # create checkpoints and callbacks\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path= BEST_MODELS_PATH + r'\\PPO',\n",
    "                log_path= TRAIN_LOG_PATH + r'\\PPO' + '_seed_' + str(i) + '_batchsize_' + str(j), # Kies de juiste training data opslag locatie\n",
    "                eval_freq=500,\n",
    "                verbose= 0\n",
    "                )\n",
    "\n",
    "            PPO_callbacks = [eval_callback]\n",
    "\n",
    "            # create and train model\n",
    "            PPO_model = PPO(\n",
    "                'CnnPolicy',\n",
    "                train_env,\n",
    "                verbose= 0,\n",
    "                device= 'cuda',\n",
    "                batch_size= j # zet de huidige batch size\n",
    "                )\n",
    "\n",
    "\n",
    "            PPO_model.learn(\n",
    "                total_timesteps= TIMESTEPS,\n",
    "                eval_env= eval_env,\n",
    "                callback= PPO_callbacks\n",
    "                )\n",
    "\n",
    "else:\n",
    "    print(f'\"{a}\" is geen geldige optie, de cell wordt niet uitgevoerd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ook hier gebeurt eenzelfde als bij learning rate en gamma\n",
    "# we openen de training data en laden het in m_df, vervolgens berekenen we averages voor de timesteps\n",
    "# de resultaten laden we in een geschoonde dataframe die we opslaan in een lijst\n",
    "# we loopen in dezelfde volgorde over de model namen en maken hier vervolgens een visualisatie van\n",
    "\n",
    "# Construct a list of dataframes with averages over the different seeds\n",
    "m_df = pd.DataFrame(columns=['timesteps', 'results', 'ep_lengths'])\n",
    "model_dfs_cleaned = []\n",
    "\n",
    "for j in BATCH_SIZES:\n",
    "    for i in SEED_LIST:\n",
    "        df_lr = npz_to_dataframe(np.load(fr'Breakout_model\\Training_evaluation_npz\\PPO_seed_{i}_batchsize_{j}\\evaluations.npz'))\n",
    "        m_df = m_df.append(df_lr)\n",
    "\n",
    "    df_cleaned = m_df.groupby(['timesteps']).mean().reset_index()\n",
    "    model_dfs_cleaned.append(df_cleaned)\n",
    "\n",
    "model_names_cleaned = [f'Average of seeds for batchsize {j}' for j in BATCH_SIZES ]\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "PPO_model_batchsize_comparison = compare_trainings(model_dfs_cleaned, model_names_cleaned).figure;\n",
    "PPO_model_batchsize_comparison.savefig(r'visualisatie\\PPO_model_batchsize_comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 Andere parameters\n",
    "\n",
    "We hebben nu drie belangrijke paramaters en de effecten gezien. In onderstaande code gaan we hetzelfde doen voor nog 3 parameters, namelijk: ```n_epochs, gae_lambda en clip_range``` Hieronder worden eerste alle drie de parameters uitgelegd, vervolgens maken we een aantal test agents en bekijken we de resultaten van deze agents net zoals bij de vorige parameters.\n",
    "\n",
    "**n_epochs**<br>\n",
    "\n",
    "Zoals eerder is uitgelegd is een epoch gelijk aan het moment dat alle data 1 keer door het neurale netwerk is gegaan. In de [documentatie](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) is terug te vinden dat deze parameter bepaald na hoeveeltijd de [*surrogate loss*](https://stats.stackexchange.com/questions/263712/what-is-a-surrogate-loss-function) wordt geoptimaliseerd. Binnen machine learning is de *loss* een bekende waarde. Het minimaliseren van de loss is vaak een van de doelstelingen. Dit is namelijk een functie die aangeeft hoe \"fout\" een model heeft voorspeld. Bij een clasificatie zal de loss doorgaans berekend worden op de false positives (wanneer het doel is de positieve klasse te voorspellen). Wanneer we alleen een normale loss proberen te optimaliseren halen we misschien een hoge precision op ons doel (positives voorspellen) maar kan de totale accuracy (true positives en true negatives) veel lager zijn. De surrogate loss kan hiervoor gebruikt worden. Binnen PPO wordt deze *surrogate loss* gebruikt om de gebruikte policy functie aan te passen, dit voorkomt dat de stappen die dit model is zijn policy functie zet te groot zou zijn (dit zou met een normale loss wel het geval zijn). De aannamen is dat, gezien het een nieuwe berekening betreft, een lagere waarde voor deze parameter zorgt voor een langere training maar een accurater model.\n",
    "\n",
    "**gae lambda**<br>\n",
    "\n",
    "Deze paramater betreft een factor voor de afweging tussen de bias en de variance. Waar doorgaans variance opgevangen kan worden door meer samples toe te voegen wilt dit bij bias niet altijd leiden tot het convergeren van een algoritme. De [generalized advantage estimator (GAE)](https://arxiv.org/pdf/1506.02438.pdf) maakt eigenlijk een afweging tussen een lagere gamma($\\gamma$) (de variantie wordt verlaagt door toekomstige beloningen minder belangrijk te maken) en heeft direct impact op de (schaal van de) value functie. $\\lambda$ (lambda) daarentegen telt eigenlijk als een extra discount functie, die achteraf wordt toegepast en niet direct op de value-/reward functie. Gestelt wordt dat $\\lambda$ lager moet zijn dan $\\gamma$. Hierbij is aangenomen dat de value functie 'accuraat' is. om een beter beeld te krijgen van de trade-off tussen gamma en lambda trainen we het model op de eerder gebruikte $\\gamma$ waarde en zetten we deze uit tegenover verschillende $\\lambda$ waarde. Om het testen te versnellen wilde ik  hiervoor de timesteps inkorten naar 400 (10000/ (aantal gamma * aantal lambda)). Helaas liep ik hier alleen tegen een error aan waarbij de environment na verloop van tijd niet gereset kon worden. Dit gebeurde rond het draaien van de tweede seed. Ik heb hier helaas geen oplossing voor gevonden en heb besloten de gae $\\lambda$ alleen maar te testen op seed 13.\n",
    "\n",
    "\n",
    "**clip range**<br>\n",
    "\n",
    "De [clip range](https://openai.com/blog/openai-baselines-ppo/) heeft invloed op de policy function. Ik kan hier helaas nog niet heel veel over vinden maar het volgende lijkt duidelijk te worden:\n",
    "1. clip-range is van toepassing op de policy function\n",
    "2. het clippen is bedoelt om de bandbreedte die uit het netwerk wordt terug gegeven te verkleinen.\n",
    "3. Een neuraal-netwerk werkt het best met genormaliseerde data (data dicht rond de 0 te brengen).<br>\n",
    "\n",
    "De cliprange zorgt dat in iedergeval de policy functie met ['geclipte data' werkt](https://www.reddit.com/r/reinforcementlearning/comments/k4oa4t/why_clip_reward_in_1_1_in_actor_critic/).\n",
    "Aangezien ik te weinig duidelijke uitleg kan vinden over de clip-range is dit voor mij expirimenteren om te zien wat er gebeurt. Wel blijkt uit het lezen van de achterliggende functie dat clip_range toegepast wordt op de surrogate loss. De parameter vraagt alleen een value (bijvoorbeeld 0.5) achter de schermen past het model deze toe als een range van -0.5 t/m 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorkom verdere warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# nieuwe constants\n",
    "EPOCH_LIST = [2, 4, 10, 20, 40]\n",
    "GAMMA_OPTIONS = [0.9999, 0.99, 0.80, 0.5, 0.05]\n",
    "GAE_LAMBDA_LIST = [0.99, 0.95, 0.8, 0.6, 0.5 ]\n",
    "CLIP_RANGE_LIST = [1, 0.5, 0.2, 0.1, 0.05 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epoch parameter training (zelfde als gamma, learning rate en batch sizes)\n",
    "a = input(\n",
    "    '''\n",
    "    Het uitvoeren van deze cell kost +/- 2 uur. \n",
    "    De modellen zijn al een keer getrained en in de volgende cell kan je de resultaten terug vinden zonder dat je het model opnieuw heoft te trainen.\n",
    "    \n",
    "    wil je deze cell overslaan?\n",
    "    type: \"overslaan\"\n",
    "    \n",
    "    wil je de cell uitvoeren?\n",
    "    type: \"uitvoeren\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "if a == \"overslaan\":\n",
    "    print('Je hebt er voor gekozen de cell niet uit te voeren')\n",
    "\n",
    "elif a == \"uitvoeren\":\n",
    "    print('de cell wordt nu uitgevoerd...')\n",
    "\n",
    "    # Trainen van een PPO agent op basis van n_epoch surrogate loss optimalisatie\n",
    "\n",
    "    for i in SEED_LIST:\n",
    "        print(f'working with seed: {i}')\n",
    "        # Create an environment based on the current itteration in te seedlist\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = VecFrameStack(train_env, n_stack=1)\n",
    "\n",
    "        for j in EPOCH_LIST:\n",
    "            print(f'working with n_epochs: {j}')\n",
    "            \n",
    "            # create checkpoints and callbacks\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path= BEST_MODELS_PATH + r'\\PPO',\n",
    "                log_path= TRAIN_LOG_PATH + r'\\PPO' + '_seed_' + str(i) + 'n_epochs' + str(j),\n",
    "                eval_freq=500,\n",
    "                verbose= 0\n",
    "                )\n",
    "\n",
    "            PPO_callbacks = [eval_callback]\n",
    "\n",
    "            # create and train model\n",
    "            PPO_model = PPO(\n",
    "                'CnnPolicy',\n",
    "                train_env,\n",
    "                verbose= 0,\n",
    "                device= 'cuda',\n",
    "                n_epochs= j\n",
    "                )\n",
    "\n",
    "\n",
    "            PPO_model.learn(\n",
    "                total_timesteps= TIMESTEPS,\n",
    "                eval_env= eval_env,\n",
    "                callback= PPO_callbacks\n",
    "                )\n",
    "\n",
    "else:\n",
    "    print(f'\"{a}\" is geen geldige optie, de cell wordt niet uitgevoerd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epoch parameter visualisation (zelfde als gamma, learning rate en batch sizes)\n",
    "m_df = pd.DataFrame(columns=['timesteps', 'results', 'ep_lengths'])\n",
    "model_dfs_cleaned = []\n",
    "\n",
    "for j in EPOCH_LIST:\n",
    "    for i in SEED_LIST:\n",
    "        df_lr = npz_to_dataframe(np.load(fr'Breakout_model\\Training_evaluation_npz\\PPO_seed_{i}n_epochs{j}\\evaluations.npz'))\n",
    "        m_df = m_df.append(df_lr)\n",
    "\n",
    "    df_cleaned = m_df.groupby(['timesteps']).mean().reset_index()\n",
    "    model_dfs_cleaned.append(df_cleaned)\n",
    "\n",
    "model_names_cleaned = [f'Average of seeds for n_epochs {j}' for j in EPOCH_LIST ]\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "PPO_model_batchsize_comparison = compare_trainings(model_dfs_cleaned, model_names_cleaned).figure;\n",
    "PPO_model_batchsize_comparison.savefig(r'visualisatie\\PPO_model_n_epochs_comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder wordt een model getrained met verschillende gae $\\lambda$ waarden. Zoals in 6.3.4 toegelicht liep ik hier tijdens het trainen tegen een error aan. De bedoeling was hetzelfde als voorgaande trainingen, om 5 seeds te gebruiken en te middelen. Hier liep ik alleen bij het overgaan naar de tweede seed tegen een error aan, blijkbaar kon de omgeving niet meer gereset worden. Normaal zou de learnstap dit zelf doen. Om toch meer controle te hebben heb ik geprobeert deze reset zelf uit te voeren op verschillende locaties in de loop. Maar het probleem bleef voorkomen. Om deze reden heb ik gekozen alleen op seed 13 te richten om toch een indruk te krijgen van de impact van de gae $\\lambda$ functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gae_lambda parameter training (zelfde als gamma, learning rate en batch sizes)\n",
    "a = input(\n",
    "    '''\n",
    "    Het uitvoeren van deze cell kost +/- 2 uur. \n",
    "    De modellen zijn al een keer getrained en in de volgende cell kan je de resultaten terug vinden zonder dat je het model opnieuw heoft te trainen.\n",
    "    \n",
    "    wil je deze cell overslaan?\n",
    "    type: \"overslaan\"\n",
    "    \n",
    "    wil je de cell uitvoeren?\n",
    "    type: \"uitvoeren\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "if a == \"overslaan\":\n",
    "    print('Je hebt er voor gekozen de cell niet uit te voeren')\n",
    "\n",
    "elif a == \"uitvoeren\":\n",
    "    print('de cell wordt nu uitgevoerd...')\n",
    "\n",
    "    # Trainen van een PPO agent op basis van n_epoch surrogate loss optimalisatie\n",
    "\n",
    "    for i in SEED_LIST[0]:\n",
    "        print(f'working with seed: {i}')\n",
    "        # Create an environment based on the current itteration in te seedlist\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = VecFrameStack(train_env, n_stack=1)\n",
    "        \n",
    "        for g in GAMMA_OPTIONS:\n",
    "\n",
    "\n",
    "            for l in GAE_LAMBDA_LIST:\n",
    "                print(\n",
    "                    f'''\n",
    "                    working with gamma: {g}\n",
    "                    and Gae lambda factor : {l}\n",
    "                    '''\n",
    "                    )\n",
    "                \n",
    "                # create checkpoints and callbacks\n",
    "                eval_callback = EvalCallback(\n",
    "                    eval_env,\n",
    "                    best_model_save_path= BEST_MODELS_PATH + r'\\PPO',\n",
    "                    log_path= TRAIN_LOG_PATH + r'\\PPO' + '_seed_' + str(i) + 'gamma_' + str(g) + 'gae_factor_' + str(l),\n",
    "                    eval_freq=500,\n",
    "                    verbose= 0\n",
    "                    )\n",
    "\n",
    "                PPO_callbacks = [eval_callback]\n",
    "\n",
    "                # create and train model\n",
    "                PPO_model = PPO(\n",
    "                    'CnnPolicy',\n",
    "                    train_env,\n",
    "                    verbose= 0,\n",
    "                    device= 'cuda',\n",
    "                    gamma= g,\n",
    "                    gae_lambda= l\n",
    "                    )\n",
    "\n",
    "\n",
    "                PPO_model.learn(\n",
    "                    total_timesteps= (TIMESTEPS / 1000),\n",
    "                    eval_env= eval_env,\n",
    "                    callback= PPO_callbacks\n",
    "                    )\n",
    "\n",
    "                eval_env.reset()\n",
    "                # train_env.reset()\n",
    "else:\n",
    "    print(f'\"{a}\" is geen geldige optie, de cell wordt niet uitgevoerd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omdat de GAE $\\lambda$ functie een trade-off maakt tussen variance en bias op basis van de $\\gamma$ waarde moet de visualisatie anders worden weergegeven. De GAE $\\lambda$ functie opzichzelf zou niet genoeg zeggen. Om deze reden wordt de reward geplot in een heatmap waarbij x en y de  $\\gamma$ en $\\lambda$ values zijn en een lichtere kleur de gelijk staat aan een hogere reward. In onderstaande visual lijkt het er op dat $\\gamma = 0.5$ en $\\lambda = 0.99$ het beste resultaat oplevert. Dit lijkt verder overeen te komen met de test waar alleen $\\gamma$ werd getuned.\n",
    "\n",
    "![gae lambda trade-off](visualisatie\\PPO_model_GAE_lambda_comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gae_lambda parameter visualisation\n",
    "m_df = pd.DataFrame(columns=['timesteps', 'results', 'ep_lengths', 'GAE_Lambda', 'Gamma'])\n",
    "model_dfs_cleaned = []\n",
    "\n",
    "# dit keer loopen we niet over seeds maar over combinaties van gamma en lambda\n",
    "for g in GAE_LAMBDA_LIST:\n",
    "    for j in GAMMA_OPTIONS:\n",
    "    \n",
    "        df_lr = npz_to_dataframe(np.load(fr'Breakout_model\\Training_evaluation_npz\\PPO_seed_13gamma_{j}gae_factor_{g}\\evaluations.npz'))\n",
    "        df_lr['GAE λ'] = g # maak een kolom aan met de gamma waarde\n",
    "        df_lr['γ'] = j # maak een kolom aan met de lambda waarde\n",
    "        df_lr = df_lr[df_lr['timesteps'] == 2000] # filter de dataframe op 2000 timesteps (dit keer geen average maar het uiteindelijke resultaat)\n",
    "        m_df = m_df.append(df_lr)\n",
    "\n",
    "# pivot de dataframe zodat lambda de index is en gamma de kolom namen zijn\n",
    "m_df_pivot = m_df[['GAE λ', 'γ', 'results']].pivot('GAE λ', 'γ', 'results')\n",
    "\n",
    "# eventueel kan deze parameter meegegeven worden in sns.set_theme(rc=rc). Dit helpt bij visualisaties bij een notebook in darkmode\n",
    "rc =  {\n",
    "    'axes.labelcolor': 'white'\n",
    "    ,'figure.facecolor': '#1e1e1e'\n",
    "    , 'text.color': 'white'\n",
    "    , 'xtick.color': 'white'\n",
    "    , 'ytick.color': 'white'\n",
    "    }\n",
    "\n",
    "# bepaal eventueel een ander thema of gebruik het standaard thema\n",
    "sns.set_theme()\n",
    "\n",
    "# maak een heatmap en zet een kleurenspectrum geschikt voor coninue variabelen\n",
    "sns.heatmap(m_df_pivot, cmap= 'rocket')\n",
    "\n",
    "# bepaal titel (suptitel) en onder titel (title)\n",
    "plt.title('Lighter color indicates higher score', fontsize= 10)\n",
    "plt.suptitle('Heatmap GAE λ vs. γ at 2000 timesteps', y=1.0, x=0.435, fontsize= 16);\n",
    "\n",
    "# sla het figuur op zodat deze gebruikt kan worden in het notebook\n",
    "plt.savefig(r'visualisatie\\PPO_model_GAE_lambda_comparison')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_range parameter training (zelfde als gamma, learning rate en batch sizes)\n",
    "a = input(\n",
    "    '''\n",
    "    Het uitvoeren van deze cell kost +/- 2 uur. \n",
    "    De modellen zijn al een keer getrained en in de volgende cell kan je de resultaten terug vinden zonder dat je het model opnieuw heoft te trainen.\n",
    "    \n",
    "    wil je deze cell overslaan?\n",
    "    type: \"overslaan\"\n",
    "    \n",
    "    wil je de cell uitvoeren?\n",
    "    type: \"uitvoeren\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "if a == \"overslaan\":\n",
    "    print('Je hebt er voor gekozen de cell niet uit te voeren')\n",
    "\n",
    "elif a == \"uitvoeren\":\n",
    "    print('de cell wordt nu uitgevoerd...')\n",
    "\n",
    "    # Trainen van een PPO agent op basis van n_epoch surrogate loss optimalisatie\n",
    "\n",
    "    for i in SEED_LIST:\n",
    "        print(f'working with seed: {i}')\n",
    "        # Create an environment based on the current itteration in te seedlist\n",
    "        eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= i)\n",
    "        train_env = VecFrameStack(train_env, n_stack=1)\n",
    "\n",
    "        for j in CLIP_RANGE_LIST:\n",
    "            print(f'working with clip range: {j}')\n",
    "            \n",
    "            # create checkpoints and callbacks\n",
    "            eval_callback = EvalCallback(\n",
    "                eval_env,\n",
    "                best_model_save_path= BEST_MODELS_PATH + r'\\PPO',\n",
    "                log_path= TRAIN_LOG_PATH + r'\\PPO' + '_seed_' + str(i) + 'clip_range_' + str(j),\n",
    "                eval_freq=500,\n",
    "                verbose= 0\n",
    "                )\n",
    "\n",
    "            PPO_callbacks = [eval_callback]\n",
    "\n",
    "            # create and train model\n",
    "            PPO_model = PPO(\n",
    "                'CnnPolicy',\n",
    "                train_env,\n",
    "                verbose= 0,\n",
    "                device= 'cuda',\n",
    "                clip_range= j\n",
    "                )\n",
    "\n",
    "\n",
    "            PPO_model.learn(\n",
    "                total_timesteps= TIMESTEPS,\n",
    "                eval_env= eval_env,\n",
    "                callback= PPO_callbacks\n",
    "                )\n",
    "\n",
    "else:\n",
    "    print(f'\"{a}\" is geen geldige optie, de cell wordt niet uitgevoerd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_range parameter visualisation (zelfde als gamma, learning rate en batch sizes)\n",
    "m_df = pd.DataFrame(columns=['timesteps', 'results', 'ep_lengths'])\n",
    "model_dfs_cleaned = []\n",
    "\n",
    "for j in CLIP_RANGE_LIST:\n",
    "    for i in SEED_LIST:\n",
    "        df_lr = npz_to_dataframe(np.load(fr'Breakout_model\\Training_evaluation_npz\\PPO_seed_{i}clip_range_{j}\\evaluations.npz'))\n",
    "        m_df = m_df.append(df_lr)\n",
    "\n",
    "    df_cleaned = m_df.groupby(['timesteps']).mean().reset_index()\n",
    "    model_dfs_cleaned.append(df_cleaned)\n",
    "\n",
    "model_names_cleaned = [f'Average of seeds for clip range {j}' for j in CLIP_RANGE_LIST ]\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "PPO_model_batchsize_comparison = compare_trainings(model_dfs_cleaned, model_names_cleaned).figure;\n",
    "PPO_model_batchsize_comparison.savefig(r'visualisatie\\PPO_model_clip_range_comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Model langdurig trainen, opslaan en laden\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/save_format.html <br>\n",
    "https://stable-baselines.readthedocs.io/en/master/guide/examples.html <br>\n",
    "https://github.com/hill-a/stable-baselines/issues/599 <br>\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/imitation.html <br>\n",
    "\n",
    "We hebben nu 6 parameters getest en gezien hoe deze het model kunnen beïnvloeden. In de volgende cellen gaan we een model langdurig trainen met de beste parameters. Hiermee is de hoop dat de gekozen parameters samen ook goed werk verichten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.atari_wrappers import WarpFrame\n",
    "\n",
    "# Create an environment based on the current itteration in te seedlist\n",
    "eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "train_env = make_atari_env('Breakout-v0', n_envs = 4, seed= 13)\n",
    "train_env = VecFrameStack(train_env, n_stack=4)\n",
    "\n",
    "\n",
    "# create checkpoints and callbacks\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=25000,\n",
    "    save_path= MODEL_CHECKPOINT_PATH +  r'\\PPO_tuned_second_try',\n",
    "     name_prefix='PPO_tuned_second_try'\n",
    "     )\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path= BEST_MODELS_PATH + r'\\PPO_tuned_second_try',\n",
    "    log_path= TRAIN_LOG_PATH + r'\\PPO' + '_tuned_second_try',\n",
    "    eval_freq=500,\n",
    "    verbose= 0\n",
    "    )\n",
    "\n",
    "PPO_callbacks = [checkpoint_callback, eval_callback]\n",
    "\n",
    "# create and train model\n",
    "PPO_model = PPO(\n",
    "    'CnnPolicy',\n",
    "    train_env,\n",
    "    verbose= 0,\n",
    "    device= 'cuda:0',\n",
    "    learning_rate= 1e-4,\n",
    "    gamma= 0.98,\n",
    "    batch_size= 32,\n",
    "    n_epochs= 10,\n",
    "    # gae_lambda= 0.99,\n",
    "    clip_range=0.15     \n",
    "    )\n",
    "\n",
    "\n",
    "PPO_model.learn(\n",
    "    total_timesteps= 2000000,\n",
    "    callback= PPO_callbacks\n",
    "    )\n",
    "\n",
    "PPO_model.save(path = r'Breakout_model\\Saved_Model\\PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 | Score: [2.]\n",
      "Episode: 2 | Score: [2.]\n",
      "Episode: 3 | Score: [2.]\n",
      "Episode: 4 | Score: [0.]\n",
      "Episode: 5 | Score: [0.]\n",
      "Episode: 6 | Score: [0.]\n",
      "Episode: 7 | Score: [0.]\n",
      "Episode: 8 | Score: [0.]\n",
      "Episode: 9 | Score: [5.]\n",
      "Episode: 10 | Score: [4.]\n",
      "Episode: 11 | Score: [8.]\n",
      "Episode: 12 | Score: [0.]\n",
      "Episode: 13 | Score: [4.]\n",
      "Episode: 14 | Score: [0.]\n",
      "Episode: 15 | Score: [0.]\n",
      "Episode: 16 | Score: [0.]\n",
      "Episode: 17 | Score: [5.]\n",
      "Episode: 18 | Score: [0.]\n",
      "Episode: 19 | Score: [0.]\n",
      "Episode: 20 | Score: [0.]\n"
     ]
    }
   ],
   "source": [
    "train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "train_env = VecFrameStack(train_env, n_stack=4)\n",
    "agent = PPO.load(r'Breakout_model\\Best_model\\PPO_tuned_second_try\\best_model.zip')\n",
    "run_agent(train_env, agent, episodes= 20, render= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## install session_info om een requirements.txt file voor de huidige jupyter sessie samen te stellen\n",
    "# %pip install session_info\n",
    "# import session_info\n",
    "# session_info.show(dependencies= True, write_req_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.0 Bronnen\n",
    "[3 hour course](https://www.youtube.com/watch?v=Mut_u40Sqz4)<br>\n",
    "[baseline docummentatie](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5041ba25690102580176b8f7b4720df17ebf4bc4d40fd191e3f1a5800ffdb29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
