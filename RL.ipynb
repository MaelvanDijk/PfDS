{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voorwoord en opdracht\n",
    "Hoe ontsluit je data en verwerk je het tot bruikbare inzichten? Python biedt heel veel mogelijkheden \n",
    "om data te ontsluiten, te prepareren en te analyseren. Aan al deze mogelijkheden heb je alleen wat als \n",
    "je ze ook echt toe kunt passen. Aan jou de opdracht om de technieken die je tijdens de colleges hebt \n",
    "geleerd in de praktijk te brengen. Dit doe je door een eigen dataproject uit te voeren: zelf relevante \n",
    "data zoeken en inladen, deze data opschonen en combineren tot relevante inzichten.  \n",
    "\n",
    "## Beoordeling \n",
    "We zijn daarom op zoek naar een originele toepassing waarmee je de inhoud van de college kan \n",
    "vertalen in goed werkende en netjes gedocumenteerde python code. De specifieke scores en weging \n",
    "van deze criteria kun je op de volgende pagina vinden. \n",
    "\n",
    "\n",
    "| **Originaliteit** | **Inhoud** | **Code**| **Documentatie**|\n",
    "|---|---|---|---|\n",
    "| Het project omvat een unieke toepassing die in college niet aan bod is gekomen en de student weet hiermee perfect gebruik te maken van de kracht van python. | Alle data-analysestappen worden doorlopen Hierbij wordt aandacht geschonken aan alle geleerde onderwerpen. Geleerde packages worden verrijkt met code uit meerdere andere bronnen | De gehele code werkt en geeft de juiste resultaten weer. Fouten worden automatisch en correct afgehandeld. De code is volledig modulair en werkt met minimale aanpassingen ook op andere datasets. | De code, documentatie en bijpassende titels en teksten maken van het project een duidelijk geheel dat door anderen zonder verdere uitleg zou kunnen worden begrepen |\n",
    "## Deadline \n",
    "We ontvangen je project graag uiterlijk 1 februari, voor (14:00) via Canvas. Mocht je project tot een \n",
    "onvoldoende beoordeling leiden, dan heb je nog een keer de kans om een herkansing in te leveren. \n",
    "Het is je eigen verantwoordelijkheid om op tijd met je project te starten, we zullen tussendoor dus niet \n",
    "controleren hoe ver je bent of gedeeltelijke projecten beoordelen. \n",
    "Tijdens het laatste college vragen we je om collega studenten een korte presentatie te geven over je \n",
    "project. Die presentatie wordt zelf niet beoordeeld maar geeft je wel de kans om te laten zien hoe je \n",
    "met criteria zoals originaliteit of onvoorziene tegenslagen bent omgegaan.   \n",
    "We raden je aan om de code in Jupyter notebooks te schrijven, maar het staat je vrij om daar een \n",
    "ander programma voor te kiezen zolang wij aan het eind van het project de code en resultaten kunnen \n",
    "beoordelen.  \n",
    "\n",
    "## Onderwerp \n",
    "Omdat we het leuk vinden als je de geleerde python vaardigheden meteen in de praktijk kan \n",
    "toepassen, willen we je vragen om zelf een onderwerp te kiezen. Tijdens de colleges zullen we een \n",
    "aantal voorbeelden geven van eerdere projecten. \n",
    "Mocht je nog vragen hebben over het project dan kun je je vragen het best stellen op de Canvas-\n",
    "cursuspagina. Natuurlijk kun je ons ook altijd mailen of tijdens het college aanspreken.\n",
    "\n",
    "## Wat moet er aan de pas komen\n",
    "\n",
    "### Masterclass 0\n",
    "- ~~Variables~~\n",
    "- ~~Calculations / Arithmetics~~\n",
    "- Statements & Expressions\n",
    "- Datatypes\n",
    "- Operators\n",
    "\n",
    "### Masterclass 1\n",
    "- if, else and elif-statements\n",
    "- One-line if-statements\n",
    "- Conditional expression\n",
    "- ~~While-loop~~\n",
    "- ~~For-loop~~\n",
    "\n",
    "### Masterclass 2\n",
    "- functions, parameters and arguments\n",
    "- ~~docstrings~~\n",
    "- classes (niet verplicht)\n",
    "- imports\n",
    "\n",
    "\n",
    "### Masterclass 3\n",
    "\n",
    "### Masterclass 4\n",
    "\n",
    "### Masterclass 5\n",
    "\n",
    "### Masterclass 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL)\n",
    "In deze notebook ga ik een eerste simpele kennismaking met Reinforcement Learning doen. Reinforcement Learning, RL in het kort, is een tak van machinelearning/deeplearning waarbij er een **agent** wordt aangemaakt die in een **environment** diverse **actions** onderneemt en hiervoor een reward **krijgt**. Het eerste deel van dit notebook wordt gewijt aan het implementeren van een basale RL agent in een voorafgemaakte environment. \n",
    "\n",
    "Zodra de basis van RL is uitgewerkt (Sectie 1 t/m 5) gaan we verder met het toepassen van de kennis op een andere omgeving (Sectie 6). Als dit allemaal geslaagd is gaan we proberen een custom environment te maken voor Pac-Man.\n",
    "\n",
    "## 1.0 Wat is RL in het kort?\n",
    "Met reinforcement Learning wordt geprobeerd een agent te trainen de juiste beslissingen te maken. Beslissingen worden gemaakt in een bepaalde omgeving (bijvoorbeeld een lab of een videospel). Om de analogie van het videospel aan te houden: de agent (mario) voert een actie uit (springen) en maakt een observatie (een muntje komt uit een blokje). Deze observaties en acties worden aan elkaar gekoppeld doormiddel van een beloningssysteem. Gedurende veel itteraties zal de agent trainen en het beste resultaat proberen te behalen.\n",
    "\n",
    "Binnen RL doen we een aantal aannamens of stellen we een paar beperkingen:\n",
    "* Het probleem is (zeer) complex, anders is RL overkill\n",
    "* Het betreft een Markov achtige omgeving --> elke actie heeft een reactie ofwel observaties en acties volgen elkaar op\n",
    "* trainen kan lang duren en is niet altijd stabiel\n",
    "\n",
    "\n",
    "## 1.1 RL raamwerk\n",
    "In onderstaande afbeelding is schematisch weergegeven hoe het raamwerk van Reinforcement Learning inwerking treed. De agent (Pacman) voert een actie uit (linksaf, rechtsaf, omlaag, omhoog), hij eet daardoor meer bolletjes en zijn scoren stijgt (imediate/shaped reward). De spookjes bewegen ook, de algehele staat van het speelveld komt als observatie binnen bij de agent die vervolgens weer een actie neemt. Deze cyclus herhaalt zich tot de agent het level heeft gehaald of af is gegaan. Zodra de agent het level haalt zal deze nog een reward krijgen (sparse rewards). Dit geheel vormt het raamwerk voor RL.\n",
    "\n",
    "\n",
    "![Reinforcement Learning Framework](https://miro.medium.com/max/1400/1*CjLFVeYssOIJaeijrxPHPg.png)\n",
    "\n",
    "Er zijn twee nieuwe termen benoemd ten opzichten van de mario analogie:\n",
    "* *Sparse Rewards*: RL problemen hebben vaak te maken met een belonging die 1-to-many staan (bijv. 1x een reward op elke 1000 zetten/spellen). Dit concept worden dan ook *Sparse rewards* genoemd en vormt een van de uitdagingen van RL ten opzichten van traditionele ML. In Mario en PacMan zijn het halen van levels voorbeelden van *Sparse Rewards*\n",
    "* *Imediate/shaped Rewards*: In bepaalde environments kunnen *imediate/shaped rewards* worden gegeven. In Mario zouden dit muntjes zijn en vijanden verslaan, in PacMan zou dit zijn wanneer de bolletjes en spookjes opgegeten worden. Dit proces van additionele beloningen toekennen kan er voor zorgen dat agent beter leert wat deze moet doen om tot een goed resultaat te komen. Hier schuilt echter ook het gevaar, het algoritme kan te gretig worden en daardoor de voorkeur gaan geven aan vroegtijdige belonginge ten opzichten van de echte beloning aan het einde van het spel.\n",
    "\n",
    "## 1.2 RL formules\n",
    "\n",
    "Om de agent te trainen staan een aantal functies centraal. Namelijk de *Reward function*, *Value funtion* en de *Policy function*. In deze paragraaf bespreken we deze functies in het kort om een beeld te krijgen van wat er onder de motorkap gebeurt:\n",
    "\n",
    "### 1.2.1 *Reward function*\n",
    "de beloning die een agent krijgt voor een bepaalde actie. deze functie kan als onderstaande worden opgeschreven:\n",
    "\n",
    "$$r_{t} = R(s_{t},a_{t},s_{t+1})$$\n",
    "hierin is *r* de beloning, *t* de huidige tijdstap, *R()* een specifieke beloningsfunctie, *s* de huidige obsertvatie/state, *a* de genomen actie. De formule stelt dus \"De huidige beloning, is gelijk aan de functie van de huidige staat de huidige actie en de staat/observatie van de volgende tijdstap.\" Deze functie wordt vaak versimpeld naar R() van de huidige staat of R() van de huidige actie/staat combinatie.\n",
    "\n",
    "Het doel is dus om de cummulatieve beloning over een periode te maximalizeren. De benadering van deze cummulatieve beloning bepaalt de functie van R() hieronder worden twee varianten van de functie besproken.<br> \n",
    "\n",
    "allereerst: **finite-horizon undiscounted return** waarin de functie puur een opsomming is van de belongingen over *n* tijdsstappen.<br>\n",
    "\n",
    "De tweede variant heet: **infinite-horizon discounted return**, hierbij worden de beloningen van alle tijdstappen meegenomen, echter hoeverder de tijdstap (en beloning) in de toekomst ligt, hoe groter de discount. Dit betekent dat een beloning die over 100*t* plaatsvindt minder waard is dan de beloning over 1*t*. De formules voor deze functies zijn respectievelijk:\n",
    "$$R(\\tau) = \\sum_ {t=0}^T\\ r_{t}$$\n",
    "$$R(\\tau) = \\sum_ {t=0}^\\infty \\ r_{t} $$\n",
    "\n",
    "$\\tau$ is in dit geval een opeenvolging van staten en acties: $\\tau = (S_{0},a_{0},S_{1},a_{0}, \\cdots)$ dit wordt ookwel *trajectory, episodes, of rollouts* genoemd. De start staat is hierbij vaak willekeurig gekozen. de volgende staten worden bepaald door de genomen actie van de agent.\n",
    "\n",
    "### 1.2.2 *Value function*\n",
    "De value function kijkt naar de huidige *value* van een staat/observatie. In dit geval wordt met *value* het volgende bedoeld: de verwachte beloning voor de huidige staat of staat-actie combinatie wanneer één policy gekozen wordt en hier niet meer van afgeweken wordt. Van deze value functies zijn er 4 belangrijke te benoemen:\n",
    "\n",
    "1. **On-Policy Value Function**: $V^{\\pi}(s)$ geeft de verwachte return (cummulatieve beloning) wanneer je start in staat *s* en altijd policy $\\pi$ aanhoudt:\n",
    "$$V^{\\pi}(s) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "2. **On-Policy Action-Value Function**: $Q^{\\pi}(s,a)$ de verwachte return wanneer een willekeurige actie wordt genomen vanaf de eerste state endaaropvolgend alleen maar de gekozen policy inzet:\n",
    "$$Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "3. **Optimal Value Function**: $V^{*}(s)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen:\n",
    "$$V^{*}(s) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s]$$ \n",
    "<br>\n",
    "\n",
    "4. **Optimal Action-Value Function**: $Q^{*}(s,a)$ de verwachte return gegeven de start staat *s* en vervolgense de optimale policy binnen de environment continu toe te passen na eerst eenmalig een willekeurige actie te hebben gekozen:\n",
    "$$Q^{*}(s,a) = max_{\\pi} E_{\\tau \\sim \\pi}[R(\\tau) | s_{0} = s, a_{0} = a]$$ \n",
    "<br>\n",
    "\n",
    "### 1.2.3 Policy function\n",
    "De policy function (of agent function). Is een functie die aangeeft welke actie een agent neemt. Deze agent en policy termen worden vaak doorelkaar gebruikt. Zo kan policy in: \"de policy probeert de beloning te maximalizeren\" vervangen worden door \"de agent\". Er zijn twee variante van de policy function, namelijk:\n",
    "\n",
    "1. deterministisch, toeval speelt hier geen rol, zijn de begin voorwaarden gelijk dan zal de uitkomst ook gelijk zijn. In dit geval wordt de policy function aangegeven als $a_{t} = \\mu(s_{t})$. Hier staat \"de huidige actie wordt bepaald door de functie $\\mu()$ gegeven de huidige staat\n",
    "\n",
    "2. Stochastisch, in dit geval speelt toeval een rol. De uitkomst is van te voren nog niet bekend. Hier wordt gekeken naar de kans van een actie - vervolg staat . De functie kan opgeschreven worden als: $a_{t} \\sim \\pi(a|s_{t})$ wat betekent dat \"de actie evenredig is met de policy functie $\\pi()$ van een kans gegeven staat *t*.\" ofwel \"kies de actie die de meeste waarschijnlijkheid heeft om te leiden tot de volgende beste staat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python 3.9.7 is nodig voor het gebruik van deze modules\n",
    "# !pip install stable-baselines3[extra] ## docummentatie voor deze module die RL algoritmes bevat: https://stable-baselines3.readthedocs.io/en/master/\n",
    "# !pip install gym\n",
    "# !pip install pyglet ## extra dependency voor OpenAI gym\n",
    "!pip list -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying adventure.bin from Roms\\Roms\\ROMS\\Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\adventure.bin\n",
      "copying air_raid.bin from Roms\\Roms\\ROMS\\Air Raid (Men-A-Vision) (PAL) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\air_raid.bin\n",
      "copying alien.bin from Roms\\Roms\\ROMS\\Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\alien.bin\n",
      "copying amidar.bin from Roms\\Roms\\ROMS\\Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\amidar.bin\n",
      "copying assault.bin from Roms\\Roms\\ROMS\\Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\assault.bin\n",
      "copying asterix.bin from Roms\\Roms\\ROMS\\Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\asterix.bin\n",
      "copying atlantis.bin from Roms\\Roms\\ROMS\\Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\atlantis.bin\n",
      "copying bank_heist.bin from Roms\\Roms\\ROMS\\Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\bank_heist.bin\n",
      "copying battle_zone.bin from Roms\\Roms\\ROMS\\Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\battle_zone.bin\n",
      "copying beam_rider.bin from Roms\\Roms\\ROMS\\Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\beam_rider.bin\n",
      "copying berzerk.bin from Roms\\Roms\\ROMS\\Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\berzerk.bin\n",
      "copying bowling.bin from Roms\\Roms\\ROMS\\Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\bowling.bin\n",
      "copying boxing.bin from Roms\\Roms\\ROMS\\Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\boxing.bin\n",
      "copying breakout.bin from Roms\\Roms\\ROMS\\Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\breakout.bin\n",
      "copying carnival.bin from Roms\\Roms\\ROMS\\Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\carnival.bin\n",
      "copying centipede.bin from Roms\\Roms\\ROMS\\Centipede (1983) (Atari - GCC) (CX2676) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\centipede.bin\n",
      "copying chopper_command.bin from Roms\\Roms\\ROMS\\Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\chopper_command.bin\n",
      "copying crazy_climber.bin from Roms\\Roms\\ROMS\\Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\crazy_climber.bin\n",
      "copying defender.bin from Roms\\Roms\\ROMS\\Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\defender.bin\n",
      "copying demon_attack.bin from Roms\\Roms\\ROMS\\Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\demon_attack.bin\n",
      "copying donkey_kong.bin from Roms\\Roms\\ROMS\\Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\donkey_kong.bin\n",
      "copying double_dunk.bin from Roms\\Roms\\ROMS\\Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\double_dunk.bin\n",
      "copying elevator_action.bin from Roms\\Roms\\ROMS\\Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\elevator_action.bin\n",
      "copying enduro.bin from Roms\\Roms\\ROMS\\Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\enduro.bin\n",
      "copying fishing_derby.bin from Roms\\Roms\\ROMS\\Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\fishing_derby.bin\n",
      "copying freeway.bin from Roms\\Roms\\ROMS\\Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\freeway.bin\n",
      "copying frogger.bin from Roms\\Roms\\ROMS\\Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\frogger.bin\n",
      "copying frostbite.bin from Roms\\Roms\\ROMS\\Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\frostbite.bin\n",
      "copying galaxian.bin from Roms\\Roms\\ROMS\\Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\galaxian.bin\n",
      "copying gopher.bin from Roms\\Roms\\ROMS\\Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\gopher.bin\n",
      "copying gravitar.bin from Roms\\Roms\\ROMS\\Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\gravitar.bin\n",
      "copying hero.bin from Roms\\Roms\\ROMS\\H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\hero.bin\n",
      "copying ice_hockey.bin from Roms\\Roms\\ROMS\\Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\ice_hockey.bin\n",
      "copying jamesbond.bin from Roms\\Roms\\ROMS\\James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\jamesbond.bin\n",
      "copying journey_escape.bin from Roms\\Roms\\ROMS\\Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\journey_escape.bin\n",
      "copying kaboom.bin from Roms\\Roms\\ROMS\\Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\kaboom.bin\n",
      "copying kangaroo.bin from Roms\\Roms\\ROMS\\Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\kangaroo.bin\n",
      "copying keystone_kapers.bin from Roms\\Roms\\ROMS\\Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\keystone_kapers.bin\n",
      "copying king_kong.bin from Roms\\Roms\\ROMS\\King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\king_kong.bin\n",
      "copying koolaid.bin from Roms\\Roms\\ROMS\\Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\koolaid.bin\n",
      "copying krull.bin from Roms\\Roms\\ROMS\\Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\krull.bin\n",
      "copying kung_fu_master.bin from Roms\\Roms\\ROMS\\Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\kung_fu_master.bin\n",
      "copying laser_gates.bin from Roms\\Roms\\ROMS\\Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\laser_gates.bin\n",
      "copying montezuma_revenge.bin from Roms\\Roms\\ROMS\\Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\montezuma_revenge.bin\n",
      "copying mr_do.bin from Roms\\Roms\\ROMS\\Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\mr_do.bin\n",
      "copying ms_pacman.bin from Roms\\Roms\\ROMS\\Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\ms_pacman.bin\n",
      "copying name_this_game.bin from Roms\\Roms\\ROMS\\Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\name_this_game.bin\n",
      "copying pacman.bin from Roms\\Roms\\ROMS\\Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pacman.bin\n",
      "copying phoenix.bin from Roms\\Roms\\ROMS\\Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\phoenix.bin\n",
      "copying video_pinball.bin from Roms\\Roms\\ROMS\\Pinball (AKA Video Pinball) (Zellers).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\video_pinball.bin\n",
      "copying pitfall.bin from Roms\\Roms\\ROMS\\Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pitfall.bin\n",
      "copying pooyan.bin from Roms\\Roms\\ROMS\\Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pooyan.bin\n",
      "copying private_eye.bin from Roms\\Roms\\ROMS\\Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\private_eye.bin\n",
      "copying qbert.bin from Roms\\Roms\\ROMS\\Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\qbert.bin\n",
      "copying riverraid.bin from Roms\\Roms\\ROMS\\River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\riverraid.bin\n",
      "copying road_runner.bin from patched version of Roms\\Roms\\ROMS\\Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\road_runner.bin\n",
      "copying robotank.bin from Roms\\Roms\\ROMS\\Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\robotank.bin\n",
      "copying seaquest.bin from Roms\\Roms\\ROMS\\Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\seaquest.bin\n",
      "copying sir_lancelot.bin from Roms\\Roms\\ROMS\\Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\sir_lancelot.bin\n",
      "copying skiing.bin from Roms\\Roms\\ROMS\\Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\skiing.bin\n",
      "copying solaris.bin from Roms\\Roms\\ROMS\\Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\solaris.bin\n",
      "copying space_invaders.bin from Roms\\Roms\\ROMS\\Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\space_invaders.bin\n",
      "copying star_gunner.bin from Roms\\Roms\\ROMS\\Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\star_gunner.bin\n",
      "copying surround.bin from Roms\\Roms\\ROMS\\Surround (32 in 1) (Bit Corporation) (R320).bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\surround.bin\n",
      "copying tennis.bin from Roms\\Roms\\ROMS\\Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\tennis.bin\n",
      "copying time_pilot.bin from Roms\\Roms\\ROMS\\Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\time_pilot.bin\n",
      "copying trondead.bin from Roms\\Roms\\ROMS\\TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\trondead.bin\n",
      "copying tutankham.bin from Roms\\Roms\\ROMS\\Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\tutankham.bin\n",
      "copying up_n_down.bin from Roms\\Roms\\ROMS\\Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\up_n_down.bin\n",
      "copying venture.bin from Roms\\Roms\\ROMS\\Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\venture.bin\n",
      "copying pong.bin from Roms\\Roms\\ROMS\\Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\pong.bin\n",
      "copying wizard_of_wor.bin from Roms\\Roms\\ROMS\\Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\wizard_of_wor.bin\n",
      "copying yars_revenge.bin from Roms\\Roms\\ROMS\\Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\yars_revenge.bin\n",
      "copying zaxxon.bin from Roms\\Roms\\ROMS\\Zaxxon (1983) (Coleco) (2454) ~.bin to C:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\atari_roms\\zaxxon.bin\n"
     ]
    }
   ],
   "source": [
    "import os # importeren voor het omgaan met paths\n",
    "import gym # voor het beheren van de omgeving van onze agents\n",
    "# from gym import wrappers # gebruiken om geen visualisaties te doen van de environment\n",
    "from stable_baselines3 import DQN, PPO, A2C # Specifieke algoritme voor RL\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # wrapper voor environment beheersing\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # beoordeling van agent policies\n",
    "from stable_baselines3.common.vec_env import VecFrameStack # gebruikt voor visual based learners om frames opelkaar te leggen\n",
    "from stable_baselines3.common.env_util import make_atari_env #specifieke wrapper voor atari environments\n",
    "import pandas as pd # dataframe en manipulatie\n",
    "import seaborn as sns # visualisatie\n",
    "from matplotlib import pyplot as plt # extra functionaliteit voor visualisatie\n",
    "\n",
    "\n",
    "# nodig om atari spellen te mogen/kunnen gebruiken\n",
    "!python -m atari_py.import_roms \"Roms\\Roms\\ROMS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Environments\n",
    "Voor de environments maken we gebruik van [OpenAI gym](https://gym.openai.com/envs/#classic_control). In deze environments gaat de agent leren door tijdens een aantal episodes \"random\" acties uit te voeren. De environment is de overkoepelende omgeving waarin de agent acteerd, observaties worden gedaan en waaruit beloning worden gehaald. Een OpenAI gym omgeving kan in verschillende vormen (spaces) voorkomen:\n",
    "* **Box:** n-dimensionele tensor, die een range van waarden bevat --> Box(0, 1, shape=(3,3))\n",
    "* **Discrete:** set van discrete waarden --> Discrete(3)\n",
    "* **Tuple:** Een Tuple van andere spaces --> Tuple((Discrete(3), Box(0, 1, shape=(3,3))))\n",
    "* **Dict:** Dictionary van spaces --> Dict({('height': Discrete(3), 'speed':Box(0, 1, shape=(3,3)))})\n",
    "* **MultiBinary:** One hot encoded binary waarde --> MultiBinary(4) --> [0,0,0,0] of [0,1,0,1] etc.\n",
    "* **MultiDiscrete:** Meerdere discrete waarde --> MultiDiscrete([2,5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inladen van environment\n",
    "environment_name ='CartPole-v0'\n",
    "env = gym.make(environment_name)\n",
    "# Test om te zien wat de environment bevat\n",
    "print(\n",
    "    f'''\n",
    "    env.action_space: {env.action_space}\n",
    "    env.observation_space: {env.observation_space}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CartPole-v0\n",
    "Voor de eerste test wordt de CartPole-v0 environment gebruikt. Deze environment bestaat (zoals hierboven te zien) uit:\n",
    "* Een action space Discrete(2), hierbij is 0 == links en 1 == rechts op een virtuele controller.\n",
    "* de observation space, het scherm dat de speler/agent ziet is een Box() type de eerste array geeft de lower-bound aan van 4 type observaties, de tweede array de upper-bound. een array eenmaal aangeroepen bestaat uit 4 elementen van het type float32\n",
    "\n",
    "### 2.2 De observation space\n",
    "```\n",
    "env.observation_space: \n",
    "    Box(\n",
    "        [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38],\n",
    "        [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38],\n",
    "        (4,),\n",
    "        float32)\n",
    "```\n",
    "\n",
    "dit kan als volgt geïnterpeteerd worden: <br>\n",
    "env.observation_space[0] = [-4.8000002e+00] en [4.8000002e+00] --> dit zijn de min en max posities van het wagentje<br>\n",
    "env.observation_space[1] = [-3.4028235e+38] en [3.4028235e+38] --> de velocity (snelheid) van het wagentje<br>\n",
    "env.observation_space[2] = [-4.1887903e-01] en [4.1887903e-01] --> hoek van de paal ten opzichten van het wagentje<br>\n",
    "env.observation_space[3] = [-3.4028235e+38] en [3.4028235e+38] --> velocity van de paal<br>\n",
    "\n",
    "binnen deze observation space wordt er per env.observation_space[*x*] een waarde teruggeven voor de agent om op te acteren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    " # het aantal keer dat de agent het spel speelt.\n",
    "\n",
    "for episode in range(0, episodes): # loop over de episodes.\n",
    "    state = env.reset() # stel de omgeving opnieuw in.\n",
    "    done = False # zodra het spel is afgelopen verandert dit naar done, en begint dit op false.\n",
    "    score = 0 # de score die onze agent moet verhogen.\n",
    "\n",
    "    while not done: # not done == not False == True.\n",
    "        env.render() #render het spel in een omgeving. normaal gesproken staat dit uit voor snelheid.\n",
    "        action = env.action_space.sample() # van onze action space kies 1 actie random (links of rechts).\n",
    "        n_state, reward, done, info = env.step(action) # neem een stap (frame/actie), dit returned 4 waarden. een nieuwe Box(), de beloning, False/True (het spel is afgelopen), en eventueel extra info.\n",
    "        score += reward # tel de beloning voor deze actie op bij de score van deze episode.\n",
    "    print(f'Episode: {episode+1} | Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Algorithmes\n",
    "\n",
    "Er zijn diverse algorithmes algoritmes beschikbaar. Binnen deze notebook wordt gebruik gemaakt van Model-Free RL. dit zijn algorithmes die zich minder/niet bezig houden met het voorspellen van de volgende state maar puur kijken naar een een set regels die de agent maakt op basis van ervaring. Zo kan de agent een regel opstellen dat het wagentje naar links moet als de stok een beetje links kantelt. In onderstaande afbeelding zijn de diverse opties te zien. In dit geval ligt de focus op PPO (Proximal Policy Optimization) dit is een policy algoritme die minder kans heeft om vast te komen zitten in een minima. Normaal gesproken leert policy based algoritmes van de huidige staat en kiest een nieuwe policy. Met PPO gebeurt dit ook maar wordt het verschil tussen policy *p<sub>t</sub>* en *p<sub>t-1</sub>* geminimaliseerd. <br>\n",
    "\n",
    "![algorithmes](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Metrics\n",
    "Voor elk type algoritme zijn diverse metrics beschikbaar. deze kunnen in 4 categoriën worden opgedeeld:\n",
    "* Evaluation: beschrijving van de lengte en reward voor een episode\n",
    "* Time: alles wat betreft tijd besteding\n",
    "* Loss: loss functie om het model te scoren\n",
    "* Overig: deze worden toeglicht wanneer deze aanbod komen\n",
    "\n",
    "De metrics worden in apparte mappen opgeslagen als log bestanden en zijn uit te lezen op diverse manieren. In dit notebook wordt gekeken naar de tensorboard visualisaties en .npz files die zullen worden omgezet naar een dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Evaluations'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bepaal de locatie voor het opslaan van logs en models\n",
    "log_path = os.path.join('Training', 'Evaluations')\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "training_path = os.path.join('Training', 'Training')\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name) # herhaal het maken van de environment\n",
    "env = DummyVecEnv([lambda:env]) # wrapper om de environment in een dummy vector te plaatsen\n",
    "model = PPO('MlpPolicy', env, verbose= 1, tensorboard_log= log_path) # een standaard NeuralNetwork (Multilayer Perceptron), verbose= 1 laat ons de resultaten loggen, tensorboard_log = locatie voor log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = PPO.load(r'Training\\Saved Models\\PPO_Model_Cartpole.zip')\n",
    "    print('model loaded!')\n",
    "except:\n",
    "    print('model not found... training new model')\n",
    "    model.learn(total_timesteps= 20000) # train het model voor x timesteps\n",
    "    model.save(PPO_path) # Sla het model op in de locatie: Training\\Saved Models \n",
    "    print(f'training finished, model saved to {PPO_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# het model kan geladen worden voor het geval er iets mis is gegaan\n",
    "\n",
    "try: #probeer eerst het bestaande model te verwijderen uit de variable list en laad dan een opgeslagen model\n",
    "    del model #verwijder het bestaande model\n",
    "    model = PPO.load(PPO_path, env=env) # laad het model\n",
    "except NameError: # als het model niet bestaat bij de \"del model\" functie catch the exception en laad het model\n",
    "    model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "print(f'Model is ingeladen!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evalueren van model en tensorboard\n",
    "In de onderstaande cellen gaan we het model evalueren en de de geloggde gegevens in een tensorboard plaatsen. \n",
    "Voor nu zijn de belangrijkste metrics om in de gaten te houden de *average Reward* en *average episode length*.\n",
    "Aangezien deze niet aanwezig zijn in de tensorboard worden deze appart gevisualiseerd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalueer het model op basis van een aantal parameters (average punten, standard deviation) --> max punten = 200.\n",
    "evaluate_policy(model, env, n_eval_episodes= 5, render=True) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om een agent in een specifieke environment te draaien\n",
    "def run_agent(env, model = False, episodes= 5, render = False):\n",
    "    '''\n",
    "    runs an agent (model) through the environment for n episodes, if render is set to False no window is rendered.\n",
    "    at the end of the runs a tensorlog file is written to a predetermined directory\n",
    "    '''\n",
    "    for episode in range(0, episodes): \n",
    "        obs = env.reset() \n",
    "        done = False \n",
    "        score = 0 \n",
    "\n",
    "        while not done:\n",
    "            if render: \n",
    "                env.render(mode='human') \n",
    "            if model:\n",
    "                action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draai nu de zelfde environment met een getrainde agent\n",
    "run_agent(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorlog ophalen van een locatie\n",
    "train_log_path = os.path.join(log_path, 'PPO_3')\n",
    "\n",
    "#klik de play button hieronder\n",
    "%tensorboard --logdir= train_log_path # klik de play button hierboven voor de Tensorboard evaluations grafieken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evalueren op basis van score\n",
    "\n",
    "In de onderstaande cellen wordt de agent gescoord op zijn vaardigheden. Gezien de omgeving redelijk simpel is vindt de agent over het algemeen met gemak een optimale oplossing na verloop van het trainings proces.\n",
    "\n",
    "De eerst volgende 2 cellen beoordelen een volledig getrained model. Het is duidelijk dat hier weinig interessants uitkomt. Het model scoort namelijk in 4 van de 5 episodes maximaal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad het model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "episodes = 5 \n",
    "lst_score = []\n",
    "lst_episode = []\n",
    "\n",
    "#Draai het model\n",
    "for episode in range(0, episodes): \n",
    "    obs = env.reset() \n",
    "    done = False \n",
    "    score = 0 \n",
    "\n",
    "    while not done: \n",
    "        # env.render() \n",
    "        action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    lst_score.append(score[0])\n",
    "    lst_episode.append(episode + 1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst_episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14676/891335536.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m \u001b[1;31m# extra functionaliteit voor visualisatie\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"episode\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlst_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"score\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlst_score\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lst_episode' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd # dataframe en manipulatie\n",
    "import seaborn as sns # visualisatie\n",
    "from matplotlib import pyplot as plt # extra functionaliteit voor visualisatie\n",
    "\n",
    "d = {\"episode\": lst_episode, \"score\": lst_score}\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "\n",
    "sns.lineplot(x= df[\"episode\"], y=df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evalueren van meerdere agents\n",
    "\n",
    "Hieronder worden 2 nieuwe agents getrained. Wordt de evaluatie van de agent tijdens het trainen gelogged naar een .npz file. In deze log komen alleen de scores en de duratie van de episodes terug.\n",
    "\n",
    "Door twee verschillende modellen te trainen kunnen we het effect van PPO policy duidelijk terug zien. Het is namelijk zo dat de PPO minimale aanpassingen doet in de voorgaande strategie (timestep). Deze afwijkingen zorgen doorgaans voor een langzaam stijgende lijn en voorkomt in de meeste gevallen een foute strategie door een te grote policy wijziging. In de 3e cell hieronder is dit ook te zien. PPO zorgt voor een doorgaans stijgende lijn en afwijkingen in de policy worden minimaal gemaakt hierdoor zie je dat een agent niet snel in een local optimum komt of naar een minima toe beweegt. Hier moet wel gemeld worden dat dit een zeer simpele omgeving betreft waardoor het niet vreemd is dat agents in deze omgeving een globaal optimum bereiken.\n",
    "\n",
    "![model1](visualisatie\\model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nieuw model voor loggen trainings informatie\n",
    "# env = wrappers.Monitor(env, video_callable=False ,force=True)\n",
    "model2 = PPO('MlpPolicy', env)\n",
    "model2.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timesteps</th>\n",
       "      <th>results</th>\n",
       "      <th>ep_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>20100.0</td>\n",
       "      <td>197.6</td>\n",
       "      <td>197.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>20200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>20300.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>20400.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     timesteps  results  ep_lengths\n",
       "0        100.0      9.8         9.8\n",
       "1        200.0      9.6         9.6\n",
       "2        300.0     10.0        10.0\n",
       "3        400.0      9.4         9.4\n",
       "4        500.0     10.4        10.4\n",
       "..         ...      ...         ...\n",
       "199    20000.0    200.0       200.0\n",
       "200    20100.0    197.6       197.6\n",
       "201    20200.0    200.0       200.0\n",
       "202    20300.0    200.0       200.0\n",
       "203    20400.0    200.0       200.0\n",
       "\n",
       "[204 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # gebruiken voor wiskundige operaties en npz files te lezen\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# functie voor het laden en transformeren van npz files\n",
    "def npz_to_dataframe(npz):\n",
    "    '''\n",
    "    Takes an npz file object loaded using numpy.load(path to .npz file) and returns a formatted dataframe\n",
    "    '''\n",
    "    # maak een data frame van de npz file op basis van de kolommen\n",
    "    df = pd.DataFrame.from_dict({col: npz[col] for col in npz.files}, orient='index')\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # transpose de dataframe gezien deze verkeerd om staat\n",
    "    df =df.T\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # bereken de gemiddeldes voor iedere cel\n",
    "    df = df.applymap(np.mean)\n",
    "    \n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_training_score = npz_to_dataframe(npz)\n",
    "df_training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een lineplot om de prestaties van het model na diverse timesteps te zien\n",
    "fig = sns.lineplot(x= df_training_score[\"timesteps\"], y=df_training_score[\"results\"])\n",
    "display(fig)\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vergelijk de training met een tweede model\n",
    "model3 = PPO('MlpPolicy', env)\n",
    "model3.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)\n",
    "\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# Roep de eerder gemaakte functie aan\n",
    "df_training_score2 = npz_to_dataframe(npz)\n",
    "\n",
    "#voeg aan beide data frames een kolom toe met de agent naam\n",
    "df_training_score[\"model\"] = \"model 1\"\n",
    "df_training_score2[\"model\"] = \"model 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder worden de twee agents tijdens het trainen met elkaar vergeleken. Het is goed te zien dat de twee agents andere start punten hebben. Door kleine aanpassingen in de strategie kan de score in het begin sterk stijgen/dalen om vervolgens na een redelijk stijgende lijn in een bepaald optimum te settlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_training_score.append(df_training_score2)\n",
    "fig = sns.lineplot(x= df_compare[\"timesteps\"], y=df_compare[\"results\"], hue= df_compare['model'])\n",
    "display(fig)\n",
    "\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1vsmodel2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Callback en earlystopping\n",
    "Vooral bij grote modellen kan tijdens het trainen het model onstabiel worden. Om dit te verhelpen kunnen we het trainen stoppen wanneer dit een bepaalde grenswaarden bereikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "best_model_path = \".\\Training\\Saved Models\"\n",
    "\n",
    "# StopTrainingOnRewardThreshold --> voorwaarden wanneer het trainen moet stoppen\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold= 200, verbose= 1)\n",
    "\n",
    "# EvalCallback --> evalueert het trainingproces en controlleert de treshhold\n",
    "eval_callback = EvalCallback(env, callback_on_new_best= stop_callback, eval_freq= 1000, best_model_save_path= best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Andere policies en algoritmes\n",
    "\n",
    "In de onderstaande cellen gaan we verder met het opstellen van twee alternatieve modellen. \n",
    "\n",
    "De eerste stap gaat zijn om een PPO model op te stellen met een zelf samengesteld Neural Network. Hoewel Stable Baselines goed afgestelde modellen heeft is het goed om te weten dat deze nog verder verfijnd kunnen worden. In dit geval maken we gebruik van een neural network voor de agent/decision functie (*pi*) en de value function (*vf*). beider netwerken zullen bestaan uit 4 lagen met iedere laag 128 neuronen.\n",
    "\n",
    "In de tweede cel laden we kort een ander algorithme, DQN (Deep Q Network). Dit model is voor de volgende spaces te gebruiken:\n",
    "\n",
    "|Spaces | Action | Observation |\n",
    "|---|---|---|\n",
    "|  Discrete | ✔️ | ✔️ |\n",
    "|  Box | ❌ | ✔️ |\n",
    "|  MultiDiscrete | ❌ | ✔️ |\n",
    "|  MultiBinary | ❌ | ✔️ |\n",
    "\n",
    "Dit model lijkt zich dus goed te lenen voor het Carpole probleem. Hier hebben we namelijk te maken met een **discrete action space** en een **box observation space**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Een neuralnetwork architectuur voor agent (pi) en valuefunction (vf)\n",
    "NN_arch = [dict(pi= [128,128,128,128], vf= [128,128,128,128] )]\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch': NN_arch})\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(20000, callback= eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Project 1: Breakout\n",
    "\n",
    "In deze sectie gaan we proberen het spel Breakout te halen. Breakout is een simpel spel waarbij we met een peddle onder aan het scherm proberen een balletje te raken om hiermee blokken boven in het scherm kappot te maken. Om dit tot een succes te laten verlopen, doorlopen we ongeveer dezelfde stappen als in de voorgaande secties:\n",
    "1. Omgeving inladen\n",
    "2. Omgeving verkennen\n",
    "3. Geschikte algoritmes zoeken\n",
    "4. Agent trainen\n",
    "5. Beoordelen agent\n",
    "6. Hyperparamter tuning????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0.1 Functies klaarzetten\n",
    "Om met deze omgeving aan de slag te gaan maken we eerst een aantal functies aan die gebruikt kunnen worden. De functies worden zoveel mogelijk opgeknipt zodat het duidelijk is wat een functie doet zonder enige vorm van dubbelzinnigheid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_info(env):\n",
    "    \"\"\"returns information about the environments type, observation an daction space and action meanings.\n",
    "\n",
    "    arguments:\n",
    "    env -- environment created using gym.make()\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "    f'''\n",
    "    observation space type: {type(env.observation_space)}\n",
    "    observation space shape: {env.observation_space.shape}\n",
    "    action space: {env.action_space}\n",
    "    aanwezige actions: {env.unwrapped.get_action_meanings()}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 de omgeving\n",
    "\n",
    "We hebben dus te maken met een box observation space en een discrete action space. De observation space bestaat uit drie dimensies (Rood, Groen, Blauw waarde) en heeft een afmeting van 210 height x 160 width. Daarnaast bestaat de action space uit 4 discrete acties 'NOOP', 'FIRE', 'RIGHT', en 'LEFT'. Hierbij staat 'NOOP' voor het nemen van geen actie, 'FIRE' wordt gebruikt om het spel op te starten.\n",
    "\n",
    "Voor dit probleem is er dus een algoritme nodig dat gebruik kan maken van een een discrete action space. In de [documentatie van Stable Baselines](https://stable-baselines.readthedocs.io/en/master/guide/algos.html) is te zien dat er een aantal algoritmes in aanmerking komen, namelijk: A2C, ACER, ACKTR, DQN, HER, GAIL en PPO. Deze algoritmes kunnen allemaal omgaan met een discrete action space.\n",
    "\n",
    "We gaan om te beginnen gebruik maken van A2C, DQN, en PPO. Dit verkleind de scope en geeft een beeld van het effect dat diverse modellen zullen hebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPISODES = 5\n",
    "MODEL_PATH = 'Breakout_model\\Saved_Model'\n",
    "LOG_PATH = 'Breakout_model\\Logging'\n",
    "TRAIN_LOG_PATH = 'Breakout_model\\Training_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    True\n",
      "    0\n",
      "    <torch.cuda.device object at 0x000001BD0FDD15B0>\n",
      "    1\n",
      "    NVIDIA GeForce MX250\n",
      "    Allocated: 0.0 'GB')\n",
      "    Cached: 0.0 'GB')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# controleren van eventuele GPU's\n",
    "import torch\n",
    "print(\n",
    "    f'''\n",
    "    {torch.cuda.is_available()}\n",
    "    {torch.cuda.current_device()}\n",
    "    {torch.cuda.device(0)}\n",
    "    {torch.cuda.device_count()}\n",
    "    {torch.cuda.get_device_name(0)}\n",
    "    Allocated: {round(torch.cuda.memory_allocated(0)/1024**3,1)} 'GB')\n",
    "    Cached: {round(torch.cuda.memory_reserved(0)/1024**3,1)} 'GB')\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    observation space type: <class 'gym.spaces.box.Box'>\n",
      "    observation space shape: (210, 160, 3)\n",
      "    action space: Discrete(4)\n",
      "    aanwezige actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# aanmaken environment en testen van een sample agent\n",
    "environment_name = 'Breakout-v0'\n",
    "env = gym.make(environment_name)\n",
    "env_info(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 | Score: 1.0\n",
      "Episode: 2 | Score: 1.0\n",
      "Episode: 3 | Score: 1.0\n",
      "Episode: 4 | Score: 1.0\n",
      "Episode: 5 | Score: 2.0\n"
     ]
    }
   ],
   "source": [
    "run_agent(env, render= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 De Agents\n",
    "We proberen dus een agent te trainen op het spel Breakout. Ook hebben we gezien dat de observation space een afbeelding is (rgb van 210 bij 160). Dit betekent dat het Mlp-model niet gaat werken, of in iedergeval veel minder effectief zal zijn. In dit geval proberen we een Convolutional Neural Network (CNN) te trainen. Dergelijke netwerken zijn beter in het behandelen van afbeeldingen.\n",
    "\n",
    "### 6.2.1 A2C\n",
    "[Advantage Actor Critic](https://openai.com/blog/baselines-acktr-a2c/), A2C, is een variatie op het A3C (Asynchronous Advantage Actor Critic) algoritme. A2C heeft de mogelijkheid om tegelijk meerdere workers in te zetten op de environment.\n",
    "\n",
    "A3C combineert een aantal belangrijke elementen:\n",
    "* een update aan het algorithme gebeurt na een vast aantal timesteps. Over dit segment worden estimators berkend op de return en advantage functies.\n",
    "* Lagen van de netwerk architectuur worden gedeeld tussen de policy function (pi) en de value function(vf)\n",
    "* Asynchronysche updates\n",
    "\n",
    "Waar A3C de werkers los van elkaar update doet wacht A2C met een update van de werkers totdat deze allemaal klaar zijn met een segment. Vervolgens wordt er een average van alle workers gepakt voor de update. Binnen Stable Baselines en OpenAI lijkt de performance van A2C een stuk beter dan A3C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "# probeer het model te laden \n",
    "try:\n",
    "    A2C_5K_model = A2C.load(r'Breakout_model\\Saved_Model\\a2c_Breakout_5k')\n",
    "    print('model loaded!')\n",
    "except (FileNotFoundError, AttributeError) as e:\n",
    "    # Parallel environments voor snellere training\n",
    "    train_env = make_atari_env('Breakout-v0', n_envs = 4, seed= 13)\n",
    "    train_env = VecFrameStack(train_env, n_stack=4)\n",
    "    A2C_5K_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_5k')\n",
    "    A2C_5K_model.learn(total_timesteps=5000, eval_log_path= TRAIN_LOG_PATH, eval_env=env)\n",
    "    A2C_5K_model.save(MODEL_PATH + \"\\\\a2c_Breakout_5k\")\n",
    "    print('model saved!')\n",
    "\n",
    "\n",
    "# maak een enkele environment voor visualisatie\n",
    "\n",
    "# eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "# eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "# evaluate_policy(A2C_5K_model, eval_env, n_eval_episodes= 5 )\n",
    "\n",
    "# env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "# env = VecFrameStack(env, n_stack=4)\n",
    "# A2C_25K_model = A2C('CnnPolicy', env, verbose=1)\n",
    "\n",
    "# evaluate_policy(A2C_25K_model, env, n_eval_episodes= 5, render= True )\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "# evaluate_policy(A2C_5K_model, eval_env, n_eval_episodes= 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to Breakout_model\\Logging\\a2c_Breakout_tuned_5M\\A2C_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:337: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001BD10041520> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x000001BD68625EE0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=2.40 +/- 1.36\n",
      "Episode length: 330.80 +/- 73.12\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 331       |\n",
      "|    mean_reward        | 2.4       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -4.77e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 0.00318   |\n",
      "|    value_loss         | 0.000182  |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 291      |\n",
      "|    ep_rew_mean     | 1.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=2.00 +/- 4.00\n",
      "Episode length: 308.40 +/- 195.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 308      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0444   |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000589 |\n",
      "|    value_loss         | 0.0454   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 288      |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 61       |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 231.40 +/- 18.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 231      |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0589   |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.0212  |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 275      |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 63       |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=1.60 +/- 1.36\n",
      "Episode length: 285.40 +/- 77.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 285      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0509   |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0296   |\n",
      "|    value_loss         | 0.0455   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 265      |\n",
      "|    ep_rew_mean     | 1.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 63       |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 271.20 +/- 62.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 271      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.225    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000132 |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 283      |\n",
      "|    ep_rew_mean     | 1.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 63       |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 313      |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=1.60 +/- 0.49\n",
      "Episode length: 271.20 +/- 22.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 271      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.114    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.0228   |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 283      |\n",
      "|    ep_rew_mean     | 1.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 373      |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=1.20 +/- 0.75\n",
      "Episode length: 255.00 +/- 41.66\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 255       |\n",
      "|    mean_reward        | 1.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -2.89e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.074    |\n",
      "|    value_loss         | 0.00419   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 281      |\n",
      "|    ep_rew_mean     | 1.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 700      |\n",
      "|    time_elapsed    | 436      |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=1.20 +/- 0.40\n",
      "Episode length: 243.80 +/- 16.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 244      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.374    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.0299   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 294      |\n",
      "|    ep_rew_mean     | 1.8      |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 495      |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=1.20 +/- 0.75\n",
      "Episode length: 258.40 +/- 29.59\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 258       |\n",
      "|    mean_reward        | 1.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -1.59e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.0445   |\n",
      "|    value_loss         | 0.00218   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 284      |\n",
      "|    ep_rew_mean     | 1.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 900      |\n",
      "|    time_elapsed    | 555      |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1.60 +/- 0.80\n",
      "Episode length: 268.00 +/- 39.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 268      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.185    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0102   |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | 1.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 615      |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=1.40 +/- 0.80\n",
      "Episode length: 263.40 +/- 28.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 263      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.565    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.0115   |\n",
      "|    value_loss         | 0.0207   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 273      |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1100     |\n",
      "|    time_elapsed    | 676      |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=1.60 +/- 1.02\n",
      "Episode length: 262.20 +/- 40.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 262      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.293    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.0462   |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 267      |\n",
      "|    ep_rew_mean     | 1.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1200     |\n",
      "|    time_elapsed    | 738      |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 257.20 +/- 50.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 257      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.651    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.0275   |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 283      |\n",
      "|    ep_rew_mean     | 1.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1300     |\n",
      "|    time_elapsed    | 797      |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=0.80 +/- 0.75\n",
      "Episode length: 231.80 +/- 26.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 232      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -1.7e+08 |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.0247   |\n",
      "|    value_loss         | 0.00142  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 283      |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1400     |\n",
      "|    time_elapsed    | 856      |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=3.20 +/- 1.47\n",
      "Episode length: 351.40 +/- 84.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 351      |\n",
      "|    mean_reward        | 3.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.622    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | 1.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1500     |\n",
      "|    time_elapsed    | 918      |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=2.20 +/- 1.17\n",
      "Episode length: 305.80 +/- 74.72\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 306       |\n",
      "|    mean_reward        | 2.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 64000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.28e+09 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.0393   |\n",
      "|    value_loss         | 0.00268   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 283      |\n",
      "|    ep_rew_mean     | 1.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1600     |\n",
      "|    time_elapsed    | 979      |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 291.40 +/- 29.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 291      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.441    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.0186   |\n",
      "|    value_loss         | 0.0267   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 278      |\n",
      "|    ep_rew_mean     | 1.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1700     |\n",
      "|    time_elapsed    | 1040     |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=1.20 +/- 0.75\n",
      "Episode length: 259.80 +/- 38.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 260       |\n",
      "|    mean_reward        | 1.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.63e+09 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.0196    |\n",
      "|    value_loss         | 0.0111    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 282      |\n",
      "|    ep_rew_mean     | 1.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1800     |\n",
      "|    time_elapsed    | 1100     |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=1.20 +/- 0.98\n",
      "Episode length: 240.80 +/- 46.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 241      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.214    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.0768   |\n",
      "|    value_loss         | 0.0888   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 274      |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 1900     |\n",
      "|    time_elapsed    | 1161     |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 280.40 +/- 6.89\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 280       |\n",
      "|    mean_reward        | 2         |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.38e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.0352   |\n",
      "|    value_loss         | 0.00237   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 267      |\n",
      "|    ep_rew_mean     | 1.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 1223     |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 286.60 +/- 35.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.224    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 272      |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2100     |\n",
      "|    time_elapsed    | 1284     |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=2.60 +/- 0.49\n",
      "Episode length: 315.00 +/- 37.37\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 315       |\n",
      "|    mean_reward        | 2.6       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -5.91e+07 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | -0.0152   |\n",
      "|    value_loss         | 0.00162   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 285      |\n",
      "|    ep_rew_mean     | 1.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2200     |\n",
      "|    time_elapsed    | 1345     |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 308.60 +/- 43.53\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 309       |\n",
      "|    mean_reward        | 2.4       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.62e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -0.0525   |\n",
      "|    value_loss         | 0.00214   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 272      |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2300     |\n",
      "|    time_elapsed    | 1406     |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 311.40 +/- 34.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 311      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.337    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.0604   |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 272      |\n",
      "|    ep_rew_mean     | 1.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2400     |\n",
      "|    time_elapsed    | 1468     |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 293.80 +/- 30.76\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 294       |\n",
      "|    mean_reward        | 2.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.61e+09 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -0.0772   |\n",
      "|    value_loss         | 0.00694   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 269      |\n",
      "|    ep_rew_mean     | 1.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2500     |\n",
      "|    time_elapsed    | 1529     |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=2.00 +/- 1.26\n",
      "Episode length: 287.40 +/- 82.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.485    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 278      |\n",
      "|    ep_rew_mean     | 1.54     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2600     |\n",
      "|    time_elapsed    | 1590     |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 287.00 +/- 33.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.329   |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 0.00487  |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 275      |\n",
      "|    ep_rew_mean     | 1.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2700     |\n",
      "|    time_elapsed    | 1651     |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=1.80 +/- 0.98\n",
      "Episode length: 272.80 +/- 46.03\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 273       |\n",
      "|    mean_reward        | 1.8       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 112000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.09e+09 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 0.0156    |\n",
      "|    value_loss         | 0.000407  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | 1.97     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2800     |\n",
      "|    time_elapsed    | 1710     |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 287.20 +/- 28.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.0109   |\n",
      "|    value_loss         | 0.00307  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | 1.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 2900     |\n",
      "|    time_elapsed    | 1770     |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 279.20 +/- 8.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 279      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.195    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.00885  |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 286      |\n",
      "|    ep_rew_mean     | 1.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3000     |\n",
      "|    time_elapsed    | 1830     |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 293.60 +/- 25.77\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 294       |\n",
      "|    mean_reward        | 2.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 124000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -2.62e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 0.0281    |\n",
      "|    value_loss         | 0.000984  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 288      |\n",
      "|    ep_rew_mean     | 1.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3100     |\n",
      "|    time_elapsed    | 1890     |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 304.20 +/- 26.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 304      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.00202  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 285      |\n",
      "|    ep_rew_mean     | 1.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3200     |\n",
      "|    time_elapsed    | 1951     |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=1.40 +/- 0.49\n",
      "Episode length: 239.80 +/- 20.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 240      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.647    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.0115   |\n",
      "|    value_loss         | 0.00866  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 286      |\n",
      "|    ep_rew_mean     | 1.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3300     |\n",
      "|    time_elapsed    | 2011     |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 278.80 +/- 9.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 279      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.345    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.00869 |\n",
      "|    value_loss         | 0.0455   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 272      |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3400     |\n",
      "|    time_elapsed    | 2072     |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 220.00 +/- 17.98\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 220       |\n",
      "|    mean_reward        | 0.6       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 140000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -3.08e+06 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 0.0271    |\n",
      "|    value_loss         | 0.000655  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | 1.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3500     |\n",
      "|    time_elapsed    | 2130     |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=2.00 +/- 0.63\n",
      "Episode length: 254.60 +/- 33.96\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 255       |\n",
      "|    mean_reward        | 2         |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 144000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -1.63e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 0.0421    |\n",
      "|    value_loss         | 0.00163   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 276      |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3600     |\n",
      "|    time_elapsed    | 2192     |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=1.20 +/- 0.40\n",
      "Episode length: 250.60 +/- 31.85\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 251       |\n",
      "|    mean_reward        | 1.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 148000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -3.05e+07 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 0.0208    |\n",
      "|    value_loss         | 0.000731  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | 1.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3700     |\n",
      "|    time_elapsed    | 2252     |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 307.00 +/- 26.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 307      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.706    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.00499  |\n",
      "|    value_loss         | 0.00718  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 282      |\n",
      "|    ep_rew_mean     | 1.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3800     |\n",
      "|    time_elapsed    | 2312     |\n",
      "|    total_timesteps | 152000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 276.80 +/- 9.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 277      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 156000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.0169   |\n",
      "|    value_loss         | 0.00994  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 296      |\n",
      "|    ep_rew_mean     | 1.82     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 3900     |\n",
      "|    time_elapsed    | 2371     |\n",
      "|    total_timesteps | 156000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 282.20 +/- 25.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 282      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.0571   |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 284      |\n",
      "|    ep_rew_mean     | 1.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 2432     |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 289.20 +/- 34.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 289      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 164000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | -0.0952  |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.0455  |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 288      |\n",
      "|    ep_rew_mean     | 1.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4100     |\n",
      "|    time_elapsed    | 2492     |\n",
      "|    total_timesteps | 164000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=1.60 +/- 0.49\n",
      "Episode length: 262.00 +/- 26.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 262      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.00773 |\n",
      "|    value_loss         | 0.00775  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 286      |\n",
      "|    ep_rew_mean     | 1.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4200     |\n",
      "|    time_elapsed    | 2553     |\n",
      "|    total_timesteps | 168000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 293.20 +/- 26.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 293      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 172000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 0.0258   |\n",
      "|    value_loss         | 0.0129   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 286      |\n",
      "|    ep_rew_mean     | 1.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4300     |\n",
      "|    time_elapsed    | 2614     |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 292.00 +/- 33.52\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 292       |\n",
      "|    mean_reward        | 2.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 176000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -1.12e+07 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -0.00391  |\n",
      "|    value_loss         | 0.00102   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 281      |\n",
      "|    ep_rew_mean     | 1.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4400     |\n",
      "|    time_elapsed    | 2675     |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 283.40 +/- 8.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 283      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.685    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.0195   |\n",
      "|    value_loss         | 0.00789  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | 1.97     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4500     |\n",
      "|    time_elapsed    | 2734     |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=2.40 +/- 0.80\n",
      "Episode length: 301.60 +/- 49.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 302      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.34     |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.0266  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | 1.54     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4600     |\n",
      "|    time_elapsed    | 2795     |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 307.60 +/- 36.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 308      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 188000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.575    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.0341   |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 280      |\n",
      "|    ep_rew_mean     | 1.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4700     |\n",
      "|    time_elapsed    | 2856     |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 309.40 +/- 33.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 309      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -0.0395  |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 282      |\n",
      "|    ep_rew_mean     | 1.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4800     |\n",
      "|    time_elapsed    | 2916     |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 302.40 +/- 33.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 302      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 196000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.589    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | 1.99     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 4900     |\n",
      "|    time_elapsed    | 2976     |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 278.60 +/- 5.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 279      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.125    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.0212  |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 281      |\n",
      "|    ep_rew_mean     | 1.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5000     |\n",
      "|    time_elapsed    | 3037     |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=1.60 +/- 0.80\n",
      "Episode length: 265.20 +/- 34.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 265      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 204000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.609    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 0.00927  |\n",
      "|    value_loss         | 0.00957  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | 1.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5100     |\n",
      "|    time_elapsed    | 3098     |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=2.40 +/- 1.36\n",
      "Episode length: 334.60 +/- 77.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 335      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.0144   |\n",
      "|    value_loss         | 0.0048   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 286      |\n",
      "|    ep_rew_mean     | 1.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5200     |\n",
      "|    time_elapsed    | 3160     |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=1.60 +/- 0.80\n",
      "Episode length: 290.00 +/- 41.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 290      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 212000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.0282   |\n",
      "|    value_loss         | 0.00774  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 272      |\n",
      "|    ep_rew_mean     | 1.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5300     |\n",
      "|    time_elapsed    | 3221     |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 275.60 +/- 57.10\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 276       |\n",
      "|    mean_reward        | 1.4       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 216000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -2.72e+09 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | 0.016     |\n",
      "|    value_loss         | 0.00197   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 291      |\n",
      "|    ep_rew_mean     | 1.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5400     |\n",
      "|    time_elapsed    | 3280     |\n",
      "|    total_timesteps | 216000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 322.00 +/- 34.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 322      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.0195   |\n",
      "|    value_loss         | 0.00605  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 283      |\n",
      "|    ep_rew_mean     | 1.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5500     |\n",
      "|    time_elapsed    | 3341     |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=1.60 +/- 1.50\n",
      "Episode length: 288.80 +/- 83.58\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 289       |\n",
      "|    mean_reward        | 1.6       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 224000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -8.44e+06 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -0.00833  |\n",
      "|    value_loss         | 0.000191  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | 1.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5600     |\n",
      "|    time_elapsed    | 3402     |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=2.00 +/- 0.00\n",
      "Episode length: 297.60 +/- 11.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 298      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 228000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -0.00638 |\n",
      "|    value_loss         | 0.00911  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | 1.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5700     |\n",
      "|    time_elapsed    | 3462     |\n",
      "|    total_timesteps | 228000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=2.20 +/- 1.17\n",
      "Episode length: 316.80 +/- 61.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 317      |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 232000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 0.0352   |\n",
      "|    value_loss         | 0.00105  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 273      |\n",
      "|    ep_rew_mean     | 1.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5800     |\n",
      "|    time_elapsed    | 3524     |\n",
      "|    total_timesteps | 232000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=1.60 +/- 1.50\n",
      "Episode length: 292.40 +/- 87.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 292      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 236000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | 0.058    |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 289      |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 5900     |\n",
      "|    time_elapsed    | 3584     |\n",
      "|    total_timesteps | 236000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1.60 +/- 0.80\n",
      "Episode length: 286.00 +/- 40.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 286      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 0.0168   |\n",
      "|    value_loss         | 0.00478  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 296      |\n",
      "|    ep_rew_mean     | 1.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 65       |\n",
      "|    iterations      | 6000     |\n",
      "|    time_elapsed    | 3644     |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=2.40 +/- 2.06\n",
      "Episode length: 351.20 +/- 117.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 351      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 244000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 0.0385   |\n",
      "|    value_loss         | 0.00908  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 23       |\n",
      "|    iterations      | 6100     |\n",
      "|    time_elapsed    | 10490    |\n",
      "|    total_timesteps | 244000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=1.20 +/- 0.98\n",
      "Episode length: 257.40 +/- 54.72\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 257       |\n",
      "|    mean_reward        | 1.2       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 248000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -7.59e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 0.0224    |\n",
      "|    value_loss         | 0.000686  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | 1.54     |\n",
      "| time/              |          |\n",
      "|    fps             | 23       |\n",
      "|    iterations      | 6200     |\n",
      "|    time_elapsed    | 10561    |\n",
      "|    total_timesteps | 248000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=1.80 +/- 1.60\n",
      "Episode length: 302.40 +/- 89.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 302      |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 252000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.00599  |\n",
      "|    value_loss         | 0.00719  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 289      |\n",
      "|    ep_rew_mean     | 1.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 23       |\n",
      "|    iterations      | 6300     |\n",
      "|    time_elapsed    | 10645    |\n",
      "|    total_timesteps | 252000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=1.20 +/- 0.98\n",
      "Episode length: 255.80 +/- 52.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 256      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 256000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -0.00256 |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 298      |\n",
      "|    ep_rew_mean     | 1.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 23       |\n",
      "|    iterations      | 6400     |\n",
      "|    time_elapsed    | 10735    |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=2.40 +/- 0.80\n",
      "Episode length: 322.60 +/- 51.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 323      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 260000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.0105   |\n",
      "|    value_loss         | 0.00108  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 291      |\n",
      "|    ep_rew_mean     | 1.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 6500     |\n",
      "|    time_elapsed    | 10822    |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=3.00 +/- 1.10\n",
      "Episode length: 368.40 +/- 77.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 368      |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 264000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.684    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 0.0105   |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 281      |\n",
      "|    ep_rew_mean     | 1.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 6600     |\n",
      "|    time_elapsed    | 10908    |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=1.20 +/- 0.98\n",
      "Episode length: 269.40 +/- 52.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 269      |\n",
      "|    mean_reward        | 1.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 268000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.779    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.073    |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 278      |\n",
      "|    ep_rew_mean     | 1.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 6700     |\n",
      "|    time_elapsed    | 10985    |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=2.00 +/- 1.90\n",
      "Episode length: 309.00 +/- 113.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 309      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 272000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.179    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 0.0254   |\n",
      "|    value_loss         | 0.0393   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 6800     |\n",
      "|    time_elapsed    | 11060    |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=2.00 +/- 1.10\n",
      "Episode length: 299.60 +/- 67.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 300      |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 276000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.658    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.00863  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 299      |\n",
      "|    ep_rew_mean     | 1.89     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 6900     |\n",
      "|    time_elapsed    | 11151    |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=2.00 +/- 1.10\n",
      "Episode length: 307.60 +/- 58.78\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 308       |\n",
      "|    mean_reward        | 2         |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 280000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -6.78e+08 |\n",
      "|    learning_rate      | 5e-05     |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 0.0112    |\n",
      "|    value_loss         | 0.000506  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 294      |\n",
      "|    ep_rew_mean     | 1.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 7000     |\n",
      "|    time_elapsed    | 11246    |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 273.80 +/- 67.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 274      |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 284000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.708    |\n",
      "|    learning_rate      | 5e-05    |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | -0.00266 |\n",
      "|    value_loss         | 0.00713  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 290      |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 7100     |\n",
      "|    time_elapsed    | 11338    |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14676/407462337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtuned_A2C_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA2C\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CnnPolicy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mLOG_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\a2c_Breakout_tuned_5M'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtuned_A2C_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_log_path\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mTRAIN_LOG_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mtuned_A2C_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\a2c_Breakout_tuned_5M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    190\u001b[0m     ) -> \"A2C\":\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         return super(A2C, self).learn(\n\u001b[0m\u001b[0;32m    193\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[0mnew_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Transpose the terminal observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m     ) -> Tuple[Union[np.ndarray, Dict[str, np.ndarray]], np.ndarray, np.ndarray, List[Dict[str, Any]],]:\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstackedobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[0;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             )\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mGymStepReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\stable_baselines3\\common\\monitor.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"ale.lives\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\MOI Applied Data Science\\PfDS offline\\eindproject\\.venv\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[1;34m(self, screen_data)\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### ALLEEN GEBRUIKEN ALS JE +1 DAG WILT WACHTEN OP DE TRAINING CYCLUS\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "env = make_atari_env('Breakout-v0', n_envs = 8, seed= 13)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "eval_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "# maak een callback aan elke x timesteps en save dit in \n",
    "checkpoint_callback = CheckpointCallback(save_freq=100000, save_path=r'Breakout_model\\Saved_Model', name_prefix='rl_model')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=r'Breakout_model\\best_model',log_path=r'Breakout_model\\Logging\\a2c_Breakout_tuned', eval_freq=500)\n",
    "\n",
    "\n",
    "tuned_A2C_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_tuned_5M', gamma= 0.0001, learning_rate=0.00005)\n",
    "tuned_A2C_model.learn(5000000, eval_log_path= TRAIN_LOG_PATH, eval_env=eval_env, callback=[checkpoint_callback, eval_callback])\n",
    "tuned_A2C_model.save(MODEL_PATH + \"\\\\a2c_Breakout_tuned_5M\")\n",
    "\n",
    "\n",
    "# evaluate_policy(tuned_A2C_model, eval_env, n_eval_episodes= 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALLEEN GEBRUIKEN ALS JE 5 UUR WILT WACHTEN OP DE TRAINING CYCLUS\n",
    "\n",
    "# try:\n",
    "#     A2C_1M_model = A2C.load(MODEL_PATH + '\\\\a2c_Breakout_1M.zip')\n",
    "\n",
    "# except (FileNotFoundError, AttributeError) as e:\n",
    "#     env = make_atari_env('Breakout-v0', n_envs = 4, seed= 13)\n",
    "#     env = VecFrameStack(env, n_stack=4)\n",
    "#     model = A2C('CnnPolicy', env, verbose=1, tensorboard_log= LOG_PATH + '\\\\a2c_Breakout_1M')\n",
    "#     model.learn(total_timesteps=1000000)\n",
    "#     model.save(MODEL_PATH + '\\\\a2c_Breakout_1M.zip')\n",
    "\n",
    "# env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "# env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# run_agent(env, A2C_1M_model, render= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 DQN\n",
    "\n",
    "Een DQN, Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to Breakout_model\\Logging\\DQN_Breakout_5k\\DQN_2\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 1.75     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 369      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 1000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 231      |\n",
      "|    ep_rew_mean      | 1.12     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 398      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1849     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 226      |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 433      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 2706     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 247      |\n",
      "|    ep_rew_mean      | 1.44     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 438      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 3950     |\n",
      "----------------------------------\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = DQN.load(r'Breakout_model\\Saved_Model\\DQN_Breakout_5k')\n",
    "    print('model loaded!')\n",
    "except (FileNotFoundError, AttributeError) as e:\n",
    "    train_env = make_atari_env('Breakout-v0', n_envs = 1, seed= 13)\n",
    "    train_env = VecFrameStack(train_env, n_stack=1)\n",
    "    model = DQN('CnnPolicy', env, verbose= 1, tensorboard_log= LOG_PATH + '\\\\DQN_Breakout_5k', device= 'cuda',  buffer_size= 200)\n",
    "    model.learn(total_timesteps=5000, eval_log_path= TRAIN_LOG_PATH, eval_env=env)\n",
    "    model.save(MODEL_PATH + \"\\\\DQN_Breakout_5k\")\n",
    "    print('model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for episode in range(0, EPISODES): \n",
    "  done = False \n",
    "  score = 0 \n",
    "\n",
    "  while not done: \n",
    "      # env.render() \n",
    "      action, _ = model.predict(obs, deterministic=True)\n",
    "      obs, reward, done, info = env.step(action)\n",
    "      score += reward\n",
    "  print(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# Roep de eerder gemaakte functie aan\n",
    "df_training_score2 = npz_to_dataframe(npz)\n",
    "\n",
    "#voeg aan beide data frames een kolom toe met de agent naam\n",
    "df_training_score[\"model\"] = \"model 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2.3 PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.0 Bronnen\n",
    "[3 hour course](https://www.youtube.com/watch?v=Mut_u40Sqz4)<br>\n",
    "[baseline docummentatie](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5041ba25690102580176b8f7b4720df17ebf4bc4d40fd191e3f1a5800ffdb29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
