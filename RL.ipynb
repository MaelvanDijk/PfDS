{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL)\n",
    "In deze notebook ga ik een eerste simpele kennismaking met Reinforcement Learning doen. Reinforcement Learning, RL in het kort, is een tak van machinelearning/deeplearning waarbij er een **agent** wordt aangemaakt die in een **environment** diverse **actions** onderneemt en hiervoor een reward **krijgt**. Het eerste deel van dit notebook wordt gewijt aan het implementeren van een basale RL agent in een voorafgemaakte environment. Als dit is geslaagd wordt er getracht een eigen agent te programmeren op een custom environment.\n",
    "\n",
    "## Wat is RL in het kort?\n",
    "Met reinforcement Learning wordt geprobeerd een agent te trainen de juiste beslissingen te maken. Beslissingen worden gemaakt in een bepaalde omgeving (bijvoorbeeld een lab of een videospel). Om de analogie van het videospel aan te houden: de agent (mario) voert een actie uit (springen) en maakt een observatie (een muntje komt uit een blokje). Deze observaties en acties worden aan elkaar gekoppeld doormiddel van een beloningssysteem. Gedurende veel itteraties zal de agent trainen en het beste resultaat proberen te behalen.\n",
    "\n",
    "Binnen RL doen we een aantal aannamens of stellen we een paar beperkingen:\n",
    "* Het probleem is (zeer) complex, anders is RL overkill\n",
    "* Het betreft een Markov achtige omgeving --> elke actie heeft een reactie ofwel observaties en acties volgen elkaar op\n",
    "* trainen kan lang duren en is niet altijd stabiel\n",
    "\n",
    "## Bronnen\n",
    "[3 hour course](https://www.youtube.com/watch?v=Mut_u40Sqz4)<br>\n",
    "[baseline docummentatie](https://stable-baselines3.readthedocs.io/en/master/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.9.7 is nodig voor het gebruik van deze modules\n",
    "# !pip install stable-baselines3[extra] ## docummentatie voor deze module die RL algoritmes bevat: https://stable-baselines3.readthedocs.io/en/master/\n",
    "# !pip install pyglet ## extra dependency voor OpenAI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # importeren voor het omgaan met paths\n",
    "import gym # voor het beheren van de omgeving van onze agents\n",
    "from gym import wrappers # gebruiken om geen visualisaties te doen van de environment\n",
    "from stable_baselines3 import PPO # een specifiek algorithme voor RL\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # wrapper voor environment beheersing\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # beoordeling van agent policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "Voor de environments maken we gebruik van [OpenAI gym](https://gym.openai.com/envs/#classic_control). In deze environments gaat de agent leren door tijdens een aantal episodes \"random\" acties uit te voeren. De environment is de overkoepelende omgeving waarin de agent acteerd, observaties worden gedaan en waaruit beloning worden gehaald. Een OpenAI gym omgeving kan in verschillende vormen (spaces) voorkomen:\n",
    "* **Box:** n-dimensionele tensor, die een range van waarden bevat --> Box(0, 1, shape=(3,3))\n",
    "* **Discrete:** set van discrete waarden --> Discrete(3)\n",
    "* **Tuple:** Een Tuple van andere spaces --> Tuple((Discrete(3), Box(0, 1, shape=(3,3))))\n",
    "* **Dict:** Dictionary van spaces --> Dict({('height': Discrete(3), 'speed':Box(0, 1, shape=(3,3)))})\n",
    "* **MultiBinary:** One hot encoded binary waarde --> MultiBinary(4) --> [0,0,0,0] of [0,1,0,1] etc.\n",
    "* **MultiDiscrete:** Meerdere discrete waarde --> MultiDiscrete([2,5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inladen van environment\n",
    "environment_name ='CartPole-v0'\n",
    "env = gym.make(environment_name)\n",
    "# Test om te zien wat de environment bevat\n",
    "print(\n",
    "    f'''\n",
    "    env.action_space: {env.action_space}\n",
    "    env.observation_space: {env.observation_space}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0\n",
    "Voor de eerste test wordt de CartPole-v0 environment gebruikt. Deze environment bestaat (zoals hierboven te zien) uit:\n",
    "* Een action space Discrete(2), hierbij is 0 == links en 1 == rechts op een virtuele controller.\n",
    "* de observation space, het scherm dat de speler/agent ziet is een Box() type de eerste array geeft de lower-bound aan van 4 type observaties, de tweede array de upper-bound. een array eenmaal aangeroepen bestaat uit 4 elementen van het type float32\n",
    "\n",
    "### De observation space\n",
    "```\n",
    "env.observation_space: \n",
    "    Box(\n",
    "        [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38],\n",
    "        [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38],\n",
    "        (4,),\n",
    "        float32)\n",
    "```\n",
    "\n",
    "dit kan als volgt geïnterpeteerd worden: <br>\n",
    "env.observation_space[0] = [-4.8000002e+00] en [4.8000002e+00] --> dit zijn de min en max posities van het wagentje<br>\n",
    "env.observation_space[1] = [-3.4028235e+38] en [3.4028235e+38] --> de velocity (snelheid) van het wagentje<br>\n",
    "env.observation_space[2] = [-4.1887903e-01] en [4.1887903e-01] --> hoek van de paal ten opzichten van het wagentje<br>\n",
    "env.observation_space[3] = [-3.4028235e+38] en [3.4028235e+38] --> velocity van de paal<br>\n",
    "\n",
    "binnen deze observation space wordt er per env.observation_space[*x*] een waarde teruggeven voor de agent om op te acteren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    " # het aantal keer dat de agent het spel speelt.\n",
    "\n",
    "for episode in range(0, episodes): # loop over de episodes.\n",
    "    state = env.reset() # stel de omgeving opnieuw in.\n",
    "    done = False # zodra het spel is afgelopen verandert dit naar done, en begint dit op false.\n",
    "    score = 0 # de score die onze agent moet verhogen.\n",
    "\n",
    "    while not done: # not done == not False == True.\n",
    "        env.render() #render het spel in een omgeving. normaal gesproken staat dit uit voor snelheid.\n",
    "        action = env.action_space.sample() # van onze action space kies 1 actie random (links of rechts).\n",
    "        n_state, reward, done, info = env.step(action) # neem een stap (frame/actie), dit returned 4 waarden. een nieuwe Box(), de beloning, False/True (het spel is afgelopen), en eventueel extra info.\n",
    "        score += reward # tel de beloning voor deze actie op bij de score van deze episode.\n",
    "    print(f'Episode: {episode+1} | Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmes\n",
    "\n",
    "Er zijn diverse algorithmes algoritmes beschikbaar. Binnen deze notebook wordt gebruik gemaakt van Model-Free RL. dit zijn algorithmes die zich minder/niet bezig houden met het voorspellen van de volgende state maar puur kijken naar een een set regels die de agent maakt op basis van ervaring. Zo kan de agent een regel opstellen dat het wagentje naar links moet als de stok een beetje links kantelt. In onderstaande afbeelding zijn de diverse opties te zien. In dit geval ligt de focus op PPO (Proximal Policy Optimization) dit is een policy algoritme die minder kans heeft om vast te komen zitten in een minima. Normaal gesproken leert policy based algoritmes van de huidige staat en kiest een nieuwe policy. Met PPO gebeurt dit ook maar wordt het verschil tussen policy *p<sub>t</sub>* en *p<sub>t-1</sub>* geminimaliseerd. <br>\n",
    "\n",
    "![algorithmes](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Metrics\n",
    "Voor elk type algoritme zijn diverse metrics beschikbaar. deze kunnen in 4 categoriën worden opgedeeld:\n",
    "* Evaluation: beschrijving van de lengte en reward voor een episode\n",
    "* Time: alles wat betreft tijd besteding\n",
    "* Loss: loss functie om het model te scoren\n",
    "* Overig: deze worden toeglicht wanneer deze aanbod komen\n",
    "\n",
    "De metrics worden in apparte mappen opgeslagen als log bestanden en zijn uit te lezen op diverse manieren. In dit notebook wordt gekeken naar de tensorboard visualisaties en .npz files die zullen worden omgezet naar een dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bepaal de locatie voor het opslaan van logs en models\n",
    "log_path = os.path.join('Training', 'Evaluations')\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "training_path = os.path.join('Training', 'Training')\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name) # herhaal het maken van de environment\n",
    "env = DummyVecEnv([lambda:env]) # wrapper om de environment in een dummy vector te plaatsen\n",
    "model = PPO('MlpPolicy', env, verbose= 1, tensorboard_log= log_path) # een standaard NeuralNetwork (Multilayer Perceptron), verbose= 1 laat ons de resultaten loggen, tensorboard_log = locatie voor log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps= 20000) # train het model voor x timesteps\n",
    "model.save(PPO_path) # Sla het model op in de locatie: Training\\Saved Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# het model kan geladen worden voor het geval er iets mis is gegaan\n",
    "\n",
    "try: #probeer eerst het bestaande model te verwijderen uit de variable list en laad dan een opgeslagen model\n",
    "    del model #verwijder het bestaande model\n",
    "    model = PPO.load(PPO_path, env=env) # laad het model\n",
    "except NameError: # als het model niet bestaat bij de \"del model\" functie catch the exception en laad het model\n",
    "    model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "print(f'Model is ingeladen!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren van model en tensorboard\n",
    "In de onderstaande cellen gaan we het model evalueren en de de geloggde gegevens in een tensorboard plaatsen. \n",
    "Voor nu zijn de belangrijkste metrics om in de gaten te houden de *average Reward* en *average episode length*.\n",
    "Aangezien deze niet aanwezig zijn in de tensorboard worden deze appart gevisualiseerd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalueer het model op basis van een aantal parameters (average punten, standard deviation) --> max punten = 200.\n",
    "evaluate_policy(model, env, n_eval_episodes= 10, render=True) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draai nu de zelfde environment met een getrainde agent\n",
    "def run_agent(model, episodes= 5, render = False):\n",
    "    '''\n",
    "    runs an agent (model) through the environment for n episodes, if render is set to False no window is rendered.\n",
    "    at the end of the runs a tensorlog file is written to a predetermined directory\n",
    "    '''\n",
    "    for episode in range(0, episodes): \n",
    "        obs = env.reset() \n",
    "        done = False \n",
    "        score = 0 \n",
    "\n",
    "        while not done:\n",
    "            if render: \n",
    "                env.render(mode='human') \n",
    "            action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        print(f'Episode: {episode+1} | Score: {score}')\n",
    "    env.close()\n",
    "\n",
    "run_agent(model, render= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorlog ophalen van een locatie\n",
    "train_log_path = os.path.join(log_path, 'PPO_3')\n",
    "\n",
    "#klik de play button hieronder\n",
    "%tensorboard --logdir= train_log_path # klik de play button hierboven voor de Tensorboard evaluations grafieken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalueren op basis van score\n",
    "\n",
    "In de onderstaande cellen wordt de agent gescoord op zijn vaardigheden. Gezien de omgeving redelijk simpel is vindt de agent over het algemeen met gemak een optimale oplossing na verloop van het trainings proces.\n",
    "\n",
    "De eerst volgende 2 cellen beoordelen een volledig getrained model. Het is duidelijk dat hier weinig interessants uitkomt. Het model scoort namelijk in 4 van de 5 episodes maximaal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad het model\n",
    "model = PPO.load(PPO_path, env=env)\n",
    "\n",
    "episodes = 5 \n",
    "lst_score = []\n",
    "lst_episode = []\n",
    "\n",
    "#Draai het model\n",
    "for episode in range(0, episodes): \n",
    "    obs = env.reset() \n",
    "    done = False \n",
    "    score = 0 \n",
    "\n",
    "    while not done: \n",
    "        env.render() \n",
    "        action, _ = model.predict(obs) # inplaats van een random sample van de mogelijke acties maakt het model een keuze op basis van de huidige observatie, de eerste waarde is de actie, de tweede waarde '_' is de state\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    lst_score.append(score[0])\n",
    "    lst_episode.append(episode + 1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dataframe en manipulatie\n",
    "import seaborn as sns # visualisatie\n",
    "from matplotlib import pyplot as plt # extra functionaliteit voor visualisatie\n",
    "\n",
    "d = {\"episode\": lst_episode, \"score\": lst_score}\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "\n",
    "sns.lineplot(x= df[\"episode\"], y=df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder worden 2 nieuwe agents getrained. Wordt de evaluatie van de agent tijdens het trainen gelogged naar een .npz file. In deze log komen alleen de scores en de duratie van de episodes terug.\n",
    "\n",
    "Door twee verschillende modellen te trainen kunnen we het effect van PPO policy duidelijk terug zien. Het is namelijk zo dat de PPO minimale aanpassingen doet in de voorgaande strategie (timestep). Deze afwijkingen zorgen doorgaans voor een langzaam stijgende lijn en voorkomt in de meeste gevallen een foute strategie door een te grote policy wijziging. In de 3e cell hieronder is dit ook te zien. PPO zorgt voor een doorgaans stijgende lijn en afwijkingen in de policy worden minimaal gemaakt hierdoor zie je dat een agent niet snel in een local optimum komt of naar een minima toe beweegt. Hier moet wel gemeld worden dat dit een zeer simpele omgeving betreft waardoor het niet vreemd is dat agents in deze omgeving een globaal optimum bereiken.\n",
    "\n",
    "![model1](visualisatie\\model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nieuw model voor loggen trainings informatie\n",
    "# env = wrappers.Monitor(env, video_callable=False ,force=True)\n",
    "model2 = PPO('MlpPolicy', env)\n",
    "model2.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # gebruiken voor wiskundige operaties en npz files te lezen\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "def npz_to_dataframe(npz):\n",
    "    '''\n",
    "    Takes an npz file object loaded using numpy.load(path to .npz file) and returns a formatted dataframe\n",
    "    '''\n",
    "    # maak een data frame van de npz file op basis van de kolommen\n",
    "    df = pd.DataFrame.from_dict({col: npz[col] for col in npz.files}, orient='index')\n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # transpose de dataframe gezien deze verkeerd om staat\n",
    "    df =df.T\n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "\n",
    "    # bereken de gemiddeldes voor iedere cel\n",
    "    df = df.applymap(np.mean)\n",
    "    # display(df_training_score) #-->uncomment om de transformatie te zien\n",
    "    return df\n",
    "\n",
    "df_training_score = npz_to_dataframe(npz)\n",
    "df_training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een lineplot om de prestaties van het model na diverse timesteps te zien\n",
    "fig = sns.lineplot(x= df_training_score[\"timesteps\"], y=df_training_score[\"results\"])\n",
    "display(fig)\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vergelijk de training met een tweede model\n",
    "model3 = PPO('MlpPolicy', env)\n",
    "model3.learn(total_timesteps= 20000, eval_freq  =100, eval_log_path = training_path, eval_env= env, n_eval_episodes= 5)\n",
    "\n",
    "npz = np.load('Training\\Training\\evaluations.npz')\n",
    "\n",
    "# Roep de eerder gemaakte functie aan\n",
    "df_training_score2 = npz_to_dataframe(npz)\n",
    "\n",
    "#voeg aan beide data frames een kolom toe met de agent naam\n",
    "df_training_score[\"model\"] = \"model 1\"\n",
    "df_training_score2[\"model\"] = \"model 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder worden de twee agents tijdens het trainen met elkaar vergeleken. Het is goed te zien dat de twee agents andere start punten hebben. Door kleine aanpassingen in de strategie kan de score in het begin sterk stijgen/dalen om vervolgens na een redelijk stijgende lijn in een bepaald optimum te settlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_training_score.append(df_training_score2)\n",
    "fig = sns.lineplot(x= df_compare[\"timesteps\"], y=df_compare[\"results\"], hue= df_compare['model'])\n",
    "display(fig)\n",
    "\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('visualisatie\\model1vsmodel2')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5041ba25690102580176b8f7b4720df17ebf4bc4d40fd191e3f1a5800ffdb29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
